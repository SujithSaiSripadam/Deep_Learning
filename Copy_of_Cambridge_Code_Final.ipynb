{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SujithSaiSripadam/Deep_Learning/blob/main/Copy_of_Cambridge_Code_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0Ykh8yFfS8d"
      },
      "source": [
        "## Importing necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aQ6IlIHJCgbL"
      },
      "outputs": [],
      "source": [
        "pip install dgl torch-geometric optuna > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3I9zLZK6G9ll",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "f8a715c1-7756-4bf0-be51-bf7fbb9ef8ad"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'DILL_AVAILABLE' from 'torch.utils.data.datapipes.utils.common' (/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/utils/common.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7cf447889c76>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTPESampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dgl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_backend\u001b[0m  \u001b[0;31m# usort: skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m from . import (\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mcontainer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dgl/dataloading/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preferred_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pytorch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mspot_target\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_dataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dgl/dataloading/dataloader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbatch_graphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPUCache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyFeature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheterograph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDGLGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dgl/distributed/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_context\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexit_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_dataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistGraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistGraphServer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdist_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgraph_partition_book\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraphPartitionBook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPartitionPolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dgl/distributed/dist_graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraphbolt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheterograph_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mempty_shared_mem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDGLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mETYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dgl/graphbolt/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ffi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibinfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mminibatch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dgl/graphbolt/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_datapipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIterDataPipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrecursive_apply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_extension\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatapipes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mjanitor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjanitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataChunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunctional_datapipe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"DataChunk\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"functional_datapipe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"utils\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m )\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbz2fileloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBz2FileLoaderIterDataPipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mBz2FileLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m from torchdata.datapipes.iter.util.cacheholder import (\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mEndOnDiskCacheHolderIterDataPipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mEndOnDiskCacheHolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mInMemoryCacheHolderIterDataPipe\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mInMemoryCacheHolder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchdata/datapipes/iter/util/cacheholder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mportalocker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatapipes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_unpickable_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDILL_AVAILABLE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraverse_dps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'DILL_AVAILABLE' from 'torch.utils.data.datapipes.utils.common' (/usr/local/lib/python3.10/dist-packages/torch/utils/data/datapipes/utils/common.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# importing libraries\n",
        "import os\n",
        "import pdb\n",
        "import copy\n",
        "import shutil\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import KFold,LeaveOneOut\n",
        "\n",
        "import scipy.io\n",
        "from scipy.signal import resample\n",
        "from scipy.fftpack import fft\n",
        "from scipy.io import loadmat\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Batch, Data\n",
        "from torch_geometric.utils import sparse, to_torch_coo_tensor\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GATv2Conv,GATConv, global_mean_pool, GCNConv\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from tqdm import tqdm\n",
        "\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "import networkx as nx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xZSavrMUXIa"
      },
      "outputs": [],
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GWsWppI9clP"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Set a specific seed value (replace 42 with your desired seed)\n",
        "seed_value = 42\n",
        "set_seed(seed_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MteEHxhY2pCa"
      },
      "source": [
        "# Data Pre-processing  1 ( When you dont have preprocess data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cs3Mbf74UQjm"
      },
      "source": [
        "## For Video Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8UA4oCz4U8l"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Source directory\n",
        "source_dir = \"/content/drive/MyDrive/\"\n",
        "\n",
        "# Destination directory\n",
        "destination_dir = \"/content/drive/MyDrive/Cambridge_Data\"\n",
        "#shutil.copytree(source_dir, destination_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQyaFf52FogL"
      },
      "outputs": [],
      "source": [
        "# # Demo of Cambridge Original Data\n",
        "# x=pd.read_csv(\"/content/drive/MyDrive/Cambridge_Data/Ayush/Data/s03_Task2.csv\")\n",
        "# print(x.head())\n",
        "# print(x.shape)\n",
        "# print(x.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQroVIuCFYbu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea03a86-3867-46b1-9f56-7786cf7e4e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s03_Task4.csv', 's04_Task4.csv', 's05_Task4.csv', 's06_Task4.csv', 's07_Task4.csv', 's08_Task4.csv', 's10_Task4.csv', 's11_Task4.csv', 's13_Task4.csv', 's14_Task4.csv', 's15_Task4.csv', 's16_Task4.csv', 's24_Task4.csv', 's22_Task4.csv', 's21_Task4.csv', 's20_Task4.csv', 's17_Task4.csv', 's19_Task4.csv', 's26_Task4.csv', 's25_Task4.csv', 's41_Task4.csv', 's37_Task4.csv', 's38_Task4.csv', 's28_Task4.csv', 's36_Task4.csv', 's30_Task4.csv', 's39_Task4.csv', 's40_Task4.csv', 's34_Task4.csv', 's35_Task4.csv', 's43_Task4.csv', 's31_Task4.csv', 's32_Task4.csv', 's29_Task4.csv', 's33_Task4.csv', 's42_Task4.csv', 's010_Task4_P3.csv', 's03_Task4_P1.csv', 's08_Task4_P1.csv', 's11_Task4_P1.csv', 's07_Task4_P1.csv', 's04_Task4_P1.csv', 's05_Task4_P1.csv', 's06_Task4_P1.csv', 's14_Task4_P1.csv', 's20_Task4_P1.csv', 's21_Task4_P1.csv', 's16_Task4_P1.csv', 's15_Task4_P1.csv', 's17_Task4_P1.csv', 's19_Task4_P1.csv', 's24_Task4_P1.csv', 's28_Task4_P1.csv', 's25_Task4_P1.csv', 's22_Task4_P1.csv', 's29_Task4_P1.csv', 's26_Task4_P1.csv', 's30_Task4_P1.csv', 's34_Task4_P1.csv', 's37_Task4_P1.csv', 's35_Task4_P1.csv', 's38_Task4_P1.csv', 's32_Task4_P1.csv', 's33_Task4_P1.csv', 's39_Task4_P1.csv', 's42_Task4_P1.csv', 's07_Task4_P2.csv', 's06_Task4_P2.csv', 's43_Task4_P1.csv', 's05_Task4_P2.csv', 's04_Task4_P2.csv', 's41_Task4_P1.csv', 's03_Task4_P2.csv', 's40_Task4_P1.csv', 's17_Task4_P2.csv', 's08_Task4_P2.csv', 's14_Task4_P2.csv', 's13_Task4_P2.csv', 's11_Task4_P2.csv', 's19_Task4_P2.csv', 's16_Task4_P2.csv', 's15_Task4_P2.csv', 's20_Task4_P2.csv', 's28_Task4_P2.csv', 's33_Task4_P2.csv', 's30_Task4_P2.csv', 's21_Task4_P2.csv', 's32_Task4_P2.csv', 's29_Task4_P2.csv', 's26_Task4_P2.csv', 's22_Task4_P2.csv', 's31_Task4_P2.csv', 's35_Task4_P2.csv', 's34_Task4_P2.csv', 's24_Task4_P2.csv', 's25_Task4_P2.csv', 's37_Task4_P2.csv', 's40_Task4_P2.csv', 's41_Task4_P2.csv', 's42_Task4_P2.csv', 's36_Task4_P2.csv', 's38_Task4_P2.csv', 's39_Task4_P2.csv', 's07_Task4_P3.csv', 's06_Task4_P3.csv', 's05_Task4_P3.csv', 's08_Task4_P3.csv', 's10_Task4_P3.csv', 's04_Task4_P3.csv', 's03_Task4_P3.csv', 's43_Task4_P2.csv', 's11_Task4_P3.csv', 's16_Task4_P3.csv', 's13_Task4_P3.csv', 's14_Task4_P3.csv', 's20_Task4_P3.csv', 's15_Task4_P3.csv', 's19_Task4_P3.csv', 's21_Task4_P3.csv', 's17_Task4_P3.csv', 's29_Task4_P3.csv', 's28_Task4_P3.csv', 's24_Task4_P3.csv', 's25_Task4_P3.csv', 's22_Task4_P3.csv', 's26_Task4_P3.csv', 's38_Task4_P3.csv', 's37_Task4_P3.csv', 's40_Task4_P3.csv', 's31_Task4_P3.csv', 's36_Task4_P3.csv', 's33_Task4_P3.csv', 's34_Task4_P3.csv', 's39_Task4_P3.csv', 's30_Task4_P3.csv', 's35_Task4_P3.csv', 's32_Task4_P3.csv', 's43_Task4_P3.csv', 's41_Task4_P3.csv', 's42_Task4_P3.csv', 's13_Task4_P1.csv', 's31_Task4_P1.csv', 's36_Task4_P1.csv', 's10_Task4_P1.csv']\n",
            "['s010_Task4_P3.csv', 's03_Task4.csv', 's03_Task4_P1.csv', 's03_Task4_P2.csv', 's03_Task4_P3.csv', 's04_Task4.csv', 's04_Task4_P1.csv', 's04_Task4_P2.csv', 's04_Task4_P3.csv', 's05_Task4.csv', 's05_Task4_P1.csv', 's05_Task4_P2.csv', 's05_Task4_P3.csv', 's06_Task4.csv', 's06_Task4_P1.csv', 's06_Task4_P2.csv', 's06_Task4_P3.csv', 's07_Task4.csv', 's07_Task4_P1.csv', 's07_Task4_P2.csv', 's07_Task4_P3.csv', 's08_Task4.csv', 's08_Task4_P1.csv', 's08_Task4_P2.csv', 's08_Task4_P3.csv', 's10_Task4.csv', 's10_Task4_P1.csv', 's10_Task4_P3.csv', 's11_Task4.csv', 's11_Task4_P1.csv', 's11_Task4_P2.csv', 's11_Task4_P3.csv', 's13_Task4.csv', 's13_Task4_P1.csv', 's13_Task4_P2.csv', 's13_Task4_P3.csv', 's14_Task4.csv', 's14_Task4_P1.csv', 's14_Task4_P2.csv', 's14_Task4_P3.csv', 's15_Task4.csv', 's15_Task4_P1.csv', 's15_Task4_P2.csv', 's15_Task4_P3.csv', 's16_Task4.csv', 's16_Task4_P1.csv', 's16_Task4_P2.csv', 's16_Task4_P3.csv', 's17_Task4.csv', 's17_Task4_P1.csv', 's17_Task4_P2.csv', 's17_Task4_P3.csv', 's19_Task4.csv', 's19_Task4_P1.csv', 's19_Task4_P2.csv', 's19_Task4_P3.csv', 's20_Task4.csv', 's20_Task4_P1.csv', 's20_Task4_P2.csv', 's20_Task4_P3.csv', 's21_Task4.csv', 's21_Task4_P1.csv', 's21_Task4_P2.csv', 's21_Task4_P3.csv', 's22_Task4.csv', 's22_Task4_P1.csv', 's22_Task4_P2.csv', 's22_Task4_P3.csv', 's24_Task4.csv', 's24_Task4_P1.csv', 's24_Task4_P2.csv', 's24_Task4_P3.csv', 's25_Task4.csv', 's25_Task4_P1.csv', 's25_Task4_P2.csv', 's25_Task4_P3.csv', 's26_Task4.csv', 's26_Task4_P1.csv', 's26_Task4_P2.csv', 's26_Task4_P3.csv', 's28_Task4.csv', 's28_Task4_P1.csv', 's28_Task4_P2.csv', 's28_Task4_P3.csv', 's29_Task4.csv', 's29_Task4_P1.csv', 's29_Task4_P2.csv', 's29_Task4_P3.csv', 's30_Task4.csv', 's30_Task4_P1.csv', 's30_Task4_P2.csv', 's30_Task4_P3.csv', 's31_Task4.csv', 's31_Task4_P1.csv', 's31_Task4_P2.csv', 's31_Task4_P3.csv', 's32_Task4.csv', 's32_Task4_P1.csv', 's32_Task4_P2.csv', 's32_Task4_P3.csv', 's33_Task4.csv', 's33_Task4_P1.csv', 's33_Task4_P2.csv', 's33_Task4_P3.csv', 's34_Task4.csv', 's34_Task4_P1.csv', 's34_Task4_P2.csv', 's34_Task4_P3.csv', 's35_Task4.csv', 's35_Task4_P1.csv', 's35_Task4_P2.csv', 's35_Task4_P3.csv', 's36_Task4.csv', 's36_Task4_P1.csv', 's36_Task4_P2.csv', 's36_Task4_P3.csv', 's37_Task4.csv', 's37_Task4_P1.csv', 's37_Task4_P2.csv', 's37_Task4_P3.csv', 's38_Task4.csv', 's38_Task4_P1.csv', 's38_Task4_P2.csv', 's38_Task4_P3.csv', 's39_Task4.csv', 's39_Task4_P1.csv', 's39_Task4_P2.csv', 's39_Task4_P3.csv', 's40_Task4.csv', 's40_Task4_P1.csv', 's40_Task4_P2.csv', 's40_Task4_P3.csv', 's41_Task4.csv', 's41_Task4_P1.csv', 's41_Task4_P2.csv', 's41_Task4_P3.csv', 's42_Task4.csv', 's42_Task4_P1.csv', 's42_Task4_P2.csv', 's42_Task4_P3.csv', 's43_Task4.csv', 's43_Task4_P1.csv', 's43_Task4_P2.csv', 's43_Task4_P3.csv']\n"
          ]
        }
      ],
      "source": [
        "#Data -> OpenFace_data\n",
        "directory_path=\"/content/drive/MyDrive/Cambridge_Data/OpenFace_data\"\n",
        "Task_4=[]\n",
        "for file in os.listdir(directory_path):\n",
        "   if \"Task4\" in file:\n",
        "      Task_4.append(file)\n",
        "print(Task_4)\n",
        "Task_4.sort()\n",
        "print(Task_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aoo4VIu_18qD"
      },
      "outputs": [],
      "source": [
        "#  Modify csv to take only essential columns from it\n",
        "def modify_df(df):\n",
        "      df_new =df[[\" gaze_0_x\",\" gaze_0_y\",\" gaze_0_z\",\" gaze_1_x\",\" gaze_1_y\",\" gaze_1_z\",\" gaze_angle_x\",\" gaze_angle_y\",\" pose_Tx\",\" pose_Ty\",\" pose_Tz\",\" pose_Rx\",\" pose_Ry\",\" pose_Rz\",\" AU01_r\",\" AU02_r\",\" AU04_r\",\" AU05_r\",\" AU06_r\",\" AU07_r\",\" AU09_r\",\" AU10_r\",\" AU12_r\",\" AU14_r\",\" AU15_r\",\" AU17_r\",\" AU20_r\",\" AU23_r\",\" AU25_r\",\" AU26_r\",\" AU45_r\"]]\n",
        "      return df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUDL3kTqUQcD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54fdc3c8-57e0-4291-dc84-d9e164911e5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s03_Task2.csv', 's05_Task2.csv', 's06_Task2.csv', 's07_Task2.csv', 's08_Task2.csv', 's10_Task2.csv', 's11_Task2.csv', 's13_Task2.csv', 's14_Task2.csv', 's15_Task2.csv', 's16_Task2.csv', 's17_Task2.csv', 's24_Task2.csv', 's26_Task2.csv', 's19_Task2.csv', 's25_Task2.csv', 's21_Task2.csv', 's22_Task2.csv', 's20_Task2.csv', 's35_Task2.csv', 's42_Task2.csv', 's40_Task2.csv', 's29_Task2.csv', 's30_Task2.csv', 's33_Task2.csv', 's34_Task2.csv', 's36_Task2.csv', 's38_Task2.csv', 's41_Task2.csv', 's39_Task2.csv', 's28_Task2.csv', 's43_Task2.csv', 's04_Task2.csv', 's31_Task2.csv', 's32_Task2.csv', 's37_Task2.csv']\n",
            "['s03_Task2.csv', 's04_Task2.csv', 's05_Task2.csv', 's06_Task2.csv', 's07_Task2.csv', 's08_Task2.csv', 's10_Task2.csv', 's11_Task2.csv', 's13_Task2.csv', 's14_Task2.csv', 's15_Task2.csv', 's16_Task2.csv', 's17_Task2.csv', 's19_Task2.csv', 's20_Task2.csv', 's21_Task2.csv', 's22_Task2.csv', 's24_Task2.csv', 's25_Task2.csv', 's26_Task2.csv', 's28_Task2.csv', 's29_Task2.csv', 's30_Task2.csv', 's31_Task2.csv', 's32_Task2.csv', 's33_Task2.csv', 's34_Task2.csv', 's35_Task2.csv', 's36_Task2.csv', 's37_Task2.csv', 's38_Task2.csv', 's39_Task2.csv', 's40_Task2.csv', 's41_Task2.csv', 's42_Task2.csv', 's43_Task2.csv']\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n"
          ]
        }
      ],
      "source": [
        "directory_path=\"/content/drive/MyDrive/Cambridge_Data/OpenFace_data\"\n",
        "Task_2=[]\n",
        "for file in os.listdir(directory_path):\n",
        "   if \"Task2\" in file:\n",
        "      Task_2.append(file)\n",
        "print(Task_2)\n",
        "Task_2.sort()\n",
        "print(Task_2)\n",
        "# Split TASK 2 into 2 separate CSV representing Happy and Sad\n",
        "timestamp_Task2_h=[63, 60, 70, 71, 102, 103, 110, 74, 86, 74, 70, 83, 75, 78, 91, 71, 73, 84, 78, 70, 70, 84, 72, 72, 71, 71, 74, 72, 70, 73, 69, 77, 98, 72, 94, 82]\n",
        "timestamp_Task2_s=[63, 60, 70, 71, 107, 112, 126, 74, 86, 74, 70, 83, 75, 78, 91, 71, 73, 84, 78, 70, 70, 84, 72, 72, 71, 71, 74, 72, 72, 73, 69, 77, 98, 72, 94, 82]\n",
        "# subject_id=[S03, S04, S05, S06, S07, S08, S10, S11, S13, S14, S15, S16, S17, S19, S20, S21, S22, S24, S25, S26, S28, S29, S30, S31, S32, S33, S34, S35, S36, S37, S38, S39, S40, S41, S42, S43]\n",
        "subject_id=['S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S10', 'S11', 'S13', 'S14', 'S15', 'S16', 'S17', 'S19', 'S20', 'S21', 'S22', 'S24', 'S25', 'S26', 'S28', 'S29', 'S30', 'S31', 'S32', 'S33', 'S34', 'S35', 'S36', 'S37', 'S38', 'S39', 'S40', 'S41', 'S42', 'S43']\n",
        "i=0\n",
        "directory_path=\"/content/drive/MyDrive/Cambridge_Data/OpenFace_data\"\n",
        "for file in Task_2:\n",
        "      df = pd.read_csv(directory_path+'/'+file)\n",
        "      mask_1 = df[' timestamp'] <= timestamp_Task2_h[i]\n",
        "      df_1 = df[mask_1]\n",
        "      df_1=modify_df(df_1)\n",
        "      df_1.to_csv('/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_2_h/'+f\"{subject_id[i]}.csv\")\n",
        "      mask_2 = df[' timestamp'] >= timestamp_Task2_s[i]\n",
        "      df_2 = df[mask_2]\n",
        "      df_2=modify_df(df_2)\n",
        "      df_2.to_csv('/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_2_s/'+f\"{subject_id[i]}.csv\")\n",
        "      i+=1\n",
        "      print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIoQ_8Y1Ty4n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b63385-456e-41f8-ffc4-a0acf9288c2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s03_Task3.csv', 's04_Task3.csv', 's05_Task3.csv', 's07_Task3.csv', 's08_Task3.csv', 's11_Task3.csv', 's14_Task3.csv', 's15_Task3.csv', 's16_Task3.csv', 's21_Task3.csv', 's25_Task3.csv', 's17_Task3.csv', 's26_Task3.csv', 's24_Task3.csv', 's20_Task3.csv', 's22_Task3.csv', 's40_Task3.csv', 's28_Task3.csv', 's36_Task3.csv', 's33_Task3.csv', 's39_Task3.csv', 's41_Task3.csv', 's38_Task3.csv', 's35_Task3.csv', 's30_Task3.csv', 's29_Task3.csv', 's43_Task3.csv', 's32_Task3.csv', 's19_Task3.csv', 's34_Task3.csv', 's42_Task3.csv', 's37_Task3.csv', 's06_Task3.csv', 's10_Task3.csv', 's13_Task3.csv', 's31_Task3.csv']\n",
            "['s03_Task3.csv', 's04_Task3.csv', 's05_Task3.csv', 's06_Task3.csv', 's07_Task3.csv', 's08_Task3.csv', 's10_Task3.csv', 's11_Task3.csv', 's13_Task3.csv', 's14_Task3.csv', 's15_Task3.csv', 's16_Task3.csv', 's17_Task3.csv', 's19_Task3.csv', 's20_Task3.csv', 's21_Task3.csv', 's22_Task3.csv', 's24_Task3.csv', 's25_Task3.csv', 's26_Task3.csv', 's28_Task3.csv', 's29_Task3.csv', 's30_Task3.csv', 's31_Task3.csv', 's32_Task3.csv', 's33_Task3.csv', 's34_Task3.csv', 's35_Task3.csv', 's36_Task3.csv', 's37_Task3.csv', 's38_Task3.csv', 's39_Task3.csv', 's40_Task3.csv', 's41_Task3.csv', 's42_Task3.csv', 's43_Task3.csv']\n"
          ]
        }
      ],
      "source": [
        "# Modify all csv files of Task 3\n",
        "directory_path=\"/content/drive/MyDrive/Cambridge_Data/OpenFace_data\"\n",
        "subject_id=['S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S10', 'S11', 'S13', 'S14', 'S15', 'S16', 'S17', 'S19', 'S20', 'S21', 'S22', 'S24', 'S25', 'S26', 'S28', 'S29', 'S30', 'S31', 'S32', 'S33', 'S34', 'S35', 'S36', 'S37', 'S38', 'S39', 'S40', 'S41', 'S42', 'S43']\n",
        "i=0\n",
        "Task_3=[]\n",
        "for file in os.listdir(directory_path):\n",
        "   if \"Task3\" in file:\n",
        "      Task_3.append(file)\n",
        "print(Task_3)\n",
        "Task_3.sort()\n",
        "print(Task_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaFCX_VjZQYM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "8a3ddd70-59fc-4512-d868-7b58fddea1ce"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Task_3' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-9fb00eda856a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# file=file[30:]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdirectory_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Cambridge_Data/OpenFace_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTask_3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0mdf_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mdf_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodify_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Task_3' is not defined"
          ]
        }
      ],
      "source": [
        "# file=file[30:]\n",
        "directory_path=\"/content/drive/MyDrive/Cambridge_Data/OpenFace_data\"\n",
        "for file in Task_3:\n",
        "      df_1 = pd.read_csv(directory_path+'/'+file)\n",
        "      df_1=modify_df(df_1)\n",
        "      df_1.to_csv('/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task3_final/'+f\"{subject_id[i]}.csv\")\n",
        "      i+=1\n",
        "      print(i,\" subject id:\", subject_id[i-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gvzGF9Ikrxq"
      },
      "outputs": [],
      "source": [
        "dir_path=\"/content/drive/MyDrive/Cambridge_Data/Modify_Audio_Data/Task5p_A\"\n",
        "len(os.listdir(dir_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHSaNcv0wy9d"
      },
      "outputs": [],
      "source": [
        "# Modify all csv files of Task 5\n",
        "directory_path=\"/content/drive/MyDrive/Cambridge_Data/Data\"\n",
        "subject_id=['S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S10', 'S11', 'S13', 'S14', 'S15', 'S16', 'S17', 'S19', 'S20', 'S21', 'S22', 'S24', 'S25', 'S26', 'S28', 'S29', 'S30', 'S31', 'S32', 'S33', 'S34', 'S35', 'S36', 'S37', 'S38', 'S39', 'S40', 'S41', 'S42', 'S43']\n",
        "i=0\n",
        "Task_5=[]\n",
        "for file in os.listdir(directory_path):\n",
        "   if \"Task5\" in file:\n",
        "      Task_5.append(file)\n",
        "print(Task_5)\n",
        "Task_5.sort()\n",
        "print(Task_5)\n",
        "# file=file[0:]\n",
        "for file in Task_5:\n",
        "      df_1 = pd.read_csv(directory_path+'/'+file)\n",
        "      df_1=modify_df(df_1)\n",
        "      df_1.to_csv('/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_5/'+f\"{subject_id[i]}.csv\")\n",
        "      i+=1\n",
        "      print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgXG1uk__4tP"
      },
      "outputs": [],
      "source": [
        "#  Checking directory length\n",
        "len(os.listdir('/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_4_P3'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6zJq7HnvX0D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "272c5115-3bfe-4cdf-fd74-33c1ad8207cf"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index([' gaze_0_x', ' gaze_0_y', ' gaze_0_z', ' gaze_1_x', ' gaze_1_y',\\n       ' gaze_1_z', ' gaze_angle_x', ' gaze_angle_y', ' pose_Tx', ' pose_Ty',\\n       ' pose_Tz', ' pose_Rx', ' pose_Ry', ' pose_Rz', ' AU01_r', ' AU02_r',\\n       ' AU04_r', ' AU05_r', ' AU06_r', ' AU07_r', ' AU09_r', ' AU10_r',\\n       ' AU12_r', ' AU14_r', ' AU15_r', ' AU17_r', ' AU20_r', ' AU23_r',\\n       ' AU25_r', ' AU26_r', ' AU45_r'],\\n      dtype='object')] are in the [columns]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-46ee28fb64cf>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msubject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodify_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mdf_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_3/S'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34mf\"{subject_id}.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-9dc01ca509a2>\u001b[0m in \u001b[0;36mmodify_df\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#  Modify csv to take only essential columns from it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodify_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m       \u001b[0mdf_new\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\" gaze_0_x\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" gaze_0_y\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" gaze_0_z\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" gaze_1_x\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" gaze_1_y\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" gaze_1_z\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" gaze_angle_x\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" gaze_angle_y\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" pose_Tx\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" pose_Ty\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" pose_Tz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" pose_Rx\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" pose_Ry\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" pose_Rz\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU01_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU02_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU04_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU05_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU06_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU07_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU09_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU10_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU12_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU14_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU15_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU17_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU20_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU23_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU25_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU26_r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" AU45_r\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mdf_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3813\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3815\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6068\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6070\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6072\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6129\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6130\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6132\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([' gaze_0_x', ' gaze_0_y', ' gaze_0_z', ' gaze_1_x', ' gaze_1_y',\\n       ' gaze_1_z', ' gaze_angle_x', ' gaze_angle_y', ' pose_Tx', ' pose_Ty',\\n       ' pose_Tz', ' pose_Rx', ' pose_Ry', ' pose_Rz', ' AU01_r', ' AU02_r',\\n       ' AU04_r', ' AU05_r', ' AU06_r', ' AU07_r', ' AU09_r', ' AU10_r',\\n       ' AU12_r', ' AU14_r', ' AU15_r', ' AU17_r', ' AU20_r', ' AU23_r',\\n       ' AU25_r', ' AU26_r', ' AU45_r'],\\n      dtype='object')] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "#  For modification of files which are not converted in loop due to some problem in file\n",
        "file='/content/drive/MyDrive/Cambridge_Data/OpenFace_data/s19_Task3.csv'\n",
        "directory_path=\"/content/drive/MyDrive/Cambridge_Data/OpenFace_data\"\n",
        "subject_id=19\n",
        "df_1 = pd.read_csv(file)\n",
        "df_1=modify_df(df_1)\n",
        "df_1.to_csv('/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_3/S'+f\"{subject_id}.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTEFVb5VGnDU"
      },
      "outputs": [],
      "source": [
        "directory_path=\"/content/drive/MyDrive/Cambridge_Data/Data\"\n",
        "Task_4=[]\n",
        "for file in os.listdir(directory_path):\n",
        "   if \"Task4\" in file:\n",
        "      Task_4.append(file)\n",
        "print(Task_4)\n",
        "Task_4.sort()\n",
        "print(Task_4)\n",
        "# Split TASK 4 into 3 separate CSV representing Picture 1,2 and 3\n",
        "timestamp_Task4_1=[103, 107, 132, 139, 142, 158, 167, 131, 151, 140, 157, 129, 146, 131, 152, 129, 138, 136, 153, 130, 140, 122, 136, 145, 136, 99, 131, 152, 146, 137, 130, 131, 133, 131, 144, 132] #Between start to picture 1\n",
        "timestamp_Task4_2=[172, 164, 203, 239, 215, 271, 252, 201, 243, 211, 237, 204, 236, 202, 235, 201, 210, 212, 232, 203, 210, 208, 209, 218, 218, 176, 206, 228, 222, 212, 202, 201, 207, 208, 224, 216] # Start of picture 3 to end\n",
        "subject_id=['S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S10', 'S11', 'S13', 'S14', 'S15', 'S16', 'S17', 'S19', 'S20', 'S21', 'S22', 'S24', 'S25', 'S26', 'S28', 'S29', 'S30', 'S31', 'S32', 'S33', 'S34', 'S35', 'S36', 'S37', 'S38', 'S39', 'S40', 'S41', 'S42', 'S43']\n",
        "i=0\n",
        "# file=file[21:22]\n",
        "directory_path=\"/content/drive/MyDrive/Cambridge_Data/Data\"\n",
        "for file in Task_4:\n",
        "      df = pd.read_csv(directory_path+'/'+file)\n",
        "      mask_1 = df[' timestamp']  <= timestamp_Task4_1[i]\n",
        "      df_1 = df[mask_1]\n",
        "      df_1=modify_df(df_1)\n",
        "      df_1.to_csv('/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task4_P1_final/'+f\"{subject_id[i]}.csv\")\n",
        "      mask_2 = ((df[' timestamp']  > timestamp_Task4_1[i]) & (df[' timestamp']  <= timestamp_Task4_2[i]))\n",
        "      df_2 = df[mask_2]\n",
        "      df_2=modify_df(df_2)\n",
        "      df_2.to_csv('/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task4_P2_final/'+f\"{subject_id[i]}.csv\")\n",
        "      mask_3 = df[' timestamp']  > timestamp_Task4_2[i]\n",
        "      df_3 = df[mask_3]\n",
        "      df_3=modify_df(df_3)\n",
        "      df_3.to_csv('/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task4_P3_final/'+f\"{subject_id[i]}.csv\")\n",
        "      i+=1\n",
        "      print(i,\" Subject id:\", subject_id[i-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoQ647vC4gQL"
      },
      "outputs": [],
      "source": [
        "#  For modification of files which are not converted in loop due to some problem in file\n",
        "file='/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_3/S03.csv'\n",
        "subject_id=10\n",
        "df_1 = pd.read_csv(file)\n",
        "# df_1=df_1.drop('Unnamed: 0', axis=1)\n",
        "df_1.to_csv('/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_3/S'+f\"{subject_id}.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyfcGeQp5lvl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "c6ab0e4c-a03b-43bd-b03a-7a3f4ab17d82"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_3/S03.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d4b43f671787>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_3/S03.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_3/S03.csv'"
          ]
        }
      ],
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_3/S03.csv', index_col=[0])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "galksqwHUWYZ"
      },
      "source": [
        "## For audio Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZF9Nac8ZUdvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd6139a-9972-4943-d8a3-2b746db861df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Cambridge_Data/Audio_data/.DS_Store',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s10/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s10/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s10/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s10/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s10/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s10/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s10/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s11/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s11/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s11/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s11/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s11/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s11/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s11/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s13/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s13/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s13/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s13/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s13/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s13/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s13/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s14/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s14/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s14/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s14/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s14/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s14/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s14/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s15/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s15/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s15/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s15/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s15/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s15/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s15/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s16/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s16/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s16/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s16/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s16/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s16/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s16/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s17/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s17/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s17/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s17/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s17/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s17/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s17/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s19/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s19/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s19/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s19/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s19/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s19/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s19/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s20/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s20/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s20/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s20/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s20/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s20/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s20/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s21/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s21/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s21/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s21/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s21/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s21/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s21/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s22/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s22/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s22/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s22/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s22/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s22/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s22/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s24/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s24/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s24/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s24/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s24/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s24/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s24/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s25/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s25/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s25/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s25/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s25/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s25/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s25/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s26/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s26/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s26/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s26/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s26/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s26/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s26/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s28/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s28/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s28/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s28/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s28/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s28/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s28/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s29/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s29/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s29/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s29/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s29/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s29/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s29/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s3/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s3/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s3/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s3/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s3/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s3/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s3/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s30/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s30/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s30/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s30/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s30/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s30/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s30/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s31/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s31/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s31/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s31/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s31/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s31/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s31/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s32/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s32/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s32/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s32/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s32/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s32/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s32/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s33/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s33/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s33/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s33/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s33/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s33/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s33/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s34/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s34/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s34/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s34/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s34/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s34/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s34/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s35/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s35/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s35/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s35/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s35/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s35/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s35/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s36/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s36/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s36/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s36/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s36/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s36/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s36/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s37/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s37/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s37/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s37/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s37/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s37/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s37/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s38/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s38/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s38/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s38/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s38/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s38/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s38/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s39/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s39/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s39/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s39/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s39/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s39/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s39/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s4/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s4/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s4/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s4/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s4/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s4/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s4/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s40/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s40/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s40/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s40/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s40/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s40/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s40/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s41/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s41/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s41/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s41/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s41/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s41/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s41/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s42/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s42/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s42/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s42/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s42/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s42/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s42/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s43/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s43/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s43/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s43/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s43/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s43/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s43/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s5/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s5/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s5/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s5/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s5/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s5/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s5/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s6/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s6/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s6/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s6/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s6/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s6/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s6/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s7/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s7/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s7/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s7/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s7/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s7/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s7/v1Task5p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s8/v1Task2h.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s8/v1Task2s.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s8/v1Task3p.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s8/v1Task4_P1.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s8/v1Task4_P2.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s8/v1Task4_P3.csv',\n",
              " '/content/drive/MyDrive/Cambridge_Data/Audio_data/s8/v1Task5p.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "file_list=[]\n",
        "def list_files_in_directory(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_list.append(file_path)\n",
        "\n",
        "# Replace 'your_directory_path' with the path of the directory you want to explore\n",
        "directory_path = '/content/drive/MyDrive/Cambridge_Data/Audio_data'\n",
        "\n",
        "list_files_in_directory(directory_path)\n",
        "file_list=sorted(file_list)\n",
        "file_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjhfvAB_Wh0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d1372a-5bf0-4781-a917-558100bf5c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36 36 36 36 36 36 36\n"
          ]
        }
      ],
      "source": [
        "# Task 2\n",
        "Task2h_A=[]\n",
        "Task2s_A=[]\n",
        "Task3_A=[]\n",
        "Task4_P1_A=[]\n",
        "Task4_P2_A=[]\n",
        "Task4_P3_A=[]\n",
        "Task5_A=[]\n",
        "for file in file_list:\n",
        "   if 'Task2h' in file:\n",
        "    Task2h_A.append(file)\n",
        "   elif 'Task2s' in file:\n",
        "    Task2s_A.append(file)\n",
        "   elif 'Task3p' in file:\n",
        "    Task3_A.append(file)\n",
        "   elif 'Task4_P1' in file:\n",
        "    Task4_P1_A.append(file)\n",
        "   elif 'Task4_P2' in file:\n",
        "    Task4_P2_A.append(file)\n",
        "   elif 'Task4_P3' in file:\n",
        "    Task4_P3_A.append(file)\n",
        "   elif 'Task5p' in file:\n",
        "    Task5_A.append(file)\n",
        "\n",
        "# Task2h_A.sort(); Task2s_A.sort(); Task3_A.sort(); Task4_P1_A.sort(); Task4_P2_A.sort(); Task4_P3_A.sort() ; Task5_A.sort()\n",
        "print(len(Task2h_A), len(Task2s_A), len(Task3_A),len(Task4_P1_A), len(Task4_P2_A), len(Task4_P3_A),len(Task5_A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIOIxNeQJkTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af48ed15-4b3c-4d9d-cda3-4963577a7412"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "import shutil\n",
        "subject_id=['S03', 'S04', 'S05', 'S06', 'S07', 'S08', 'S10', 'S11', 'S13', 'S14', 'S15', 'S16', 'S17', 'S19', 'S20', 'S21', 'S22', 'S24', 'S25', 'S26', 'S28', 'S29', 'S30', 'S31', 'S32', 'S33', 'S34', 'S35', 'S36', 'S37', 'S38', 'S39', 'S40', 'S41', 'S42', 'S43']\n",
        "def move_files(file_paths, destination_directory):\n",
        "    i=0\n",
        "    for file_path in file_paths:\n",
        "        dest =os.path.join(destination_directory+'/'+ subject_id[i])\n",
        "        shutil.move(file_path, dest)\n",
        "        i+=1\n",
        "destination_directory = '/content/drive/MyDrive/Cambridge_Data/Modify_Audio_Data/Task5p_A'\n",
        "move_files(Task5_A, destination_directory)\n",
        "len(os.listdir(destination_directory))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBeu9fsIyxFN"
      },
      "source": [
        "# Data Preprocessing -2 (Used when you have preprocess data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoABNskzJesJ"
      },
      "outputs": [],
      "source": [
        "# Preprocess function, We can chnage this as per our requirement\n",
        "def preprocess(raw_data):\n",
        "    num, length = raw_data.shape\n",
        "    out_data = np.zeros((num, length))\n",
        "    for i in range(num):\n",
        "        out_data[i, :] = raw_data.iloc[i] - np.median(raw_data.iloc[i])\n",
        "    return out_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpsWkoO_MJ5B"
      },
      "outputs": [],
      "source": [
        "def getVideoFeature(feaVec):\n",
        "    feaNum = feaVec.shape[0]\n",
        "    videoFea = np.zeros((1, feaNum*20-6*6))\n",
        "    # print(videoFea.shape)\n",
        "    for i in range(feaNum):\n",
        "        videoFea[0, (i-1)*12 + 1] = np.mean(feaVec[i,:])\n",
        "        videoFea[0, (i-1)*12 + 2] = np.std(feaVec[i,:])\n",
        "        videoFea[0, (i-1)*12 + 3] = np.max(feaVec[i,:])\n",
        "        videoFea[0, (i-1)*12 + 4] = np.min(feaVec[i,:])\n",
        "        videoFea[0, (i-1)*12 + 5] = np.mean(np.diff(feaVec[i,:]))\n",
        "        videoFea[0, (i-1)*12 + 6] = np.std(np.diff(feaVec[i,:]))\n",
        "        videoFea[0, (i-1)*12 + 7] = np.max(np.diff(feaVec[i,:]))\n",
        "        videoFea[0, (i-1)*12 + 8] = np.min(np.diff(feaVec[i,:]))\n",
        "        a = feaVec[i,0:-2]\n",
        "        b = feaVec[i,2:]\n",
        "        videoFea[0, (i-1)*12 + 9] = np.mean(a-b)\n",
        "        videoFea[0, (i-1)*12 + 10] = np.std(a-b)\n",
        "        videoFea[0, (i-1)*12 + 11] = np.max(a-b)\n",
        "        videoFea[0, (i-1)*12 + 12] = np.min(a-b)\n",
        "    return videoFea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2h9VL4RhXwyM"
      },
      "outputs": [],
      "source": [
        "def flat_data(data, t_length, N):\n",
        "    flat_data = np.zeros(t_length)\n",
        "    row_num = data.shape[0]\n",
        "    for i in range(row_num):\n",
        "        flat_data[i * N:(i + 1) * N] = data[i, :]\n",
        "    return flat_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_U1XOLNQ5Bb"
      },
      "outputs": [],
      "source": [
        "def cut_videos(raw_data, fre_resolution):\n",
        "    num_frames = raw_data.shape[1]\n",
        "    # raw_data=raw_data.astype(int)\n",
        "    # print(raw_data.shape)\n",
        "    raw_num_keep_frame = np.floor(num_frames /fre_resolution)\n",
        "    num_keep_frame = raw_num_keep_frame * fre_resolution\n",
        "    num_delete = num_frames - num_keep_frame\n",
        "\n",
        "    # num_delete_start = np.ceil(num_delete / 2)\n",
        "    # num_delete_end = np.floor(num_delete / 2)\n",
        "    num_delete_start = num_delete // 2\n",
        "    num_delete_end = num_delete - num_delete_start\n",
        "    # print(raw_data.shape)\n",
        "\n",
        "    # Remove the specified number of frames from the beginning and end\n",
        "    raw_data = np.delete(raw_data, np.r_[:num_delete_start, num_frames - num_delete_end:].astype(int), axis=1)\n",
        "    # raw_data = np.delete(raw_data, np.concatenate((np.arange(0, num_delete_start), np.arange(num_frames - num_delete_end, num_frames))).astype(int), axis=1)\n",
        "\n",
        "    return raw_data, num_keep_frame, raw_num_keep_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTnl534Vg8ny"
      },
      "outputs": [],
      "source": [
        "# Plotting heat map\n",
        "def plot_heatmap(amp_map):\n",
        "  print(amp_map.shape)\n",
        "  print(amp_map)\n",
        "  dataplot=sns.heatmap(amp_map, cmap='Reds')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9CNTDnzzTNn"
      },
      "source": [
        "# Conversion of Time-series to spectral data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhgxzHSB2W7O"
      },
      "source": [
        "##First Frequency Alignment (Not using currently)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw92dSXuGQKs"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "# first frequency alignment method (select) - Zero- Padding\n",
        "def fourier_transform_select(all_data, N, num_multiple, fre_resolution):\n",
        "    channel_num, length = all_data.shape\n",
        "    amp_map = np.zeros((channel_num, length))\n",
        "    phase_map = np.zeros((channel_num, length))\n",
        "    common_temp_amp = np.zeros((channel_num, fre_resolution))\n",
        "    common_temp_pha = np.zeros((channel_num, fre_resolution))\n",
        "\n",
        "    for i in range(channel_num):\n",
        "        temp_contain = np.fft.fft(all_data[i,:])\n",
        "        amp_map[i,:] = np.abs(temp_contain/length)\n",
        "        phase_map[i,:] = np.angle(temp_contain)\n",
        "\n",
        "    for j in range(fre_resolution):\n",
        "        common_temp_amp[:,j] = amp_map[:,1+(j-1)*num_multiple]\n",
        "        common_temp_pha[:,j] = phase_map[:,1+(j-1)*num_multiple]\n",
        "\n",
        "    amp_map_return = np.column_stack((amp_map[:,int(length//2)+1], common_temp_amp[:,1:N]))\n",
        "    phase_map_return = np.column_stack((phase_map[:,int(length//2)+1], common_temp_pha[:,1:N]))\n",
        "\n",
        "    return amp_map_return, phase_map_return\n",
        "\n",
        "# demo for extracting spectral map and features using the first frequency alignment method (select)\n",
        "Primitive_num = 18\n",
        "N = 80\n",
        "fre_resolution = 256\n",
        "\n",
        "# pre_processing\n",
        "t_length = N * Primitive_num\n",
        "raw_data_1=pd.read_csv(\"/content/Cambridge_Data_Modify/s05_Task2 - Output.csv\")\n",
        "print(raw_data_1.shape)\n",
        "raw_data, num_keep_frame, num_multiple = cut_videos(raw_data_1, fre_resolution)\n",
        "processed_data_1 = preprocess(raw_data_1)\n",
        "processed_data_1 = processed_data_1.T\n",
        "print(processed_data_1.shape)\n",
        "# feature extraction\n",
        "sta_fea = getVideoFeature(processed_data_1)\n",
        "amp_map_1, phase_map_1 = fourier_transform_select(processed_data_1, N, int(num_multiple), fre_resolution)\n",
        "print(amp_map_1.shape, phase_map_1.shape)\n",
        "# amp_flat_data = flat_data(amp_map_1, t_length, N)\n",
        "# phase_flat_data = flat_data(phase_map_1, t_length, N)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sa4hFYjy2uk5"
      },
      "source": [
        "## Second Frequency Alignment ( Using Currently)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE2u2uCsKZIi"
      },
      "outputs": [],
      "source": [
        "# second frequency alignment method (resample)- extracts fixed-size spectral signals from variable length time-series data by choosing k common frequencies from\n",
        "# the spectral signals obtained from each video. In this case, the values of k chosen frequencies are obtained from the original signal rather than an extended signal\n",
        "def fourier_transform_resample(all_data, N, num_fre):\n",
        "    channel_num, length = all_data.shape\n",
        "    amp_map = np.zeros((channel_num, num_fre))\n",
        "    phase_map = np.zeros((channel_num, num_fre))\n",
        "    # print(all_data.shape, amp_map.shape, phase_map.shape)\n",
        "\n",
        "    for i in range(channel_num):\n",
        "        # print(np.fft.fft(all_data[i,:].shape))\n",
        "        temp_contain = np.fft.fft(all_data[i,:])\n",
        "        # print(temp_contain.ndim)\n",
        "        # print(temp_contain.shape)\n",
        "\n",
        "        if length % 2 == 0:\n",
        "            temp_contain = temp_contain[:length//2+1]\n",
        "        else:\n",
        "            temp_contain = temp_contain[:(length+1)//2]\n",
        "\n",
        "        temp_resample_data = resample(temp_contain, num_fre)\n",
        "        amp_map[i,:] = np.abs(temp_resample_data) / length\n",
        "        phase_map[i,:] = np.angle(temp_resample_data)\n",
        "\n",
        "    amp_map_return = amp_map[:, :N]\n",
        "    phase_map_return = phase_map[:, :N]\n",
        "\n",
        "    return amp_map_return, phase_map_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaHElRSTW3xu"
      },
      "outputs": [],
      "source": [
        "# Applying 2nd frequency alignment method to the csv file containing temporal data of 1 of the sbject for task1 and then coverting this data to spectral\n",
        "# representaion in form of spectral heatmap\n",
        "\n",
        "# pre_processing\n",
        "def generate_spectral_heatmap(dir_path,name, Primitive_num, K = 60, fre_resolution = 128):\n",
        "  t_length = K * Primitive_num\n",
        "  # raw_data = scipy.io.loadmat(file_name)['example_data']\n",
        "  # raw_data=pd.read_csv(\"/content/Cambridge_Data_Modify/s05_Task2 - Output.csv\")\n",
        "  filename=dir_path+'/'+ name\n",
        "  raw_data=pd.read_csv(filename, index_col=[0])\n",
        "  # print(raw_data.shape)\n",
        "  # print(raw_data.iloc[2]-np.median(raw_data.iloc[2]))\n",
        "  processed_data = preprocess(raw_data)\n",
        "  processed_data = processed_data.T\n",
        "\n",
        "  # feature extraction\n",
        "  sta_fea = getVideoFeature(processed_data)\n",
        "  amp_map, phase_map = fourier_transform_resample(processed_data,K, fre_resolution)\n",
        "  # plot_heatmap(amp_map)\n",
        "  # plot_heatmap(phase_map)\n",
        "  # amp_flat_data = amp_map.flatten()[:t_length]\n",
        "  # phase_flat_data = phase_map.flatten()[:t_length]\n",
        "  spectral_data= np.concatenate((amp_map, phase_map), axis=1)\n",
        "  # print(spectral_data.shape)\n",
        "  df_spectral_data=pd.DataFrame(spectral_data)\n",
        "  # print(df_spectral_data)\n",
        "  return df_spectral_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzhtWi2jaUKI"
      },
      "outputs": [],
      "source": [
        "def graph_visualization(G):\n",
        "  pos = nx.spring_layout(G)\n",
        "  plt.title(\"DGL Graph Visualization\")\n",
        "  nx.draw(G,pos, with_labels=True, node_color = 'green',node_size = 1500, font_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM2edhxQzdBk"
      },
      "source": [
        "# Necessary modules for Multi-dimenisonal Graph Geeneration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qRwVLgs5sGj"
      },
      "source": [
        "##MEFG Module Layers(GRATIS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNd2EHb2TLV0"
      },
      "outputs": [],
      "source": [
        "1# Classes of Layers used in MEFG module of GRATIS\n",
        "\"\"\"\n",
        "    ResGatedGCN: Residual Gated Graph ConvNets\n",
        "    An Experimental Study of Neural Networks for Variable Graphs (Xavier Bresson and Thomas Laurent, ICLR 2018)\n",
        "    https://arxiv.org/pdf/1711.07553v2.pdf\n",
        "\"\"\"\n",
        "\n",
        "class GatedGCNLayer(nn.Module):\n",
        "    \"\"\"\n",
        "        Param: []\n",
        "    \"\"\"\n",
        "    # input_dim= 120, output_dim= 240\n",
        "    def __init__(self, input_dim, output_dim, dropout, batch_norm, residual=False):\n",
        "        super().__init__()\n",
        "        self.in_channels = input_dim\n",
        "        self.out_channels = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.batch_norm = batch_norm\n",
        "        self.residual = residual\n",
        "\n",
        "        if input_dim != output_dim:\n",
        "            self.residual = False\n",
        "        #  self.residual = False\n",
        "        self.A = nn.Linear(input_dim, output_dim, bias=True)\n",
        "        self.B = nn.Linear(input_dim, output_dim, bias=True)\n",
        "        self.D = nn.Linear(input_dim, output_dim, bias=True)\n",
        "        self.E = nn.Linear(input_dim, output_dim, bias=True)\n",
        "        self.bn_node_h = nn.BatchNorm1d(output_dim)\n",
        "        self.bn_node_e = nn.BatchNorm1d(output_dim)\n",
        "        self.C = nn.Linear(input_dim, output_dim, bias=True)\n",
        "\n",
        "    def forward(self, g, h, e):\n",
        "\n",
        "        h_in = h # for residual connection\n",
        "        e_in = e # for residual connection\n",
        "        # print(\"node matrix shape :\", h.shape,\"edge matrix shape:\", e.shape)\n",
        "        g.ndata['h']  = h\n",
        "        # h=h.T\n",
        "        g.ndata['Ah'] = self.A(h)\n",
        "        g.ndata['Bh'] = self.B(h)\n",
        "        g.ndata['Dh'] = self.D(h)\n",
        "        g.ndata['Eh'] = self.E(h)\n",
        "        g.edata['e']  = e\n",
        "        # g.edata['Ce'] = self.C(e)\n",
        "\n",
        "        g.apply_edges(fn.u_add_v('Dh', 'Eh', 'DEh'))\n",
        "        # print(\"Size of DEh :\", g.edata['DEh'].shape)\n",
        "        # g.edata['e'] = g.edata['DEh'] + g.edata['Ce'] # Original code\n",
        "        g.edata['e'] = g.edata['DEh'] + e # Changed code excluding g.edata['Ce']\n",
        "        g.edata['sigma'] = torch.sigmoid(g.edata['e'])\n",
        "        g.update_all(fn.u_mul_e('Bh', 'sigma', 'm'), fn.sum('m', 'sum_sigma_h'))\n",
        "        g.update_all(fn.copy_e('sigma', 'm'), fn.sum('m', 'sum_sigma'))\n",
        "        g.ndata['h'] = g.ndata['Ah'] + g.ndata['sum_sigma_h'] / (g.ndata['sum_sigma'] + 1e-6)\n",
        "        #g.update_all(self.message_func,self.reduce_func)\n",
        "        h = g.ndata['h'] # result of graph convolution\n",
        "        e = g.edata['e'] # result of graph convolution\n",
        "        # print(\"h shape:\", h.shape,\"e shape :\", e.shape)\n",
        "\n",
        "        if self.batch_norm:\n",
        "            h = self.bn_node_h(h) # batch normalization\n",
        "            e = self.bn_node_e(e) # batch normalization\n",
        "\n",
        "        h = F.relu(h) # non-linear activation\n",
        "        e = F.relu(e) # non-linear activation\n",
        "\n",
        "        if self.residual:\n",
        "            h = h_in + h # residual connection\n",
        "            e = e_in + e # residual connection\n",
        "        h = F.dropout(h, self.dropout, training=self.training)\n",
        "        e = F.dropout(e, self.dropout, training=self.training)\n",
        "        # print(\"h shape:\", h.shape,\"e shape :\", e.shape)\n",
        "        return h, e\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}(in_channels={}, out_channels={})'.format(self.__class__.__name__,\n",
        "                                             self.in_channels,\n",
        "                                             self.out_channels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fIZshwsPRSQ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Attention with bool_masked_pos argument.\n",
        "'''\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(\n",
        "            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n",
        "            proj_drop=0., window_size=None, attn_head_dim=None):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        if attn_head_dim is not None:\n",
        "            head_dim = attn_head_dim\n",
        "        all_head_dim = head_dim * self.num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.q = nn.Linear(dim, all_head_dim, bias=False)\n",
        "        self.k = nn.Linear(dim, all_head_dim, bias=False)\n",
        "        self.v = nn.Linear(dim, all_head_dim, bias=False)\n",
        "\n",
        "        if qkv_bias:\n",
        "            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "            # self.k_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n",
        "        else:\n",
        "            self.q_bias = None\n",
        "            self.k_bias = None\n",
        "            self.v_bias = None\n",
        "\n",
        "\n",
        "        self.window_size = None\n",
        "        self.relative_position_bias_table = None\n",
        "        self.relative_position_index = None\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(all_head_dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x, bool_masked_pos=None, k=None, v=None):\n",
        "        B, N, C = x.shape\n",
        "\n",
        "        if k is None:\n",
        "            k = x\n",
        "            v = x\n",
        "            N_k = N\n",
        "            N_v = N\n",
        "        else:\n",
        "            N_k = k.shape[1]\n",
        "            N_v = v.shape[1]\n",
        "\n",
        "        q_bias, k_bias, v_bias = None, None, None\n",
        "        if self.q_bias is not None:\n",
        "            q_bias = self.q_bias\n",
        "            k_bias = torch.zeros_like(self.v_bias, requires_grad=False)\n",
        "            v_bias = self.v_bias\n",
        "\n",
        "        q = F.linear(input=x, weight=self.q.weight, bias=q_bias)                        # (B, N_q, dim)\n",
        "        k = F.linear(input=k, weight=self.k.weight, bias=k_bias)                        # (B, N_k, dim)\n",
        "        v = F.linear(input=v, weight=self.v.weight, bias=v_bias)\n",
        "\n",
        "        q = q.reshape(B, N, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4).squeeze(0)    # (B, num_heads, N_q, dim)\n",
        "        k = k.reshape(B, N_k, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4).squeeze(0)    # (B, num_heads, N_k, dim)\n",
        "        v = v.reshape(B, N_v, 1, self.num_heads, -1).permute(2, 0, 3, 1, 4).squeeze(0)    # (B, num_heads, N_v, dim)\n",
        "\n",
        "\n",
        "        q = q * self.scale\n",
        "        attn = (q @ k.transpose(-2, -1))      # (B, N_head, N, N)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Linear Transformer proposed in \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\"\n",
        "Modified from: https://github.com/idiap/fast-transformers/blob/master/fast_transformers/attention/linear_attention.py\n",
        "\"\"\"\n",
        "\n",
        "def elu_feature_map(x):\n",
        "    return torch.nn.functional.elu(x) + 1\n",
        "\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.feature_map = elu_feature_map\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, queries, keys, values, q_mask=None, kv_mask=None):\n",
        "        \"\"\" Multi-Head linear attention proposed in \"Transformers are RNNs\"\n",
        "        Args:\n",
        "            queries: [N, L, H, D]\n",
        "            keys: [N, S, H, D]\n",
        "            values: [N, S, H, D]\n",
        "            q_mask: [N, L]\n",
        "            kv_mask: [N, S]\n",
        "        Returns:\n",
        "            queried_values: (N, L, H, D)\n",
        "        \"\"\"\n",
        "        Q = self.feature_map(queries)\n",
        "        K = self.feature_map(keys)\n",
        "\n",
        "        # set padded position to zero\n",
        "        if q_mask is not None:\n",
        "            Q = Q * q_mask[:, :, None, None]\n",
        "        if kv_mask is not None:\n",
        "            K = K * kv_mask[:, :, None, None]\n",
        "            values = values * kv_mask[:, :, None, None]\n",
        "\n",
        "        v_length = values.size(1)\n",
        "        values = values / v_length  # prevent fp16 overflow\n",
        "        KV = torch.einsum(\"nshd,nshv->nhdv\", K, values)  # (S,D)' @ S,V\n",
        "        Z = 1 / (torch.einsum(\"nlhd,nhd->nlh\", Q, K.sum(dim=1)) + self.eps)\n",
        "        queried_values = torch.einsum(\"nlhd,nhdv,nlh->nlhv\", Q, KV, Z) * v_length\n",
        "\n",
        "        return queried_values.contiguous()\n",
        "\n",
        "\n",
        "class FullAttention(nn.Module):\n",
        "    def __init__(self, use_dropout=False, attention_dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.use_dropout = use_dropout\n",
        "        self.dropout = nn.Dropout(attention_dropout)\n",
        "\n",
        "    def forward(self, queries, keys, values, q_mask=None, kv_mask=None):\n",
        "        \"\"\" Multi-head scaled dot-product attention, a.k.a full attention.\n",
        "        Args:\n",
        "            queries: [N, L, H, D]\n",
        "            keys: [N, S, H, D]\n",
        "            values: [N, S, H, D]\n",
        "            q_mask: [N, L]\n",
        "            kv_mask: [N, S]\n",
        "        Returns:\n",
        "            queried_values: (N, L, H, D)\n",
        "        \"\"\"\n",
        "\n",
        "        # Compute the unnormalized attention and apply the masks\n",
        "        QK = torch.einsum(\"nlhd,nshd->nlsh\", queries, keys)\n",
        "        if kv_mask is not None:\n",
        "            QK.masked_fill_(~(q_mask[:, :, None, None] * kv_mask[:, None, :, None]), float('-inf'))\n",
        "\n",
        "        # Compute the attention and the weighted average\n",
        "        softmax_temp = 1. / queries.size(3)**.5  # sqrt(D)\n",
        "        A = torch.softmax(softmax_temp * QK, dim=2)\n",
        "        if self.use_dropout:\n",
        "            A = self.dropout(A)\n",
        "\n",
        "        queried_values = torch.einsum(\"nlsh,nshd->nlhd\", A, values)\n",
        "\n",
        "        return queried_values.contiguous()\n",
        "\n",
        "\n",
        "class CrossTransformerEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 nhead,\n",
        "                 attention='linear',\n",
        "                 drop = 0.1):\n",
        "        super(CrossTransformerEncoder, self).__init__()\n",
        "\n",
        "        self.dim = d_model // nhead\n",
        "        self.nhead = nhead\n",
        "\n",
        "        # multi-head attention\n",
        "        self.q_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.k_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.v_proj = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.attention = LinearAttention() if attention == 'linear' else FullAttention()\n",
        "        self.merge = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "        # feed-forward network\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model*2, d_model*2, bias=False),\n",
        "            #nn.Dropout(drop),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(d_model*2, d_model, bias=False),\n",
        "            nn.Dropout(drop),\n",
        "        )\n",
        "\n",
        "        # norm and dropout\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, source, x_mask=None, source_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x (torch.Tensor): [N, L, C]\n",
        "            source (torch.Tensor): [N, S, C]\n",
        "            x_mask (torch.Tensor): [N, L] (optional)\n",
        "            source_mask (torch.Tensor): [N, S] (optional)\n",
        "        \"\"\"\n",
        "        bs = x.size(0)\n",
        "        # print(\"shape of x:\", x.shape,\" shape of source:\",source.shape)\n",
        "        query, key, value = x, source, source\n",
        "        # print(\" shape of query:\", query.shape,\" shape of key:\", key.shape,\" shape of value:\", value.shape)\n",
        "        # multi-head attention\n",
        "        query = self.q_proj(query).view(bs, -1, self.nhead, self.dim)  # [N, L, (H, D)]\n",
        "        # print(\" query shape: [N, L, (H, D)]\", query.shape)\n",
        "        key = self.k_proj(key).view(bs, -1, self.nhead, self.dim)  # [N, S, (H, D)]\n",
        "        # print(\"shape of key should be [N, S, (H, D)] :\", key.shape)\n",
        "        value = self.v_proj(value).view(bs, -1, self.nhead, self.dim)\n",
        "        message = self.attention(query, key, value, q_mask=x_mask, kv_mask=source_mask)  # [N, L, (H, D)]\n",
        "        # print(\"message shape should be [N, L, (H, D)] :\", message.shape)\n",
        "        message = self.merge(message.view(bs, -1, self.nhead*self.dim))  # [N, L, C]\n",
        "        # print(\"message shape should be [N, L, C] :\", message.shape)\n",
        "        message = self.norm1(message)\n",
        "\n",
        "        # feed-forward network\n",
        "        message = self.mlp(torch.cat([x, message], dim=2))\n",
        "        message = self.norm2(message)\n",
        "\n",
        "        return x + message\n",
        "\n",
        "class CrossTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, nhead=1, layer_nums=1, attention_type='linear'):\n",
        "        super().__init__()\n",
        "\n",
        "        encoder_layer = CrossTransformerEncoder(d_model, nhead, attention_type)\n",
        "        self.VCR_layers = nn.ModuleList([encoder_layer for _ in range(layer_nums)])\n",
        "        self.VVR_layers = nn.ModuleList([encoder_layer for _ in range(layer_nums)])\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "    def forward(self, qfea, kfea, mask0=None, mask1=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            qfea (torch.Tensor): [B, N, D]\n",
        "            kfea (torch.Tensor): [B, D]\n",
        "            mask0 (torch.Tensor): [B, N] (optional)\n",
        "            mask1 (torch.Tensor): [B, N] (optional)\n",
        "        \"\"\"\n",
        "        #assert self.d_model == qfea.size(2), \"the feature number of src and transformer must be equal\"\n",
        "\n",
        "        B,N,D = qfea.shape\n",
        "        kfea = kfea.unsqueeze(1).repeat(1, N, 1) #[B,N,D]\n",
        "        # print(\" shape of kfea should be [B,N,D] :\", kfea.shape)\n",
        "\n",
        "        mask1 = torch.ones([B,N]).to(qfea.device)\n",
        "        for layer in self.VCR_layers:\n",
        "            qfea = layer(qfea, kfea, mask0, mask1) #[B,N,D]\n",
        "            # print(\" shape of qfea should be [B,N,D] :\", qfea.shape)\n",
        "            #kfea = layer(kfea, qfea, mask1, mask0)\n",
        "\n",
        "        qfea_end = qfea.repeat(1,1,N).view(B,-1,D) #[B,N*N,D]\n",
        "        # print(\" shape of qfea end should be [B,N*N,D] :\", qfea_end.shape)\n",
        "        qfea_start = qfea.repeat(1,N,1).view(B,-1,D) #[B,N*N,D]\n",
        "        # print(\" shape of qfea start should be [B,N*N,D] :\", qfea_start.shape)\n",
        "        #mask2 = mask0.repeat([1,N])\n",
        "        for layer in self.VVR_layers:\n",
        "            #qfea_start = layer(qfea_start, qfea_end, mask2, mask2)\n",
        "            qfea_start = layer(qfea_start, qfea_end)#[B,N*N,D]\n",
        "            # print(\" shape of qfea start should be [B,N*N,D] :\", qfea_start.shape)\n",
        "\n",
        "        return qfea_start.view([B,N,N,D])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMC45fJe16T7"
      },
      "source": [
        "## TTP Module (GRATIS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oh8YfuMF3wTW"
      },
      "outputs": [],
      "source": [
        "# The TTP code\n",
        "class TTP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, edge_thresh):\n",
        "        super().__init__()\n",
        "        self.proj_g1 = nn.Linear(in_dim,hidden_dim**2)\n",
        "        self.bn_node_lr_g1 = nn.BatchNorm1d(hidden_dim**2)\n",
        "        self.proj_g2 = nn.Linear(in_dim,hidden_dim)\n",
        "        self.bn_node_lr_g2 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.hidden_dim = hidden_dim #lr_g\n",
        "        self.proj_g = nn.Linear(hidden_dim, 1)\n",
        "        self.edge_thresh = edge_thresh\n",
        "    def forward(self, g, h, e):\n",
        "        lr_gs = []\n",
        "        gs = dgl.unbatch(g)\n",
        "        for g in gs:\n",
        "            N = g.number_of_nodes()\n",
        "            h_single = g.ndata['feature'].to(h.device)\n",
        "            h_proj1 = F.dropout(F.relu(self.bn_node_lr_g1(self.proj_g1(h_single))), 0.1, training=self.training).view(-1,self.hidden_dim)\n",
        "            h_proj2 = F.dropout(F.relu(self.bn_node_lr_g2(self.proj_g2(h_single))), 0.1, training=self.training).permute(1,0)\n",
        "            mm = torch.mm(h_proj1,h_proj2)\n",
        "            mm = mm.view(N,self.hidden_dim,-1).permute(0,2,1) #[N, N, D]\n",
        "            # print(\"Golbal Contextual Representation shape:\", mm.shape)\n",
        "\n",
        "            mm = self.proj_g(mm).squeeze(-1)\n",
        "            # print(\"Xh matrix shape:\", mm.shape)\n",
        "            diag_mm = torch.diag(mm)\n",
        "            diag_mm = torch.diag_embed(diag_mm)\n",
        "            mm -= diag_mm\n",
        "            #matrix = torch.sigmoid(mm)\n",
        "            #matrix = F.softmax(mm, dim=0)\n",
        "            matrix = F.softmax(mm, dim=0) * F.softmax(mm, dim=1)\n",
        "            # print(\"Adjacency probability matrix\",matrix.shape)\n",
        "            #binarized = BinarizedF()\n",
        "            #matrix = binarized.apply(matrix) #(0/1)\n",
        "            lr_connetion = torch.where(matrix>self.edge_thresh)\n",
        "            # print(lr_connetion[0], lr_connetion[1])\n",
        "            g.add_edges(lr_connetion[0], lr_connetion[1])\n",
        "            # print(\"Learned TTP Graph\", g)\n",
        "            lr_gs.append(g)\n",
        "        g = dgl.batch(lr_gs).to(h.device)\n",
        "\n",
        "        return g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkkVOylG3zCa"
      },
      "source": [
        "## MEFG Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdeADQdIPRLE"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "M =num_edges\n",
        "Din= input_dim = 120\n",
        "hidden_num= hidden dim =64\n",
        "Nmax= num_nodes = 31\n",
        "D= node_features =120\n",
        "B = Batch\n",
        "'''\n",
        "class MEFG(nn.Module):\n",
        "    def __init__(self, in_dim,hidden_dim, max_node_num, global_layer_num = 1, dropout = 0.1):\n",
        "        super().__init__()\n",
        "        self.edge_proj = nn.Conv1d(in_channels=2,out_channels=1,kernel_size=3,padding=1)\n",
        "        self.edge_proj2 = nn.Linear(in_dim,hidden_dim) #baseline4\n",
        "        self.edge_proj3 = nn.Linear(in_dim,hidden_dim)\n",
        "        self.edge_proj4 = nn.Linear(hidden_dim,hidden_dim)\n",
        "        self.hidden_dim = hidden_dim #baseline4\n",
        "        self.bn_node_lr_e = nn.BatchNorm1d(hidden_dim)\n",
        "\n",
        "        self.max_node_num = max_node_num\n",
        "        self.global_layers = nn.ModuleList([GatedGCNLayer(in_dim, hidden_dim, dropout,\n",
        "                                    True, True) for _ in range(global_layer_num -1) ])\n",
        "        self.global_layers.append(GatedGCNLayer(in_dim, hidden_dim, dropout, True, True))\n",
        "        #self.global_layers = nn.ModuleList([ ResidualAttentionBlock( d_model = hidden_dim, n_head = 1)\n",
        "        #                                    for _ in range(global_layer_num) ])\n",
        "        self.CrossT = CrossTransformer(hidden_dim, nhead=1, layer_nums=1, attention_type='linear')\n",
        "\n",
        "    def forward(self, g, h, e):\n",
        "\n",
        "        g.apply_edges(lambda edges: {'src' : edges.src['feature']})\n",
        "        src = g.edata['src'].unsqueeze(1) #[M,1,D]\n",
        "        # print(\"src shape:\", src.shape)\n",
        "        g.apply_edges(lambda edges: {'dst' : edges.dst['feature']})\n",
        "        dst = g.edata['dst'].unsqueeze(1) #[M,1,D]\n",
        "        # print(\"dst shape:\", dst.shape)\n",
        "        edge = torch.cat((src,dst),1).to(h.device) #[M,2,D] [num_edges,2,node_features]\n",
        "        # print(\"edge shape:\", edge.shape)\n",
        "        lr_e_local = self.edge_proj(edge).squeeze(1)#[M,D]\n",
        "        # print('lr_e_local shape:', lr_e_local.shape)\n",
        "        lr_e_local = self.edge_proj2(lr_e_local)  #[num_edges,hidden_dim]\n",
        "        # print('lr_e_local shape:', lr_e_local.shape)\n",
        "\n",
        "        hs = []\n",
        "        gs = dgl.unbatch(g)\n",
        "        # print(\"Graph g :\", g)\n",
        "        # print(\"Graph gs:\", gs)\n",
        "        mask0 = torch.zeros([len(gs),self.max_node_num]).to(h.device) #[1, num_nodes]\n",
        "        for i,g0 in enumerate(gs):\n",
        "            Ng = g0.number_of_nodes()\n",
        "            padding = nn.ConstantPad1d((0,self.max_node_num - Ng),0)\n",
        "            pad_h = padding(g0.ndata['feature'].T).T #[Nmax, D]\n",
        "            hs.append(pad_h.unsqueeze(0))\n",
        "            mask0[i,:Ng] = 1\n",
        "        hs = torch.cat(hs,0).to(h.device) #[B,Nmax,Din]\n",
        "        # print(\"vertex feature shape:\", hs.shape)\n",
        "        hs = self.edge_proj3(hs) #[B,Nmax,hidden_num]\n",
        "        # print(\"vertex feature shape 2:\", hs.shape)\n",
        "\n",
        "        if e is None:\n",
        "            e = torch.ones([g.number_of_edges() ,h.shape[-1]]).to(h.device)\n",
        "        # Gated-GCN for extract global feature\n",
        "        hs2g = h\n",
        "        # print(\"hs2g shape :\", hs2g.shape)\n",
        "        # print(\"e :\",e.shape)\n",
        "        # print(\"global layers :\", self.global_layers)\n",
        "        # print(\"length of global layers:\", len(self.global_layers))\n",
        "        for conv in self.global_layers:\n",
        "            # print(\"conv :\", conv)\n",
        "            hs2g, _ = conv(g, hs2g, e)\n",
        "        g.ndata['hs2g'] = hs2g\n",
        "        g.ndata['feature']=hs2g\n",
        "        # print(\"hs2g shape:\", hs2g.shape)\n",
        "        global_g = dgl.mean_nodes(g, 'hs2g') #[B,hidden_num]\n",
        "        # print(\"shape of global layer:\", global_g.shape)\n",
        "\n",
        "        '''\n",
        "        # Transformer for extract global feature\n",
        "        mask_t = mask0.unsqueeze(1)*mask0.unsqueeze(2)\n",
        "        mask_t = (mask_t==0)\n",
        "        #mask_t = None\n",
        "\n",
        "        hs2g = hs.permute((1,0,2))\n",
        "        for conv in self.global_layers:\n",
        "            hs2g = conv(hs2g, mask_t)\n",
        "        global_g = hs2g.permute((1,0,2)).mean(1) #[B,D]\n",
        "        '''\n",
        "        # hs ([B, MaxnumNode, Hidden_Num])\n",
        "        # global_g ([B, Hidden_Num])\n",
        "        # print(\"hs shape:\", hs.shape)\n",
        "        edge = self.CrossT(hs, global_g, mask0) #[B,N,N,D]\n",
        "        # print(\"edge shape :\", edge.shape)\n",
        "\n",
        "        index_edge = []\n",
        "        for i,g0 in enumerate(gs):\n",
        "            index_edge.append(edge[i, g0.all_edges()[0],g0.all_edges()[1],:])\n",
        "        index_edge = torch.cat(index_edge,0)\n",
        "\n",
        "        lr_e_global = self.edge_proj4(index_edge)\n",
        "\n",
        "        if e is not None:\n",
        "            e = e + lr_e_local + lr_e_global\n",
        "        else:\n",
        "            e = lr_e_local + lr_e_global\n",
        "#        lr_e = lr_e_local + lr_e_global\n",
        "        # bn=>relu=>dropout\n",
        "        # print(\"edge matrix shape :\", e.shape)\n",
        "        e = self.bn_node_lr_e(e)\n",
        "        e = F.relu(e)\n",
        "        e = F.dropout(e, 0.1, training=self.training)\n",
        "        # print(\"edge matrix shape 2 :\", e.shape)\n",
        "        g.edata['feature']=e\n",
        "        # print(\"graph in MEFG module:\", g)\n",
        "        # return e\n",
        "        return g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkpxQnGYkQW4"
      },
      "source": [
        "# Graph Formation and Graph Modification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqzosWHO8DPW"
      },
      "source": [
        "## Functions for Graph Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXZu3lJjg_17"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Defining Labels from values as per distribution\n",
        "values   labels\n",
        "0-2         0\n",
        "3-4         1\n",
        "5-19        2\n",
        "'''\n",
        "labels=[]\n",
        "values=[13,4,19,14,3,3,2,4,7,1,5,4,0,0,3,4,2,13,8,0,2,2,1,6,3,2,3,4,2,4,2,0,7,1,13,2]\n",
        "for i in values:\n",
        "  if (i<=2):\n",
        "        labels.append(0)\n",
        "  elif (i>=5):\n",
        "        labels.append(2)\n",
        "  else :\n",
        "        labels.append(1)\n",
        "y_train=torch.tensor(labels)\n",
        "# print(y_train)\n",
        "# print(len(set(labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v6v24tjZ9z-"
      },
      "outputs": [],
      "source": [
        "# Function to create a fully connected graph, defining edge and node features, and apply TTP module on it\n",
        "def create_basic_graph(df_spdata, num_nodes):\n",
        "  nx_G = nx.complete_graph(num_nodes)\n",
        "  g = dgl.from_networkx(nx_G)\n",
        "  # Adding node fatures to each node from the dataframe containing spectral representaion of each feature\n",
        "  node_features = torch.tensor(df_spdata.values, dtype=torch.float32)\n",
        "  # Set node features in the DGL graph\n",
        "  g.ndata['feature'] = node_features\n",
        "  g.edata['feature'] = torch.rand(g.num_edges(), 1)\n",
        "  batch_graphs = g\n",
        "  batch_x = batch_graphs.ndata['feature']\n",
        "  batch_e = batch_graphs.edata['feature']\n",
        "  # print('Graph before TTP',batch_graphs)\n",
        "  ttp = TTP(in_dim = 120, hidden_dim = 240, edge_thresh=0.2)\n",
        "  lr_g = ttp(batch_graphs, batch_x, batch_e)\n",
        "  return lr_g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIXN7dWlGJNu"
      },
      "outputs": [],
      "source": [
        "# Function to apply MEFG module on a TTP graph\n",
        "def apply_MEFG(g, num_nodes):\n",
        "  batch_graphs = g\n",
        "  batch_x = batch_graphs.ndata['feature']\n",
        "  batch_e = batch_graphs.edata['feature']\n",
        "  # print(\"batch_e shape:\", batch_e.shape)\n",
        "  # batch_x = increase_dim(batch_graphs,batch_x)\n",
        "  mefg=MEFG(in_dim=120,hidden_dim=240, max_node_num= num_nodes, global_layer_num = 1, dropout = 0.1)\n",
        "  # e_afterMEFG = mefg(batch_graphs, batch_x, batch_e)\n",
        "  # return e_afterMEFG\n",
        "  graph_afterMEFG = mefg(batch_graphs, batch_x, batch_e)\n",
        "  return graph_afterMEFG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvd98W1b0bVo"
      },
      "source": [
        "## Graph for Video Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wYUo9sX0klD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ab15366-a873-4e8f-b6d8-0f5dccd4c668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n"
          ]
        }
      ],
      "source": [
        "# ### Video\n",
        "#  Late Function to create list of graphs obatined after application of TTP and MEFG module sequentially on a Fully connected graph\n",
        "G_list_MEFG_V=[]\n",
        "num_nodes = 31\n",
        "def create_graph_list_V(task_dir,csv_files):\n",
        "  i=0\n",
        "  for file in csv_files:\n",
        "    if i< 37:\n",
        "      df_file=generate_spectral_heatmap(task_dir,file, Primitive_num = num_nodes, K = 60, fre_resolution = 128)\n",
        "      graph_afterTTP= create_basic_graph(df_file, num_nodes)\n",
        "      # print(\"TTP module output before MEFG:\",graph_afterTTP)\n",
        "      # G_list.append(graph_afterTTP)\n",
        "      # e_afterMEFG=apply_MEFG(graph_afterTTP)\n",
        "      graph_afterMEFG=apply_MEFG(graph_afterTTP, num_nodes)\n",
        "      # print(\" MEFG module ouput graph multi-dimensional edges shape:\", e_afterMEFG.shape)\n",
        "      # print(\"TTP module output after MEFG:\",graph_afterTTP)\n",
        "      # print(\"TTP module output after MEFG:\",graph_afterMEFG)\n",
        "      # graph_afterTTP.edata['feature']= e_afterMEFG\n",
        "      # print(\"TTP module output after MEFG 2:\",graph_afterTTP)\n",
        "      # G_list.append(graph_afterTTP)\n",
        "      G_list_MEFG_V.append(graph_afterMEFG)\n",
        "      i+=1\n",
        "      print(i)\n",
        "\n",
        "# Defining Task directory and creating list of graphs\n",
        "task_dir_V=\"/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_2_h\"\n",
        "task_csv_files_V=os.listdir(task_dir_V)\n",
        "task_csv_files_V.sort()\n",
        "create_graph_list_V(task_dir_V,task_csv_files_V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NopeS8i61DWq"
      },
      "outputs": [],
      "source": [
        "# # Generated list containing graph data\n",
        "# print(G_list_MEFG_V)\n",
        "# print('size of list is:',len(G_list_MEFG_V))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wNg5CWB2Uto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7d03ec-5799-46cd-ea17-04527cc9215c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Data(x=[31, 240], edge_index=[2, 961], edge_attr=[961, 240], y=2),\n",
              " Data(x=[31, 240], edge_index=[2, 934], edge_attr=[934, 240], y=1),\n",
              " Data(x=[31, 240], edge_index=[2, 933], edge_attr=[933, 240], y=2),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=2),\n",
              " Data(x=[31, 240], edge_index=[2, 961], edge_attr=[961, 240], y=1),\n",
              " Data(x=[31, 240], edge_index=[2, 961], edge_attr=[961, 240], y=1),\n",
              " Data(x=[31, 240], edge_index=[2, 961], edge_attr=[961, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 961], edge_attr=[961, 240], y=1),\n",
              " Data(x=[31, 240], edge_index=[2, 933], edge_attr=[933, 240], y=2),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=2),\n",
              " Data(x=[31, 240], edge_index=[2, 933], edge_attr=[933, 240], y=1),\n",
              " Data(x=[31, 240], edge_index=[2, 947], edge_attr=[947, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=1),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=1),\n",
              " Data(x=[31, 240], edge_index=[2, 960], edge_attr=[960, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=2),\n",
              " Data(x=[31, 240], edge_index=[2, 961], edge_attr=[961, 240], y=2),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 931], edge_attr=[931, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 933], edge_attr=[933, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=2),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=1),\n",
              " Data(x=[31, 240], edge_index=[2, 961], edge_attr=[961, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=1),\n",
              " Data(x=[31, 240], edge_index=[2, 961], edge_attr=[961, 240], y=1),\n",
              " Data(x=[31, 240], edge_index=[2, 961], edge_attr=[961, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=1),\n",
              " Data(x=[31, 240], edge_index=[2, 933], edge_attr=[933, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 961], edge_attr=[961, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=2),\n",
              " Data(x=[31, 240], edge_index=[2, 933], edge_attr=[933, 240], y=0),\n",
              " Data(x=[31, 240], edge_index=[2, 933], edge_attr=[933, 240], y=2),\n",
              " Data(x=[31, 240], edge_index=[2, 932], edge_attr=[932, 240], y=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "'''\n",
        "Create a dataset by converting each graph into PyG Data Object\n",
        "node features                 x\n",
        "edge Adjacency Matrix    edge_index\n",
        "edge features             edge_attr\n",
        "labels                        y\n",
        "'''\n",
        "full_dataset = []\n",
        "n = 36\n",
        "for idx in range(0 ,n):\n",
        "  #edge_index_COO = torch.sparse_coo_tensor(indices=torch.stack([G_list_MEFG[idx].all_edges()[0], G_list_MEFG[idx].all_edges()[1]], dim =0), values=torch.ones_like(G_list_MEFG[idx].all_edges()[0]), size=(31, 31))\n",
        "  data=Data(x=G_list_MEFG_V[idx].ndata['feature'],  edge_index=torch.stack([G_list_MEFG_V[idx].all_edges()[0], G_list_MEFG_V[idx].all_edges()[1]], dim=0) ,edge_attr=G_list_MEFG_V[idx].edata['feature'],y=y_train[idx])\n",
        "  full_dataset.append(data)\n",
        "full_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtDHjlTo0m7c"
      },
      "source": [
        "## Graph for Audio Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS743jIGUZlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f859b4a4-cb41-4126-f876-33ba925a9eba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8192,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8192,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "1\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8207,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8207,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "2\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8281,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8281,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "3\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8281,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8281,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "4\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8195,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8195,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "5\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8192,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8192,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "6\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8281,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8281,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "7\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8192,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8192,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "8\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8192,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8192,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "9\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8193,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8193,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "10\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8195,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8195,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "11\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8202,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8202,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "12\n",
            "TTP module output before MEFG: Graph(num_nodes=91, num_edges=8281,\n",
            "      ndata_schemes={'feature': Scheme(shape=(120,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(1,), dtype=torch.float32)})\n",
            "TTP module output after MEFG: Graph(num_nodes=91, num_edges=8281,\n",
            "      ndata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'h': Scheme(shape=(240,), dtype=torch.float32), 'Ah': Scheme(shape=(240,), dtype=torch.float32), 'Bh': Scheme(shape=(240,), dtype=torch.float32), 'Dh': Scheme(shape=(240,), dtype=torch.float32), 'Eh': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma_h': Scheme(shape=(240,), dtype=torch.float32), 'sum_sigma': Scheme(shape=(240,), dtype=torch.float32), 'hs2g': Scheme(shape=(240,), dtype=torch.float32)}\n",
            "      edata_schemes={'feature': Scheme(shape=(240,), dtype=torch.float32), 'src': Scheme(shape=(120,), dtype=torch.float32), 'dst': Scheme(shape=(120,), dtype=torch.float32), 'e': Scheme(shape=(240,), dtype=torch.float32), 'DEh': Scheme(shape=(240,), dtype=torch.float32), 'sigma': Scheme(shape=(240,), dtype=torch.float32)})\n",
            "13\n"
          ]
        }
      ],
      "source": [
        "# Function to create list of graphs obatined after application of TTP and MEFG module sequentially on a Fully connected graph\n",
        "G_list=[]\n",
        "G_list_MEFG_A=[]\n",
        "num_nodes = 91\n",
        "def create_graph_list_A(task_dir,csv_files):\n",
        "  i=0\n",
        "  for file in csv_files:\n",
        "    if i< 37:\n",
        "      df_file=generate_spectral_heatmap(task_dir,file, Primitive_num = num_nodes, K = 60, fre_resolution = 128)\n",
        "      graph_afterTTP= create_basic_graph(df_file, num_nodes)\n",
        "      print(\"TTP module output before MEFG:\",graph_afterTTP)\n",
        "      # G_list.append(graph_afterTTP)\n",
        "      # e_afterMEFG=apply_MEFG(graph_afterTTP)\n",
        "      graph_afterMEFG=apply_MEFG(graph_afterTTP, num_nodes)\n",
        "      # print(\" MEFG module ouput graph multi-dimensional edges shape:\", e_afterMEFG.shape)\n",
        "      # print(\"TTP module output after MEFG:\",graph_afterTTP)\n",
        "      print(\"TTP module output after MEFG:\",graph_afterMEFG)\n",
        "      # graph_afterTTP.edata['feature']= e_afterMEFG\n",
        "      # print(\"TTP module output after MEFG 2:\",graph_afterTTP)\n",
        "      # G_list.append(graph_afterTTP)\n",
        "      G_list_MEFG_A.append(graph_afterMEFG)\n",
        "      i+=1\n",
        "      print(i)\n",
        "\n",
        "# Defining Task directory and creating list of graphs\n",
        "task_dir_A=\"/content/drive/MyDrive/Cambridge_Data/Modify_Audio_Data/Task5p_A\"\n",
        "task_csv_files_A=os.listdir(task_dir_A)\n",
        "task_csv_files_A.sort()\n",
        "create_graph_list_A(task_dir_A,task_csv_files_A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkO4GP9S1OKB"
      },
      "outputs": [],
      "source": [
        "# Generated list containing graph data\n",
        "print(G_list_MEFG_A)\n",
        "print('size of list is:',len(G_list_MEFG_A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsWLsN8N2hTq"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create a dataset by converting each graph into PyG Data Object\n",
        "node features                 x\n",
        "edge Adjacency Matrix    edge_index\n",
        "edge features             edge_attr\n",
        "labels                        y\n",
        "'''\n",
        "full_dataset = []\n",
        "n = 36\n",
        "for idx in range(0 ,n):\n",
        "  #edge_index_COO = torch.sparse_coo_tensor(indices=torch.stack([G_list_MEFG[idx].all_edges()[0], G_list_MEFG[idx].all_edges()[1]], dim =0), values=torch.ones_like(G_list_MEFG[idx].all_edges()[0]), size=(31, 31))\n",
        "  data=Data(x=G_list_MEFG_A[idx].ndata['feature'],  edge_index=torch.stack([G_list_MEFG_A[idx].all_edges()[0], G_list_MEFG_A[idx].all_edges()[1]], dim=0) ,edge_attr=G_list_MEFG_A[idx].edata['feature'],y=y_train[idx])\n",
        "  full_dataset.append(data)\n",
        "full_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKW8QJ4B1RNT"
      },
      "source": [
        "## Graph for Early fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U12_coH1SGnb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "00ae7b28-075c-4d81-f046-5bf8fe7c0938"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Cambridge_Data/Modify_Audio_Data/Task2s_A'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-badf359906d8>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mtask_dir_A\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Cambridge_Data/Modify_Audio_Data/Task2s_A\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mtask_csv_files_V\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_dir_V\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtask_csv_files_A\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_dir_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mcreate_graph_list_EarlyFusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_dir_V\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask_csv_files_V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_dir_A\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask_csv_files_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Cambridge_Data/Modify_Audio_Data/Task2s_A'"
          ]
        }
      ],
      "source": [
        "# Early fusion\n",
        "G_list=[]\n",
        "G_list_MEFG_Early=[]\n",
        "num_nodes = 122\n",
        "def create_graph_list_EarlyFusion(task_dir_V,task_csv_files_V, task_dir_A,task_csv_files_A):\n",
        "  i=0\n",
        "  for i in range (36):\n",
        "      df_file_V=generate_spectral_heatmap(task_dir_V,task_csv_files_V[i], Primitive_num = 31, K = 60, fre_resolution = 128)\n",
        "      df_file_A=generate_spectral_heatmap(task_dir_A,task_csv_files_A[i], Primitive_num = 31, K = 60, fre_resolution = 128)\n",
        "      df_concat = pd.concat([df_file_V, df_file_A], axis=0)\n",
        "      print(df_concat.shape)\n",
        "      graph_afterTTP= create_basic_graph(df_concat, num_nodes)\n",
        "      print(\"TTP module output before MEFG:\",graph_afterTTP)\n",
        "      # G_list.append(graph_afterTTP)\n",
        "      # e_afterMEFG=apply_MEFG(graph_afterTTP)\n",
        "      graph_afterMEFG=apply_MEFG(graph_afterTTP, num_nodes)\n",
        "      # print(\" MEFG module ouput graph multi-dimensional edges shape:\", e_afterMEFG.shape)\n",
        "      # print(\"TTP module output after MEFG:\",graph_afterTTP)\n",
        "      print(\"TTP module output after MEFG:\",graph_afterMEFG)\n",
        "      # graph_afterTTP.edata['feature']= e_afterMEFG\n",
        "      # print(\"TTP module output after MEFG 2:\",graph_afterTTP)\n",
        "      # G_list.append(graph_afterTTP)\n",
        "      G_list_MEFG_Early.append(graph_afterMEFG)\n",
        "      i+=1\n",
        "      print(i)\n",
        "\n",
        "# Defining Task directory and creating list of graphs\n",
        "task_dir_V=\"/content/drive/MyDrive/Cambridge_Data/Modify_Data/Task_2_s\"\n",
        "task_dir_A=\"/content/drive/MyDrive/Cambridge_Data/Modify_Audio_Data/Task2s_A\"\n",
        "task_csv_files_V=os.listdir(task_dir_V)\n",
        "task_csv_files_A=os.listdir(task_dir_A)\n",
        "create_graph_list_EarlyFusion(task_dir_V,task_csv_files_V, task_dir_A,task_csv_files_A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dd1nu54y1jim"
      },
      "outputs": [],
      "source": [
        "# Generated list containing graph data\n",
        "print(G_list_MEFG_Early)\n",
        "print('size of list is:',len(G_list_MEFG_Early))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elZkO1EO2kOM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create a dataset by converting each graph into PyG Data Object\n",
        "node features                 x\n",
        "edge Adjacency Matrix    edge_index\n",
        "edge features             edge_attr\n",
        "labels                        y\n",
        "'''\n",
        "full_dataset = []\n",
        "n = 36\n",
        "for idx in range(0 ,n):\n",
        "  #edge_index_COO = torch.sparse_coo_tensor(indices=torch.stack([G_list_MEFG[idx].all_edges()[0], G_list_MEFG[idx].all_edges()[1]], dim =0), values=torch.ones_like(G_list_MEFG[idx].all_edges()[0]), size=(31, 31))\n",
        "  data=Data(x=G_list_MEFG_Early[idx].ndata['feature'],  edge_index=torch.stack([G_list_MEFG_Early[idx].all_edges()[0], G_list_MEFG_Early[idx].all_edges()[1]], dim=0) ,edge_attr=G_list_MEFG_Early[idx].edata['feature'],y=y_train[idx])\n",
        "  full_dataset.append(data)\n",
        "full_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yT8ZtneK12q8"
      },
      "source": [
        "## Graph for Late Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bBTpyBRF77Z"
      },
      "outputs": [],
      "source": [
        "# Defining Task directory and creating list of graphs\n",
        "task_dir_V=\"/content/drive/MyDrive/Cambridge_Data/Modify_Video_Data/Task_5\"\n",
        "task_dir_A=\"/content/drive/MyDrive/Cambridge_Data/Modify_Audio_Data/Task5p_A\"\n",
        "task_csv_files_V=os.listdir(task_dir_V)\n",
        "task_csv_files_A=os.listdir(task_dir_A)\n",
        "task_csv_files_V.sort()\n",
        "task_csv_files_A.sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpafAn4mXYid"
      },
      "outputs": [],
      "source": [
        "# ### Video\n",
        "#  Late Function to create list of graphs obatined after application of TTP and MEFG module sequentially on a Fully connected graph\n",
        "G_list_MEFG_V=[]\n",
        "num_nodes = 31\n",
        "def create_graph_list_V(task_dir,csv_files):\n",
        "  i=0\n",
        "  for file in csv_files:\n",
        "    if i< 37:\n",
        "      df_file=generate_spectral_heatmap(task_dir,file, Primitive_num = num_nodes, K = 60, fre_resolution = 128)\n",
        "      graph_afterTTP= create_basic_graph(df_file, num_nodes)\n",
        "      print(\"TTP module output before MEFG:\",graph_afterTTP)\n",
        "      # G_list.append(graph_afterTTP)\n",
        "      # e_afterMEFG=apply_MEFG(graph_afterTTP)\n",
        "      graph_afterMEFG=apply_MEFG(graph_afterTTP, num_nodes)\n",
        "      # print(\" MEFG module ouput graph multi-dimensional edges shape:\", e_afterMEFG.shape)\n",
        "      # print(\"TTP module output after MEFG:\",graph_afterTTP)\n",
        "      print(\"TTP module output after MEFG:\",graph_afterMEFG)\n",
        "      # graph_afterTTP.edata['feature']= e_afterMEFG\n",
        "      # print(\"TTP module output after MEFG 2:\",graph_afterTTP)\n",
        "      # G_list.append(graph_afterTTP)\n",
        "      G_list_MEFG_V.append(graph_afterMEFG)\n",
        "      i+=1\n",
        "      print(i)\n",
        "\n",
        "create_graph_list_V(task_dir_V,task_csv_files_V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri5tvxdm2HSb"
      },
      "outputs": [],
      "source": [
        "# Generated list containing graph data\n",
        "print(G_list_MEFG_V)\n",
        "print('size of list is:',len(G_list_MEFG_V))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NEQVgcC22JM"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create a dataset by converting each graph into PyG Data Object\n",
        "node features                 x\n",
        "edge Adjacency Matrix    edge_index\n",
        "edge features             edge_attr\n",
        "labels                        y\n",
        "'''\n",
        "full_dataset_V = []\n",
        "n = 36\n",
        "for idx in range(0 ,n):\n",
        "  #edge_index_COO = torch.sparse_coo_tensor(indices=torch.stack([G_list_MEFG[idx].all_edges()[0], G_list_MEFG[idx].all_edges()[1]], dim =0), values=torch.ones_like(G_list_MEFG[idx].all_edges()[0]), size=(31, 31))\n",
        "  data=Data(x=G_list_MEFG_V[idx].ndata['feature'],  edge_index=torch.stack([G_list_MEFG_V[idx].all_edges()[0], G_list_MEFG_V[idx].all_edges()[1]], dim=0) ,edge_attr=G_list_MEFG_V[idx].edata['feature'],y=y_train[idx])\n",
        "  full_dataset_V.append(data)\n",
        "full_dataset_V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpek1YFGXlol"
      },
      "outputs": [],
      "source": [
        "# ## Audio\n",
        "# Late Function to create list of graphs obatined after application of TTP and MEFG module sequentially on a Fully connected graph\n",
        "G_list_MEFG_A=[]\n",
        "num_nodes = 91\n",
        "def create_graph_list_A(task_dir,csv_files):\n",
        "  i=0\n",
        "  for file in csv_files:\n",
        "    if i< 37:\n",
        "      df_file=generate_spectral_heatmap(task_dir,file, Primitive_num = num_nodes, K = 60, fre_resolution = 128)\n",
        "      graph_afterTTP= create_basic_graph(df_file, num_nodes)\n",
        "      print(\"TTP module output before MEFG:\",graph_afterTTP)\n",
        "      # G_list.append(graph_afterTTP)\n",
        "      # e_afterMEFG=apply_MEFG(graph_afterTTP)\n",
        "      graph_afterMEFG=apply_MEFG(graph_afterTTP, num_nodes)\n",
        "      # print(\" MEFG module ouput graph multi-dimensional edges shape:\", e_afterMEFG.shape)\n",
        "      # print(\"TTP module output after MEFG:\",graph_afterTTP)\n",
        "      print(\"TTP module output after MEFG:\",graph_afterMEFG)\n",
        "      # graph_afterTTP.edata['feature']= e_afterMEFG\n",
        "      # print(\"TTP module output after MEFG 2:\",graph_afterTTP)\n",
        "      # G_list.append(graph_afterTTP)\n",
        "      G_list_MEFG_A.append(graph_afterMEFG)\n",
        "      i+=1\n",
        "      print(i)\n",
        "\n",
        "create_graph_list_A(task_dir_A,task_csv_files_A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNBOYVcXijGu"
      },
      "outputs": [],
      "source": [
        "# Generated list containing graph data\n",
        "print(G_list_MEFG_A)\n",
        "print('size of list is:',len(G_list_MEFG_A))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMVy_4rS2xda"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Create a dataset by converting each graph into PyG Data Object\n",
        "node features                 x\n",
        "edge Adjacency Matrix    edge_index\n",
        "edge features             edge_attr\n",
        "labels                        y\n",
        "'''\n",
        "full_dataset_A = []\n",
        "n = 36\n",
        "for idx in range(0 ,n):\n",
        "  #edge_index_COO = torch.sparse_coo_tensor(indices=torch.stack([G_list_MEFG[idx].all_edges()[0], G_list_MEFG[idx].all_edges()[1]], dim =0), values=torch.ones_like(G_list_MEFG[idx].all_edges()[0]), size=(31, 31))\n",
        "  data=Data(x=G_list_MEFG_A[idx].ndata['feature'],  edge_index=torch.stack([G_list_MEFG_A[idx].all_edges()[0], G_list_MEFG_A[idx].all_edges()[1]], dim=0) ,edge_attr=G_list_MEFG_A[idx].edata['feature'],y=y_train[idx])\n",
        "  full_dataset_A.append(data)\n",
        "full_dataset_A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJxsWg4mYoXv"
      },
      "outputs": [],
      "source": [
        "# print(G_list_MEFG[0].edata['feature'])\n",
        "# print(G_list_MEFG[0].edata['feature'].shape)\n",
        "# print(G_list_MEFG[0].ndata['feature'].shape)\n",
        "# print(len(G_list_MEFG))\n",
        "# # print(G_list_MEFG[0].nodes())\n",
        "# # print(G_list_MEFG[0].edges())\n",
        "# print(\"source nodes\",len(G_list_MEFG[0].all_edges()[0]))\n",
        "# print(\"destibation nodes\",len(G_list_MEFG[0].all_edges()[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsHrTC5P0IwN"
      },
      "outputs": [],
      "source": [
        "# indices=torch.stack([G_list[0].all_edges()[0], G_list[0].all_edges()[1]] , dim = 1 )\n",
        "# values=torch.ones_like(G_list[0].all_edges()[0] )\n",
        "\n",
        "#edge_index_COO = sparse.cat(indices , values )\n",
        "#edge_index_COO = torch.sparse_coo_tensor(indices=G_list[0].all_edges()[0], values=torch.ones_like(G_list[0].all_edges()[0]), size=(31, 31) )\n",
        "#edge_index_COO = torch.sparse_coo_tensor(indices=torch.stack([G_list[0].all_edges()[0], G_list[0].all_edges()[1]] , dim =0 ), values=torch.ones_like(G_list[0].all_edges()[0] ), size=(31, 31) )\n",
        "# edge_index_COO=to_torch_coo_tensor(G_list[0].all_edges())\n",
        "# edge_index = torch.stack([G_list[0].all_edges()[0], G_list[0].all_edges()[1]], dim=0)\n",
        "# edge_index_coo = torch.sparse_coo_tensor(edge_index.t(), torch.ones_like(G_list[0].all_edges()[0]), (36, 36))\n",
        "# edge_index_COO = torch.sparse_coo_tensor(G_list[0].all_edges()[0],G_list[0].all_edges()[1], (932,932))\n",
        "#print(edge_index_COO)\n",
        "#data=Data(x=G_list[0].ndata['feature'],  edge_index=edge_index_COO ,edge_attr=G_list[0].edata['feature'],y=y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1exk95y2GK6"
      },
      "source": [
        "# Graphs Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avPI6_f5iLrP"
      },
      "source": [
        "# GCN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9S5nmJ04Zgi"
      },
      "outputs": [],
      "source": [
        "# Defining GCN class\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, node_features, hidden_channels, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(node_features, hidden_channels)\n",
        "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
        "        # self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
        "        self.lin = torch.nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, data, dropout_rate):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        # x = F.dropout(x, p=dropout_rate, training=self.training)\n",
        "        # x = self.conv3(x, edge_index).relu()\n",
        "        x = global_mean_pool(x, batch=data.batch)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "# model = GCN(hidden_channels)\n",
        "# print(model)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop for GCN"
      ],
      "metadata": {
        "id": "uwaIAQf3hS8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same training loop is for Video, Audio and Early Fusion. Different for Late fusion only"
      ],
      "metadata": {
        "id": "aNfQpqFLhki9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(train_loader, dr, optimizer, criterion, model_GCN, scheduler):\n",
        "  loss_train_per = []\n",
        "  accuracy_train = []\n",
        "  model_GCN.train()\n",
        "  for data in train_loader :\n",
        "        optimizer.zero_grad()\n",
        "        output_train =  model_GCN(data, dropout_rate= dr)\n",
        "        target_train = data.y  # Assuming your target labels are stored in 'y'\n",
        "        loss = criterion(output_train, target_train)\n",
        "        loss.backward(retain_graph= True)\n",
        "        optimizer.step()\n",
        "        predicted_train = output_train.argmax(dim=1)\n",
        "        accuracy_train.append(accuracy_score(target_train.cpu().numpy(), predicted_train.cpu().numpy()))\n",
        "        loss_train_per.append(loss.item())\n",
        "  loss_train_total= np.mean(loss_train_per)\n",
        "  train_accuracy= np.mean(accuracy_train)\n",
        "  if scheduler is not None:\n",
        "        scheduler.step()  # Update learning rate\n",
        "  return train_accuracy, loss_train_total,model_GCN, optimizer"
      ],
      "metadata": {
        "id": "1ka3eqOhhYSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_epoch(test_loader, dr, criterion,model_GCN):\n",
        "  val_loss_per=[]\n",
        "  accuracy_test=[]\n",
        "  model_GCN.eval()\n",
        "  with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "          output_test =  model_GCN(data, dropout_rate= dr)\n",
        "          predicted_test = output_test.argmax(dim=1)\n",
        "          target_test = data.y\n",
        "          accuracy_test.append(accuracy_score(target_test.cpu().numpy(), predicted_test.cpu().numpy()))\n",
        "          loss = criterion(output_test, target_test)\n",
        "          val_loss_per.append(loss.item())\n",
        "  val_loss_total= np.mean(val_loss_per)\n",
        "  val_accuracy= np.mean(accuracy_test)\n",
        "  return val_accuracy, val_loss_total"
      ],
      "metadata": {
        "id": "09Y0vQIKhcP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_valid_loop_ADAM(lr, dr, hidden_classes, trial_subdir, plot_subdir, task_name):\n",
        "    num_epochs = 20\n",
        "    best_epoch = -1\n",
        "    best_stop_epoch=-1\n",
        "    early_stop_count = 0\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0\n",
        "    patience = 3\n",
        "    batch_size = 1\n",
        "    all_train_losses = [[] for _ in range(36)]\n",
        "    all_train_accuracies = [[] for _ in range(36)]\n",
        "    avg_train_losses=[] # Average of all training losses across all subjects per epoch\n",
        "    avg_train_accuracies=[] # Average of all training accuracies across all subjects per epoch\n",
        "    all_val_losses = []\n",
        "    all_val_accuracies = []\n",
        "\n",
        "    # GCN MODEL with 2 layers in architecture\n",
        "    model_GCN = [ GCN(node_features= 240,hidden_channels=hidden_classes, num_classes=3).to(device) for _ in range(36)]\n",
        "    optimizer = [torch.optim.Adam(model_GCN[_].parameters(), lr= lr) for _ in range(36)]\n",
        "    scheduler = [StepLR(optimizer[_], step_size=1, gamma=0.9) for _ in range(36)] # decrease LR by a factor of 0.9 every epoch\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_subdir = os.path.join(trial_subdir, f\"epoch_{epoch}\")\n",
        "        os.makedirs(epoch_subdir, exist_ok=True)\n",
        "        # print(f\"## epoch {epoch}  ##\")\n",
        "        val_accuracies =[]\n",
        "        val_losses=[]\n",
        "        for idx in range(36):\n",
        "            # DataLoader setup\n",
        "            # print(f\"## Subject {idx} is test subject  ##\")\n",
        "            train_indices = list(range(36))\n",
        "            del train_indices[idx]\n",
        "            train_dataset = [full_dataset[i].to(device) for i in train_indices]\n",
        "            test_dataset = [full_dataset[idx].to(device)]\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            # Training loop\n",
        "            train_accuracy, train_loss, model_GCN[idx], optimizer[idx] = train_epoch(train_loader, dr, optimizer[idx], criterion, model_GCN[idx], scheduler[idx])\n",
        "            all_train_losses[idx].append(train_loss)\n",
        "            all_train_accuracies[idx].append(train_accuracy)\n",
        "\n",
        "            # Validation loop\n",
        "            val_accuracy,val_loss = valid_epoch(test_loader, dr, criterion, model_GCN[idx])\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            model_filename = os.path.join(epoch_subdir, f\"subject_{idx}_model.pt\")\n",
        "            torch.save({\n",
        "                        'model_state_dict': model_GCN[idx].state_dict(),\n",
        "                        'learning rate': lr,\n",
        "                        'dropout':dr,\n",
        "                        'hidden_classes':hidden_classes,\n",
        "                        }, model_filename)\n",
        "\n",
        "        all_val_losses.append(np.mean(val_losses))\n",
        "        all_val_accuracies.append(np.mean(val_accuracies))\n",
        "        print(f\" For epoch {epoch}, Validation Loss: {np.mean(val_losses):.4f}, Validation Accuracy: {np.mean(val_accuracies):.4f}\")\n",
        "        # Early stopping\n",
        "        if np.mean(val_losses) < best_val_loss:\n",
        "            best_val_loss = np.mean(val_losses)\n",
        "            early_stop_count = 0\n",
        "        else:\n",
        "            early_stop_count += 1\n",
        "\n",
        "        if np.mean(val_accuracies) > best_val_acc:\n",
        "            best_val_acc= np.mean(val_accuracies)\n",
        "            delete_dir_path= os.path.join(trial_subdir, f\"epoch_{best_epoch}\")\n",
        "            if os.path.exists(delete_dir_path):\n",
        "                  shutil.rmtree(delete_dir_path)\n",
        "            best_epoch=epoch\n",
        "        else:\n",
        "            delete_dir_path= os.path.join(trial_subdir, f\"epoch_{epoch}\")\n",
        "            if os.path.exists(delete_dir_path):\n",
        "                  shutil.rmtree(delete_dir_path)\n",
        "\n",
        "        if early_stop_count >= patience:\n",
        "            print(f'Early stopped at epoch: {epoch}')\n",
        "            best_stop_epoch= epoch\n",
        "            print(f\"Final Validation Accurcay: {best_val_acc}\")\n",
        "            break\n",
        "\n",
        "    # Calculating Average of all training losses across all subjects per epoch\n",
        "    for i in range(best_stop_epoch+1):\n",
        "      avg_train_losses.append(np.average([x[i] for x in all_train_losses]))\n",
        "      avg_train_accuracies.append(np.average([x[i] for x in all_train_accuracies]))\n",
        "\n",
        "#     print(f'length of average train losses, {len(avg_train_losses[0])}')\n",
        "        # Plotting code for all subjects\n",
        "    fig_loss, axs_loss = plt.subplots(6, 6, figsize=(18, 18))\n",
        "    fig_acc, axs_acc = plt.subplots(6, 6, figsize=(18, 18))\n",
        "\n",
        "    for idx in range(36):\n",
        "        # Plotting Loss\n",
        "        # epochs = range(len(all_train_losses[idx]))\n",
        "        ax_loss = axs_loss[idx // 6, idx % 6]\n",
        "        ax_loss.plot(range(len(all_train_losses[idx])), all_train_losses[idx], label='Train', color='red')\n",
        "        ax_loss.set_title(f\"Loss - Subject {idx}-{task_name}\")\n",
        "        ax_loss.legend()\n",
        "\n",
        "        # Plotting Accuracy\n",
        "        # epochs = range(len(all_train_accuracies[idx]))\n",
        "        ax_acc = axs_acc[idx // 6, idx % 6]\n",
        "        ax_acc.plot(range(len(all_train_accuracies[idx])), all_train_accuracies[idx], label='Train', color='green')\n",
        "        ax_acc.set_title(f\"Accuracy - Subject {idx}-{task_name}\")\n",
        "        ax_acc.legend()\n",
        "        # Set integer labels on x-axis\n",
        "        ax_loss.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "        ax_acc.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "\n",
        "    # Adjust layout\n",
        "    fig_loss.tight_layout()\n",
        "    fig_acc.tight_layout()\n",
        "\n",
        "    # Validation loss and Validation accuracy curves vs. epoch\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
        "    ax1.plot(range(len(all_val_losses)), all_val_losses, label='Validation Loss', color='orange')\n",
        "    ax1.plot(range(len(avg_train_losses)), avg_train_losses, label='Training Loss', color='blue')\n",
        "    ax1.set_title('Train + Val Loss vs. Epochs')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax2.plot(range(len(all_val_accuracies)), all_val_accuracies, label='Validation Accuracy', color='orange')\n",
        "    ax2.plot(range(len(avg_train_accuracies)), avg_train_accuracies, label='Training Accuracy', color='blue')\n",
        "    ax2.set_title('Train + Val Accuracy vs. Epochs')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "    ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "    plt.tight_layout()\n",
        "\n",
        "    #  Saving the plots\n",
        "    fig_loss.savefig(os.path.join(plot_subdir+\"/\"+f\"training_loss.png\"))\n",
        "    fig_acc.savefig(os.path.join(plot_subdir+ \"/\"+ f'training_accuracy.png'))\n",
        "    fig.savefig(os.path.join(plot_subdir+ \"/\"+ f'validation_plots.png'))\n",
        "\n",
        "    # Close the figures\n",
        "    # plt.close(fig_loss)\n",
        "    # plt.close(fig_acc)\n",
        "    # plt.close(fig)\n",
        "\n",
        "    # Show plots\n",
        "    plt.show()\n",
        "    print(\"Final validation accuracy :\", best_val_acc)\n",
        "    print(\"Epoch at which Best model with highest validation accuracy is obtained:\", best_epoch)\n",
        "    print(\"Epochs at which Early stopping is done:\", best_stop_epoch)\n",
        "    return best_val_acc"
      ],
      "metadata": {
        "id": "K7TlWjoUhcMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFjUUjb84RoL"
      },
      "source": [
        "## Video Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training using Optuna for hyperparameter tuning on video data"
      ],
      "metadata": {
        "id": "jcRWek4FjdWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial_dir = None\n",
        "best_trial_acc = float('-inf')\n",
        "def objective_ADAM(trial):\n",
        "    global best_trial_dir, best_trial_acc\n",
        "    # Hyperparams to tune\n",
        "    task_name = 'Task5_V'\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2)\n",
        "    dr = trial.suggest_float(\"dropout\", 0.1, 0.8)\n",
        "    hidden_classes = trial.suggest_int('hidden_classes', 16, 180)\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}\")\n",
        "    save_dir=f'/content/GCN_checkpoints_{task_name}/'\n",
        "    trial_subdir = os.path.join(save_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(trial_subdir, exist_ok=True)\n",
        "    plot_dir=f'/content/GCN_plots_{task_name}/'\n",
        "    plot_subdir = os.path.join(plot_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(plot_subdir, exist_ok=True)\n",
        "    # test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads, trial_subdir)\n",
        "    test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes, trial_subdir, plot_subdir, task_name)\n",
        "    if test_acc > best_trial_acc:\n",
        "      best_trial_acc = test_acc\n",
        "      remove_trial_subdir= os.path.join(save_dir,f\"trial_{best_trial_dir}\")\n",
        "      if os.path.exists(remove_trial_subdir):\n",
        "          shutil.rmtree(remove_trial_subdir)\n",
        "      best_trial_dir= trial.number\n",
        "    else:\n",
        "        if os.path.exists(trial_subdir):\n",
        "            shutil.rmtree(trial_subdir)\n",
        "    return test_acc\n",
        "\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed= seed_value))\n",
        "study.optimize(objective_ADAM, n_trials= 40 )\n",
        "\n",
        "print(study.best_trial)"
      ],
      "metadata": {
        "id": "tUa6xXZWjEsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load model from checkpoints and then directly test it for Video data"
      ],
      "metadata": {
        "id": "cxGHpCZ9kaJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 1\n",
        "task_name = 'Task4_P3_V' # Give the name of task for which you are testing results using checkpoints\n",
        "save_dir=\"/kaggle/working/GAT_checkpoints_Task5_A/trial_8/epoch_1\" # path to directory where checkpoints are saved\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "accuracy_list = []\n",
        "# Perform 1-subject leave-out cross-validation\n",
        "for idx in range(36):\n",
        "    # Load the checkpoint\n",
        "    checkpoint_path = os.path.join(save_dir, f\"subject_{idx}_model.pt\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    # Retrieve parameters\n",
        "    model_state_dict = checkpoint['model_state_dict']\n",
        "    lr = checkpoint['learning rate']\n",
        "    dr = checkpoint['dropout']\n",
        "    hidden_classes = checkpoint['hidden_classes']\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}\")\n",
        "\n",
        "    model_GCN = GCN(node_features= 240,hidden_channels= hidden_classes, num_classes=3).to(device)\n",
        "    # Load the model weights\n",
        "    model_GCN.load_state_dict(model_state_dict)\n",
        "    model_GCN.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Create DataLoader for the test dataset\n",
        "    test_dataset = [full_dataset[idx].to(device)]\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Testing loop\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            output_test = model_GCN(data, dropout_rate=dr)\n",
        "            predicted_test = output_test.argmax(dim=1)\n",
        "            target_test = data.y\n",
        "            print(\"Test Subject:\",idx,\"Predicted label:\", predicted_test, \"True label:\", target_test)\n",
        "            total_correct += (predicted_test == target_test).sum().item()\n",
        "            total_samples += data.y.size(0)\n",
        "            accuracy_list.append(accuracy_score(target_test.cpu().numpy(), predicted_test.cpu().numpy()))\n",
        "\n",
        "# Calculate and print test accuracy for the current subject\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(\"total correct :\", total_correct,\" total samples :\", total_samples)\n",
        "print(f\"Using Subject {idx} as test, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(\"Accuracy using inbuilt function:\", np.mean(accuracy_list))"
      ],
      "metadata": {
        "id": "As12m0WakcMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f_CNSHo4V8Q"
      },
      "source": [
        "## Audio Data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training using Optuna for hyperparameter tuning on Audio data"
      ],
      "metadata": {
        "id": "FvyNuckjjfNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial_dir = None\n",
        "best_trial_acc = float('-inf')\n",
        "def objective_ADAM(trial):\n",
        "    global best_trial_dir, best_trial_acc\n",
        "    # Hyperparams to tune\n",
        "    task_name = 'Task5_A'\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2)\n",
        "    dr = trial.suggest_float(\"dropout\", 0.1, 0.8)\n",
        "    hidden_classes = trial.suggest_int('hidden_classes', 16, 180)\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}\")\n",
        "    save_dir=f'/content/GCN_checkpoints_{task_name}/'\n",
        "    trial_subdir = os.path.join(save_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(trial_subdir, exist_ok=True)\n",
        "    plot_dir=f'/content/GCN_plots_{task_name}/'\n",
        "    plot_subdir = os.path.join(plot_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(plot_subdir, exist_ok=True)\n",
        "    # test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads, trial_subdir)\n",
        "    test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes, trial_subdir, plot_subdir, task_name)\n",
        "    if test_acc > best_trial_acc:\n",
        "      best_trial_acc = test_acc\n",
        "      remove_trial_subdir= os.path.join(save_dir,f\"trial_{best_trial_dir}\")\n",
        "      if os.path.exists(remove_trial_subdir):\n",
        "          shutil.rmtree(remove_trial_subdir)\n",
        "      best_trial_dir= trial.number\n",
        "    else:\n",
        "        if os.path.exists(trial_subdir):\n",
        "            shutil.rmtree(trial_subdir)\n",
        "    return test_acc\n",
        "\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed= seed_value))\n",
        "study.optimize(objective_ADAM, n_trials= 40 )\n",
        "\n",
        "print(study.best_trial)"
      ],
      "metadata": {
        "id": "fzplxW0KjXcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load model from checkpoints and then directly test it for audio data"
      ],
      "metadata": {
        "id": "BlUFyvsVkKEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 1\n",
        "task_name = 'Task4_P3_A' # Give the name of task for which you are testing results using checkpoints\n",
        "save_dir=\"/kaggle/working/GAT_checkpoints_Task5_A/trial_8/epoch_1\" # path to directory where checkpoints are saved\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "accuracy_list = []\n",
        "# Perform 1-subject leave-out cross-validation\n",
        "for idx in range(36):\n",
        "    # Load the checkpoint\n",
        "    checkpoint_path = os.path.join(save_dir, f\"subject_{idx}_model.pt\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    # Retrieve parameters\n",
        "    model_state_dict = checkpoint['model_state_dict']\n",
        "    lr = checkpoint['learning rate']\n",
        "    dr = checkpoint['dropout']\n",
        "    hidden_classes = checkpoint['hidden_classes']\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}\")\n",
        "\n",
        "    model_GCN = GCN(node_features= 240,hidden_channels=num_channels, num_classes=3).to(device)\n",
        "    # Load the model weights\n",
        "    model_GCN.load_state_dict(model_state_dict)\n",
        "    model_GCN.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Create DataLoader for the test dataset\n",
        "    test_dataset = [full_dataset[idx].to(device)]\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Testing loop\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            output_test = model_GCN(data, dropout_rate=dr)\n",
        "            predicted_test = output_test.argmax(dim=1)\n",
        "            target_test = data.y\n",
        "            print(\"Test Subject:\",idx,\"Predicted label:\", predicted_test, \"True label:\", target_test)\n",
        "            total_correct += (predicted_test == target_test).sum().item()\n",
        "            total_samples += data.y.size(0)\n",
        "            accuracy_list.append(accuracy_score(target_test.cpu().numpy(), predicted_test.cpu().numpy()))\n",
        "\n",
        "# Calculate and print test accuracy for the current subject\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(\"total correct :\", total_correct,\" total samples :\", total_samples)\n",
        "print(f\"Using Subject {idx} as test, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(\"Accuracy using inbuilt function:\", np.mean(accuracy_list))"
      ],
      "metadata": {
        "id": "5DnUHE8TkJTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbpA29-f4Yd1"
      },
      "source": [
        "## Early Fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training using Optuna for hyperparameter tuning on Early Fusio of video and Audio data"
      ],
      "metadata": {
        "id": "otLqad2JjjlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial_dir = None\n",
        "best_trial_acc = float('-inf')\n",
        "def objective_ADAM(trial):\n",
        "    global best_trial_dir, best_trial_acc\n",
        "    # Hyperparams to tune\n",
        "    task_name = 'Task5_E'\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2)\n",
        "    dr = trial.suggest_float(\"dropout\", 0.1, 0.8)\n",
        "    hidden_classes = trial.suggest_int('hidden_classes', 16, 180)\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}\")\n",
        "    save_dir=f'/content/GCN_checkpoints_{task_name}/'\n",
        "    trial_subdir = os.path.join(save_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(trial_subdir, exist_ok=True)\n",
        "    plot_dir=f'/content/GCN_plots_{task_name}/'\n",
        "    plot_subdir = os.path.join(plot_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(plot_subdir, exist_ok=True)\n",
        "    # test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads, trial_subdir)\n",
        "    test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes, trial_subdir, plot_subdir, task_name)\n",
        "    if test_acc > best_trial_acc:\n",
        "      best_trial_acc = test_acc\n",
        "      remove_trial_subdir= os.path.join(save_dir,f\"trial_{best_trial_dir}\")\n",
        "      if os.path.exists(remove_trial_subdir):\n",
        "          shutil.rmtree(remove_trial_subdir)\n",
        "      best_trial_dir= trial.number\n",
        "    else:\n",
        "        if os.path.exists(trial_subdir):\n",
        "            shutil.rmtree(trial_subdir)\n",
        "    return test_acc\n",
        "\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed= seed_value))\n",
        "study.optimize(objective_ADAM, n_trials= 40 )\n",
        "\n",
        "print(study.best_trial)"
      ],
      "metadata": {
        "id": "FPBiqWJajqAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load model from checkpoints and then directly test it for Early fusion of Video and Audio data"
      ],
      "metadata": {
        "id": "F6S3k-JZk-wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 1\n",
        "task_name = 'Task4_P3_E' # Give the name of task for which you are testing results using checkpoints\n",
        "save_dir=\"/kaggle/working/GAT_checkpoints_Task5_A/trial_8/epoch_1\" # path to directory where checkpoints are saved\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "accuracy_list = []\n",
        "# Perform 1-subject leave-out cross-validation\n",
        "for idx in range(36):\n",
        "    # Load the checkpoint\n",
        "    checkpoint_path = os.path.join(save_dir, f\"subject_{idx}_model.pt\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    # Retrieve parameters\n",
        "    model_state_dict = checkpoint['model_state_dict']\n",
        "    lr = checkpoint['learning rate']\n",
        "    dr = checkpoint['dropout']\n",
        "    hidden_classes = checkpoint['hidden_classes']\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}\")\n",
        "\n",
        "    model_GCN = GCN(node_features= 240,hidden_channels=num_channels, num_classes=3).to(device)\n",
        "    # Load the model weights\n",
        "    model_GCN.load_state_dict(model_state_dict)\n",
        "    model_GCN.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Create DataLoader for the test dataset\n",
        "    test_dataset = [full_dataset[idx].to(device)]\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Testing loop\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            output_test = model_GCN(data, dropout_rate=dr)\n",
        "            predicted_test = output_test.argmax(dim=1)\n",
        "            target_test = data.y\n",
        "            print(\"Test Subject:\",idx,\"Predicted label:\", predicted_test, \"True label:\", target_test)\n",
        "            total_correct += (predicted_test == target_test).sum().item()\n",
        "            total_samples += data.y.size(0)\n",
        "            accuracy_list.append(accuracy_score(target_test.cpu().numpy(), predicted_test.cpu().numpy()))\n",
        "\n",
        "# Calculate and print test accuracy for the current subject\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(\"total correct :\", total_correct,\" total samples :\", total_samples)\n",
        "print(f\"Using Subject {idx} as test, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(\"Accuracy using inbuilt function:\", np.mean(accuracy_list))"
      ],
      "metadata": {
        "id": "ZWPx8Orkk9MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojCGxRST3b0h"
      },
      "source": [
        "## Late Fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For late fusion the training and validation loop are different from previous training and validation loops."
      ],
      "metadata": {
        "id": "nu0Wd1rVlM6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_late(train_loader1, dr1, optimizer1, model_GCN1, scheduler1, train_loader2, dr2, optimizer2, model_GCN2, scheduler2,criterion):\n",
        "    loss_train_per = []\n",
        "    accuracy_train = []\n",
        "    model_GCN1.train()\n",
        "    model_GCN2.train()\n",
        "    for data1, data2 in zip(train_loader1, train_loader2):\n",
        "        optimizer1.zero_grad()\n",
        "        output_train1 = model_GCN1(data1, dropout_rate= dr1)\n",
        "        target_train1 = data1.y\n",
        "        loss1 = criterion(output_train1, target_train1)\n",
        "        loss1.backward(retain_graph= True)\n",
        "        optimizer1.step()\n",
        "\n",
        "        optimizer2.zero_grad()\n",
        "        output_train2 = model_GCN2(data2, dropout_rate= dr2)\n",
        "        target_train2 = data2.y\n",
        "        loss2 = criterion(output_train2, target_train2)\n",
        "        loss2.backward(retain_graph= True)\n",
        "        optimizer2.step()\n",
        "        final_output = (output_train1+output_train2)/2.0\n",
        "        predicted_labels = torch.argmax(final_output, dim=1)\n",
        "        target_fused = torch.max(target_train1, target_train2)\n",
        "        loss = (loss1.item()+ loss2.item())/2.0\n",
        "        loss_train_per.append(loss)\n",
        "        accuracy_train.append(accuracy_score(target_fused.cpu().numpy(), predicted_labels.cpu().numpy()))\n",
        "    loss_train_total= np.mean(loss_train_per)\n",
        "    train_accuracy= np.mean(accuracy_train)\n",
        "    if scheduler1 is not None:\n",
        "        scheduler1.step()  # Update learning rate\n",
        "    if scheduler2 is not None:\n",
        "        scheduler2.step()  # Update learning rate\n",
        "\n",
        "    return train_accuracy, loss_train_total,model_GCN1, optimizer1, model_GCN2, optimizer2"
      ],
      "metadata": {
        "id": "rPCzRphklNet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_epoch_late(test_loader1, dr1, model_GCN1, test_loader2, dr2, model_GCN2, criterion):\n",
        "  val_loss_per=[]\n",
        "  accuracy_test=[]\n",
        "  model_GCN1.eval()\n",
        "  model_GCN2.eval()\n",
        "  with torch.no_grad():\n",
        "      for data1, data2 in zip(test_loader1, test_loader2):\n",
        "        output_test1 = model_GCN1(data1, dropout_rate= dr2)\n",
        "        target_test1 = data1.y\n",
        "        output_test2 = model_GCN2(data2, dropout_rate= dr2)\n",
        "        target_test2 = data2.y\n",
        "        output_test_fused = (output_test1+output_test2)/2.0\n",
        "        predicted_labels = torch.argmax(output_test_fused, dim=1)\n",
        "        target_fused = torch.max(target_test1, target_test2)\n",
        "        accuracy_test.append(accuracy_score(target_fused.cpu().numpy(), predicted_labels.cpu().numpy()))\n",
        "        loss = criterion(output_test_fused, target_fused)\n",
        "        val_loss_per.append(loss.item())\n",
        "  val_loss_total= np.mean(val_loss_per)\n",
        "  val_accuracy= np.mean(accuracy_test)\n",
        "  return val_accuracy, val_loss_total"
      ],
      "metadata": {
        "id": "KIflZ2WBlOQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_valid_loop_ADAM_late(lr1, dr1, hidden_classes1, lr2, dr2, hidden_classes2, trial_subdir, plot_subdir, task_name):\n",
        "    num_epochs = 20\n",
        "    best_epoch = -1\n",
        "    best_stop_epoch=-1\n",
        "    early_stop_count = 0\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0\n",
        "    patience = 3\n",
        "    batch_size = 1\n",
        "    all_train_losses = [[] for _ in range(36)]\n",
        "    all_train_accuracies = [[] for _ in range(36)]\n",
        "    avg_train_losses=[] # Average of all training losses across all subjects per epoch\n",
        "    avg_train_accuracies=[] # Average of all training accuracies across all subjects per epoch\n",
        "    all_val_losses = []\n",
        "    all_val_accuracies = []\n",
        "    '''GAT MODEL with 2 layers in architecture\n",
        "    model_GAT = [GAT(node_features=240, hidden_channels= hidden_classes,num_heads=num_heads, num_classes=3, dropout_rate= dr).to(device) for _ in range(36)]\n",
        "    '''\n",
        "    # GAT model with parameters to choose no. of layers in architecture\n",
        "    model_GCN1 = [GCN(node_features= 240,hidden_channels=hidden_classes1, num_classes=3).to(device) for _ in range(36)]\n",
        "    optimizer1 = [torch.optim.Adam(model_GCN1[_].parameters(), lr= lr1) for _ in range(36)]\n",
        "    scheduler1 = [StepLR(optimizer1[_], step_size=1, gamma=0.9) for _ in range(36)] # decrease LR by a factor of 0.9 every epoch\n",
        "\n",
        "    model_GCN2 = [GCN(node_features= 240,hidden_channels=hidden_classes1, num_classes=3).to(device) for _ in range(36)]\n",
        "    optimizer2 = [torch.optim.Adam(model_GCN2[_].parameters(), lr= lr2) for _ in range(36)]\n",
        "    scheduler2 = [StepLR(optimizer2[_], step_size=1, gamma=0.9) for _ in range(36)] # decrease LR by a factor of 0.9 every epoch\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_subdir = os.path.join(trial_subdir, f\"epoch_{epoch}\")\n",
        "        os.makedirs(epoch_subdir, exist_ok=True)\n",
        "        # print(f\"## epoch {epoch}  ##\")\n",
        "        val_accuracies =[]\n",
        "        val_losses=[]\n",
        "        for idx in range(36):\n",
        "            # DataLoader setup\n",
        "            # print(f\"## Subject {idx} is test subject  ##\")\n",
        "            train_indices = list(range(36))\n",
        "            del train_indices[idx]\n",
        "            train_dataset1 = [full_dataset_V[i].to(device) for i in train_indices]\n",
        "            test_dataset1 = [full_dataset_V[idx].to(device)]\n",
        "            train_dataset2 = [full_dataset_A[i].to(device) for i in train_indices]\n",
        "            test_dataset2 = [full_dataset_A[idx].to(device)]\n",
        "\n",
        "            train_loader1 = DataLoader(train_dataset1, batch_size=batch_size, shuffle=True)\n",
        "            test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False)\n",
        "            train_loader2 = DataLoader(train_dataset2, batch_size=batch_size, shuffle=True)\n",
        "            test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            # Training loop\n",
        "            train_accuracy, train_loss, model_GCN1[idx], optimizer1[idx], model_GCN2[idx], optimizer2[idx] = train_epoch_late(train_loader1, dr1, optimizer1[idx], model_GCN1[idx], scheduler1[idx], train_loader2, dr2, optimizer2[idx], model_GCN2[idx], scheduler2[idx],criterion)\n",
        "            all_train_losses[idx].append(train_loss)\n",
        "            all_train_accuracies[idx].append(train_accuracy)\n",
        "\n",
        "            # Validation loop\n",
        "            val_accuracy,val_loss = valid_epoch_late(test_loader1, dr1, model_GCN1[idx], test_loader2, dr2, model_GCN2[idx], criterion)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            # Saving model checkpoints\n",
        "            model_filename1 = os.path.join(epoch_subdir, f\"Model1_subject_{idx}_model.pt\")\n",
        "            torch.save({\n",
        "                        'model_state_dict1': model_GCN1[idx].state_dict(),\n",
        "                        'learning rate1': lr1,\n",
        "                        'dropout1':dr1,\n",
        "                        'hidden_classes1':hidden_classes1,\n",
        "                        }, model_filename1)\n",
        "            model_filename2 = os.path.join(epoch_subdir, f\"Model2_subject_{idx}_model.pt\")\n",
        "            torch.save({\n",
        "                        'model_state_dict2': model_GCN2[idx].state_dict(),\n",
        "                        'learning rate2': lr2,\n",
        "                        'dropout2':dr2,\n",
        "                        'hidden_classes2':hidden_classes2,\n",
        "                        }, model_filename2)\n",
        "\n",
        "        all_val_losses.append(np.mean(val_losses))\n",
        "        all_val_accuracies.append(np.mean(val_accuracies))\n",
        "        print(f\" For epoch {epoch}, Validation Loss: {np.mean(val_losses):.4f}, Validation Accuracy: {np.mean(val_accuracies):.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if np.mean(val_losses) < best_val_loss:\n",
        "            best_val_loss = np.mean(val_losses)\n",
        "            early_stop_count = 0\n",
        "        else:\n",
        "            early_stop_count += 1\n",
        "\n",
        "        if np.mean(val_accuracies) > best_val_acc:\n",
        "            best_val_acc= np.mean(val_accuracies)\n",
        "            delete_dir_path= os.path.join(trial_subdir, f\"epoch_{best_epoch}\")\n",
        "            if os.path.exists(delete_dir_path):\n",
        "                  shutil.rmtree(delete_dir_path)\n",
        "            best_epoch=epoch\n",
        "        else:\n",
        "            delete_dir_path= os.path.join(trial_subdir, f\"epoch_{epoch}\")\n",
        "            if os.path.exists(delete_dir_path):\n",
        "                  shutil.rmtree(delete_dir_path)\n",
        "\n",
        "        if early_stop_count >= patience:\n",
        "            print(f'Early stopped at epoch: {epoch}')\n",
        "            best_stop_epoch= epoch\n",
        "            print(f\"Final Validation Accurcay: {best_val_acc}\")\n",
        "            break\n",
        "\n",
        "    # Calculating Average of all training losses across all subjects per epoch\n",
        "    for i in range(best_stop_epoch+1):\n",
        "      avg_train_losses.append(np.average([x[i] for x in all_train_losses]))\n",
        "      avg_train_accuracies.append(np.average([x[i] for x in all_train_accuracies]))\n",
        "\n",
        "#     print(f'length of average train losses, {len(avg_train_losses[0])}')\n",
        "        # Plotting code for all subjects\n",
        "    fig_loss, axs_loss = plt.subplots(6, 6, figsize=(18, 18))\n",
        "    fig_acc, axs_acc = plt.subplots(6, 6, figsize=(18, 18))\n",
        "\n",
        "    for idx in range(36):\n",
        "        # Plotting Loss\n",
        "        # epochs = range(len(all_train_losses[idx]))\n",
        "        ax_loss = axs_loss[idx // 6, idx % 6]\n",
        "        ax_loss.plot(range(len(all_train_losses[idx])), all_train_losses[idx], label='Train', color='red')\n",
        "        ax_loss.set_title(f\"Loss - Subject {idx}-{task_name}\")\n",
        "        ax_loss.legend()\n",
        "\n",
        "        # Plotting Accuracy\n",
        "        # epochs = range(len(all_train_accuracies[idx]))\n",
        "        ax_acc = axs_acc[idx // 6, idx % 6]\n",
        "        ax_acc.plot(range(len(all_train_accuracies[idx])), all_train_accuracies[idx], label='Train', color='green')\n",
        "        ax_acc.set_title(f\"Accuracy - Subject {idx}-{task_name}\")\n",
        "        ax_acc.legend()\n",
        "        # Set integer labels on x-axis\n",
        "        ax_loss.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "        ax_acc.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "\n",
        "    # Adjust layout\n",
        "    fig_loss.tight_layout()\n",
        "    fig_acc.tight_layout()\n",
        "\n",
        "    # Validation loss and Validation accuracy curves vs. epoch\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
        "    ax1.plot(range(len(all_val_losses)), all_val_losses, label='Validation Loss', color='orange')\n",
        "    ax1.plot(range(len(avg_train_losses)), avg_train_losses, label='Training Loss', color='blue')\n",
        "    ax1.set_title('Train + Val Loss vs. Epochs')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax2.plot(range(len(all_val_accuracies)), all_val_accuracies, label='Validation Accuracy', color='orange')\n",
        "    ax2.plot(range(len(avg_train_accuracies)), avg_train_accuracies, label='Training Accuracy', color='blue')\n",
        "    ax2.set_title('Train + Val Accuracy vs. Epochs')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "    ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "    plt.tight_layout()\n",
        "\n",
        "    #  Saving the plots\n",
        "    fig_loss.savefig(os.path.join(plot_subdir+\"/\"+f\"training_loss.png\"))\n",
        "    fig_acc.savefig(os.path.join(plot_subdir+ \"/\"+ f'training_accuracy.png'))\n",
        "    fig.savefig(os.path.join(plot_subdir+ \"/\"+ f'validation_plots.png'))\n",
        "\n",
        "    # Show plots\n",
        "    plt.show()\n",
        "    print(\"Final validation accuracy :\", best_val_acc)\n",
        "    print(\"Epoch at which Best model with highest validation accuracy is obtained:\", best_epoch)\n",
        "    print(\"Epochs at which Early stopping is done:\", best_stop_epoch)\n",
        "    return best_val_acc"
      ],
      "metadata": {
        "id": "tF6TOs6GlOIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training using Optuna for hyperparameter tuning on Late fusion of video and Audio data"
      ],
      "metadata": {
        "id": "gdl_WRIAljQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial_dir = None\n",
        "best_trial_acc = float('-inf')\n",
        "task_name = 'Task5_L'\n",
        "def objective_ADAM_late(trial):\n",
        "    global best_trial_dir, best_trial_acc, task_name\n",
        "    # Hyperparams to tune\n",
        "    lr1 = trial.suggest_float(\"lr1\", 1e-5, 1e-2)\n",
        "    lr2 = trial.suggest_float(\"lr2\", 1e-5, 1e-2)\n",
        "    dr1 = trial.suggest_float(\"dropout1\", 0.1, 0.8)\n",
        "    dr2 = trial.suggest_float(\"dropout2\", 0.1, 0.8)\n",
        "    hidden_classes1 = trial.suggest_int(\"hidden_classes1\", 16, 180)\n",
        "    hidden_classes2 = trial.suggest_int(\"hidden_classes2\", 16, 180)\n",
        "    print(\"learning rate 1:\", lr1,\"learning rate 2:\", lr2,\"dropout rate1:\", dr1, \"dropout rate2:\", dr2,\"hidden_classes1 :\", hidden_classes1, \"hidden_classes2:\", hidden_classes2)\n",
        "    save_dir=f'/kaggle/working/GCN_checkpoints_{task_name}/'\n",
        "    trial_subdir = os.path.join(save_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(trial_subdir, exist_ok=True)\n",
        "    plot_dir=f'/kaggle/working/GCN_plots_{task_name}/'\n",
        "    plot_subdir = os.path.join(plot_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(plot_subdir, exist_ok=True)\n",
        "    # test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads, trial_subdir)\n",
        "    test_acc = train_valid_loop_ADAM_late(lr1, dr1, hidden_classes1, lr2, dr2, hidden_classes2, trial_subdir, plot_subdir, task_name)\n",
        "    if test_acc > best_trial_acc:\n",
        "      best_trial_acc = test_acc\n",
        "      remove_trial_subdir= os.path.join(save_dir,f\"trial_{best_trial_dir}\")\n",
        "      if os.path.exists(remove_trial_subdir):\n",
        "          shutil.rmtree(remove_trial_subdir)\n",
        "      best_trial_dir= trial.number\n",
        "    else:\n",
        "        if os.path.exists(trial_subdir):\n",
        "            shutil.rmtree(trial_subdir)\n",
        "    return test_acc\n",
        "\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed= seed_value))\n",
        "study.optimize(objective_ADAM_late, n_trials= 40 )\n",
        "\n",
        "print(study.best_trial)"
      ],
      "metadata": {
        "id": "t-nvl4sNlOEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load model from checkpoints and then directly test it for late fusion of video and Audio data"
      ],
      "metadata": {
        "id": "K782JCaAl-Ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=1\n",
        "task_name = 'Task5_L' # name of task for which you are testing\n",
        "save_dir=\"/content/drive/MyDrive/GAT_checkpoints/Late Fusion/trial_23\" # path of directory where checkpoints are saved\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "test_accuracy = []\n",
        "# Perform 1-subject leave-out cross-validation\n",
        "for idx in range(36):\n",
        "    # Load the checkpoint\n",
        "    checkpoint_path1 = os.path.join(save_dir, f\"Model1_subject_{idx}_model.pt\")\n",
        "    checkpoint1 = torch.load(checkpoint_path1)\n",
        "    # Retrieve parameters\n",
        "    model_state_dict1 = checkpoint1['model_state_dict']\n",
        "    lr1 = checkpoint1['learning rate']\n",
        "    dr1 = checkpoint1['dropout']\n",
        "    hidden_classes1 = checkpoint1['hidden_classes']\n",
        "    print(f\"learning rate:{lr1},dropout: {dr1}, hidden_classes: {hidden_classes1}\")\n",
        "\n",
        "    model_GCN1 = GCN(node_features= 240,hidden_channels= hidden_classes1, num_classes=3).to(device)\n",
        "    # Load the model weights\n",
        "    model_GCN1.load_state_dict(model_state_dict1)\n",
        "    model_GCN1.eval()  # Set the model to evaluation mode\n",
        "\n",
        "\n",
        "    checkpoint_path2 = os.path.join(save_dir, f\"Model2_subject_{idx}_model.pt\")\n",
        "    checkpoint2 = torch.load(checkpoint_path2)\n",
        "    # Retrieve parameters\n",
        "    model_state_dict2 = checkpoint2['model_state_dict']\n",
        "    lr2 = checkpoint2['learning rate']\n",
        "    dr2 = checkpoint2['dropout']\n",
        "    hidden_classes2 = checkpoint2['hidden_classes']\n",
        "    print(f\"learning rate:{lr2},dropout: {dr2}, hidden_classes: {hidden_classes2}\")\n",
        "\n",
        "    model_GCN2 = GCN(node_features= 240,hidden_channels=hidden_classes2, num_classes=3).to(device)\n",
        "    # Load the model weights\n",
        "    model_GCN2.load_state_dict(model_state_dict2)\n",
        "    model_GCN2.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Create DataLoader for the test dataset\n",
        "    test_dataset1 = [full_dataset_V[idx].to(device)]\n",
        "    test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset2 = [full_dataset_A[idx].to(device)]\n",
        "    test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Testing loop\n",
        "    model_GCN1.eval()\n",
        "    model_GCN2.eval()\n",
        "    with torch.no_grad():\n",
        "        for data1, data2 in zip(test_loader1, test_loader2):\n",
        "            output_test1 = model_GCN1(data1, dropout_rate= dr1)\n",
        "            target_test1 = data1.y\n",
        "            output_test2 = model_GCN2(data2, dropout_rate= dr2)\n",
        "            target_test2 = data2.y\n",
        "            final_output = (output_test1+output_test2)/2.0\n",
        "            predicted_labels = torch.argmax(final_output, dim=1)\n",
        "            target_fused = torch.max(target_test1, target_test2)\n",
        "            total_correct += (predicted_labels == target_fused).sum().item()\n",
        "            total_samples += data1.y.size(0)\n",
        "            print(\"Test Subject:\",idx,\" predicted label:\", predicted_labels ,\" True label : \", target_fused)\n",
        "            test_accuracy.append(accuracy_score(target_fused.cpu().numpy(), predicted_labels.cpu().numpy()))\n",
        "\n",
        "# Calculate and print test accuracy for the current subject\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(\"total correct :\", total_correct,\" total samples :\", total_samples)\n",
        "print(f\"Using Subject {idx} as test, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(\"Accuracy using inbuilt function:\", np.mean(test_accuracy))"
      ],
      "metadata": {
        "id": "M1_tHf3qlOBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNAnb_jciRNK"
      },
      "source": [
        "# GAT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAT Model with 2 layers of GATCONV"
      ],
      "metadata": {
        "id": "VN8eWrTWkQ5Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9k87sIxGjd_"
      },
      "outputs": [],
      "source": [
        "# class GAT(torch.nn.Module):\n",
        "#     def __init__(self, node_features, hidden_channels,num_heads, num_classes, dropout_rate):\n",
        "#         super(GAT, self).__init__()\n",
        "#         self.head = num_heads\n",
        "\n",
        "#         self.conv1 = GATConv(node_features, hidden_channels, heads=self.head, dropout=dropout_rate)\n",
        "#         self.conv2 = GATConv(hidden_channels * self.head, hidden_channels, heads=self.head,\n",
        "#                              dropout=dropout_rate)\n",
        "\n",
        "#         self.lin = nn.Linear(hidden_channels * self.head, num_classes)\n",
        "\n",
        "#     def forward(self, data, dropout_rate):\n",
        "#         x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
        "\n",
        "#         x = F.dropout(x, p= dropout_rate, training=self.training)\n",
        "#         x = self.conv1(x, edge_index, edge_attr)\n",
        "#         x = F.elu(x)\n",
        "#         x = F.dropout(x, p= dropout_rate, training=self.training)\n",
        "#         x = self.conv2(x, edge_index, edge_attr)\n",
        "#         x = F.elu(x)\n",
        "\n",
        "#         x = global_mean_pool(x, batch)\n",
        "#         x = self.lin(x)\n",
        "\n",
        "#         return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print('Device', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAT model with variable layers of GATCONV (1 or 2)"
      ],
      "metadata": {
        "id": "w85dvhFHkN2n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZsQPxZgMpKd"
      },
      "outputs": [],
      "source": [
        "class GAT_variable_layer(torch.nn.Module):\n",
        "    def __init__(self, node_features, hidden_channels, num_heads, num_classes, dropout_rate, use_second_layer):\n",
        "        super(GAT_variable_layer, self).__init__()\n",
        "        self.head = num_heads\n",
        "\n",
        "        self.conv1 = GATConv(node_features, hidden_channels, heads=self.head, dropout=dropout_rate)\n",
        "\n",
        "        # Conditionally add the second convolutional layer based on the 'use_second_layer' argument\n",
        "        if use_second_layer:\n",
        "            self.conv2 = GATConv(hidden_channels * self.head, hidden_channels, heads=self.head,\n",
        "                                 dropout=dropout_rate)\n",
        "        else:\n",
        "            self.conv2 = None\n",
        "\n",
        "        self.lin = nn.Linear(hidden_channels * self.head, num_classes)\n",
        "\n",
        "    def forward(self, data, dropout_rate, use_second_layer):\n",
        "        x, batch, edge_index, edge_attr = data.x, data.batch, data.edge_index, data.edge_attr\n",
        "\n",
        "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
        "        x = self.conv1(x, edge_index, edge_attr)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=dropout_rate, training=self.training)\n",
        "\n",
        "        # Conditionally apply the second convolutional layer\n",
        "        if use_second_layer and self.conv2 is not None:\n",
        "            x = self.conv2(x, edge_index, edge_attr)\n",
        "            x = F.elu(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop for GAT"
      ],
      "metadata": {
        "id": "B2ftJ50oe2EI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same training loop is for Video, Audio and Early Fusion. Different for Late fusion only"
      ],
      "metadata": {
        "id": "SxRbw3cKb3og"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_kZE_7RKdl-"
      },
      "outputs": [],
      "source": [
        "def train_epoch(train_loader, dr,is_2nd_layer, optimizer, criterion, model_GAT, scheduler):\n",
        "  loss_train_per = []\n",
        "  accuracy_train = []\n",
        "  model_GAT.train()\n",
        "  for data in train_loader :\n",
        "        optimizer.zero_grad()\n",
        "        output_train = model_GAT(data, dropout_rate= dr, use_second_layer=is_2nd_layer)\n",
        "        target_train = data.y  # Assuming your target labels are stored in 'y'\n",
        "        loss = criterion(output_train, target_train)\n",
        "        loss.backward(retain_graph= True)\n",
        "        optimizer.step()\n",
        "        predicted_train = output_train.argmax(dim=1)\n",
        "        accuracy_train.append(accuracy_score(target_train.cpu().numpy(), predicted_train.cpu().numpy()))\n",
        "        loss_train_per.append(loss.item())\n",
        "  loss_train_total= np.mean(loss_train_per)\n",
        "  train_accuracy= np.mean(accuracy_train)\n",
        "  if scheduler is not None:\n",
        "        scheduler.step()  # Update learning rate\n",
        "  return train_accuracy, loss_train_total,model_GAT, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7rM7fbYLtWa"
      },
      "outputs": [],
      "source": [
        "def valid_epoch(test_loader, dr, is_2nd_layer, criterion,model_GAT):\n",
        "  val_loss_per=[]\n",
        "  accuracy_test=[]\n",
        "  model_GAT.eval()\n",
        "  with torch.no_grad():\n",
        "      for data in test_loader:\n",
        "          output_test = model_GAT(data, dropout_rate= dr, use_second_layer=is_2nd_layer)\n",
        "          predicted_test = output_test.argmax(dim=1)\n",
        "          target_test = data.y\n",
        "          accuracy_test.append(accuracy_score(target_test.cpu().numpy(), predicted_test.cpu().numpy()))\n",
        "          loss = criterion(output_test, target_test)\n",
        "          val_loss_per.append(loss.item())\n",
        "  val_loss_total= np.mean(val_loss_per)\n",
        "  val_accuracy= np.mean(accuracy_test)\n",
        "  return val_accuracy, val_loss_total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCzE5r0QGCHO"
      },
      "outputs": [],
      "source": [
        "def train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads,is_2nd_layer, trial_subdir, plot_subdir, task_name):\n",
        "    num_epochs = 20\n",
        "    best_epoch = -1\n",
        "    best_stop_epoch=-1\n",
        "    early_stop_count = 0\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0\n",
        "    patience = 3\n",
        "    batch_size = 1\n",
        "    all_train_losses = [[] for _ in range(36)]\n",
        "    all_train_accuracies = [[] for _ in range(36)]\n",
        "    avg_train_losses=[] # Average of all training losses across all subjects per epoch\n",
        "    avg_train_accuracies=[] # Average of all training accuracies across all subjects per epoch\n",
        "    all_val_losses = []\n",
        "    all_val_accuracies = []\n",
        "    '''GAT MODEL with 2 layers in architecture\n",
        "    model_GAT = [GAT(node_features=240, hidden_channels= hidden_classes,num_heads=num_heads, num_classes=3, dropout_rate= dr).to(device) for _ in range(36)]\n",
        "    '''\n",
        "    # GAT model with parameters to choose no. of layers in architecture\n",
        "    model_GAT = [GAT_variable_layer(node_features=240, hidden_channels= hidden_classes,num_heads=num_heads, num_classes=3, dropout_rate= dr, use_second_layer= is_2nd_layer).to(device) for _ in range(36)]\n",
        "    optimizer = [torch.optim.Adam(model_GAT[_].parameters(), lr= lr) for _ in range(36)]\n",
        "    scheduler = [StepLR(optimizer[_], step_size=1, gamma=0.9) for _ in range(36)] # decrease LR by a factor of 0.9 every epoch\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_subdir = os.path.join(trial_subdir, f\"epoch_{epoch}\")\n",
        "        os.makedirs(epoch_subdir, exist_ok=True)\n",
        "        # print(f\"## epoch {epoch}  ##\")\n",
        "        val_accuracies =[]\n",
        "        val_losses=[]\n",
        "        for idx in range(36):\n",
        "            # DataLoader setup\n",
        "            # print(f\"## Subject {idx} is test subject  ##\")\n",
        "            train_indices = list(range(36))\n",
        "            del train_indices[idx]\n",
        "            train_dataset = [full_dataset[i].to(device) for i in train_indices]\n",
        "            test_dataset = [full_dataset[idx].to(device)]\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            # Training loop\n",
        "            train_accuracy, train_loss, model_GAT[idx], optimizer[idx] = train_epoch(train_loader, dr,is_2nd_layer, optimizer[idx], criterion, model_GAT[idx], scheduler[idx])\n",
        "            all_train_losses[idx].append(train_loss)\n",
        "            all_train_accuracies[idx].append(train_accuracy)\n",
        "\n",
        "            # Validation loop\n",
        "            val_accuracy,val_loss = valid_epoch(test_loader, dr, is_2nd_layer, criterion, model_GAT[idx])\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            model_filename = os.path.join(epoch_subdir, f\"subject_{idx}_model.pt\")\n",
        "            torch.save({\n",
        "                        'model_state_dict': model_GAT[idx].state_dict(),\n",
        "                        'learning rate': lr,\n",
        "                        'dropout':dr,\n",
        "                        'hidden_classes':hidden_classes,\n",
        "                        'num_heads':num_heads,\n",
        "                        'is_model': is_2nd_layer,\n",
        "                        }, model_filename)\n",
        "\n",
        "        all_val_losses.append(np.mean(val_losses))\n",
        "        all_val_accuracies.append(np.mean(val_accuracies))\n",
        "        print(f\" For epoch {epoch}, Validation Loss: {np.mean(val_losses):.4f}, Validation Accuracy: {np.mean(val_accuracies):.4f}\")\n",
        "        # Early stopping\n",
        "        if np.mean(val_losses) < best_val_loss:\n",
        "            best_val_loss = np.mean(val_losses)\n",
        "            early_stop_count = 0\n",
        "        else:\n",
        "            early_stop_count += 1\n",
        "\n",
        "        if np.mean(val_accuracies) > best_val_acc:\n",
        "            best_val_acc= np.mean(val_accuracies)\n",
        "            delete_dir_path= os.path.join(trial_subdir, f\"epoch_{best_epoch}\")\n",
        "            if os.path.exists(delete_dir_path):\n",
        "                  shutil.rmtree(delete_dir_path)\n",
        "            best_epoch=epoch\n",
        "        else:\n",
        "            delete_dir_path= os.path.join(trial_subdir, f\"epoch_{epoch}\")\n",
        "            if os.path.exists(delete_dir_path):\n",
        "                  shutil.rmtree(delete_dir_path)\n",
        "\n",
        "        if early_stop_count >= patience:\n",
        "            print(f'Early stopped at epoch: {epoch}')\n",
        "            best_stop_epoch= epoch\n",
        "            print(f\"Final Validation Accurcay: {best_val_acc}\")\n",
        "            break\n",
        "\n",
        "    # Calculating Average of all training losses across all subjects per epoch\n",
        "    for i in range(best_stop_epoch+1):\n",
        "      avg_train_losses.append(np.average([x[i] for x in all_train_losses]))\n",
        "      avg_train_accuracies.append(np.average([x[i] for x in all_train_accuracies]))\n",
        "\n",
        "#     print(f'length of average train losses, {len(avg_train_losses[0])}')\n",
        "        # Plotting code for all subjects\n",
        "    fig_loss, axs_loss = plt.subplots(6, 6, figsize=(18, 18))\n",
        "    fig_acc, axs_acc = plt.subplots(6, 6, figsize=(18, 18))\n",
        "\n",
        "    for idx in range(36):\n",
        "        # Plotting Loss\n",
        "        # epochs = range(len(all_train_losses[idx]))\n",
        "        ax_loss = axs_loss[idx // 6, idx % 6]\n",
        "        ax_loss.plot(range(len(all_train_losses[idx])), all_train_losses[idx], label='Train', color='red')\n",
        "        ax_loss.set_title(f\"Loss - Subject {idx}-{task_name}\")\n",
        "        ax_loss.legend()\n",
        "\n",
        "        # Plotting Accuracy\n",
        "        # epochs = range(len(all_train_accuracies[idx]))\n",
        "        ax_acc = axs_acc[idx // 6, idx % 6]\n",
        "        ax_acc.plot(range(len(all_train_accuracies[idx])), all_train_accuracies[idx], label='Train', color='green')\n",
        "        ax_acc.set_title(f\"Accuracy - Subject {idx}-{task_name}\")\n",
        "        ax_acc.legend()\n",
        "        # Set integer labels on x-axis\n",
        "        ax_loss.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "        ax_acc.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "\n",
        "    # Adjust layout\n",
        "    fig_loss.tight_layout()\n",
        "    fig_acc.tight_layout()\n",
        "\n",
        "    # Validation loss and Validation accuracy curves vs. epoch\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
        "    ax1.plot(range(len(all_val_losses)), all_val_losses, label='Validation Loss', color='orange')\n",
        "    ax1.plot(range(len(avg_train_losses)), avg_train_losses, label='Training Loss', color='blue')\n",
        "    ax1.set_title('Train + Val Loss vs. Epochs')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax2.plot(range(len(all_val_accuracies)), all_val_accuracies, label='Validation Accuracy', color='orange')\n",
        "    ax2.plot(range(len(avg_train_accuracies)), avg_train_accuracies, label='Training Accuracy', color='blue')\n",
        "    ax2.set_title('Train + Val Accuracy vs. Epochs')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "    ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "    plt.tight_layout()\n",
        "\n",
        "    #  Saving the plots\n",
        "    fig_loss.savefig(os.path.join(plot_subdir+\"/\"+f\"training_loss.png\"))\n",
        "    fig_acc.savefig(os.path.join(plot_subdir+ \"/\"+ f'training_accuracy.png'))\n",
        "    fig.savefig(os.path.join(plot_subdir+ \"/\"+ f'validation_plots.png'))\n",
        "\n",
        "    # Close the figures\n",
        "    # plt.close(fig_loss)\n",
        "    # plt.close(fig_acc)\n",
        "    # plt.close(fig)\n",
        "\n",
        "    # Show plots\n",
        "    plt.show()\n",
        "    print(\"Final validation accuracy :\", best_val_acc)\n",
        "    print(\"Epoch at which Best model with highest validation accuracy is obtained:\", best_epoch)\n",
        "    print(\"Epochs at which Early stopping is done:\", best_stop_epoch)\n",
        "    return best_val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kucAfyO0CJ_R"
      },
      "source": [
        "## Video"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training using Optuna for hyperparameter tuning on video data"
      ],
      "metadata": {
        "id": "p8OARfdNcG4K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVhN--Rw1aPa"
      },
      "outputs": [],
      "source": [
        "best_trial_dir = None\n",
        "best_trial_acc = float('-inf')\n",
        "def objective_ADAM(trial):\n",
        "    global best_trial_dir, best_trial_acc\n",
        "    # Hyperparams to tune\n",
        "    task_name = 'Task5_V'\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2)\n",
        "    dr = trial.suggest_float(\"dropout\", 0.1, 0.8)\n",
        "    hidden_classes = trial.suggest_int('hidden_classes', 16, 180)\n",
        "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
        "    is_2nd_layer = trial.suggest_int(\"is_2nd_layer\", 0,1)\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}, num_heads: {num_heads}, is_2nd_layer: {is_2nd_layer}\")\n",
        "    # print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}, num_heads: {num_heads}\")\n",
        "    save_dir=f'/content/GAT_checkpoints_{task_name}/'\n",
        "    trial_subdir = os.path.join(save_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(trial_subdir, exist_ok=True)\n",
        "    plot_dir=f'/content/GAT_plots_{task_name}/'\n",
        "    plot_subdir = os.path.join(plot_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(plot_subdir, exist_ok=True)\n",
        "    # test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads, trial_subdir)\n",
        "    test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads,is_2nd_layer, trial_subdir, plot_subdir, task_name)\n",
        "    if test_acc > best_trial_acc:\n",
        "      best_trial_acc = test_acc\n",
        "      remove_trial_subdir= os.path.join(save_dir,f\"trial_{best_trial_dir}\")\n",
        "      if os.path.exists(remove_trial_subdir):\n",
        "          shutil.rmtree(remove_trial_subdir)\n",
        "      best_trial_dir= trial.number\n",
        "    else:\n",
        "        if os.path.exists(trial_subdir):\n",
        "            shutil.rmtree(trial_subdir)\n",
        "    return test_acc\n",
        "\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed= seed_value))\n",
        "study.optimize(objective_ADAM, n_trials= 40 )\n",
        "\n",
        "print(study.best_trial)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load model from checkpoints and then directly test it for Video data"
      ],
      "metadata": {
        "id": "gNcfHXD0c9mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 1\n",
        "task_name = 'Task4_P3_V' # Give the name of task for which you are testing results using checkpoints\n",
        "save_dir=\"/kaggle/working/GAT_checkpoints_Task5_A/trial_8/epoch_1\" # path to directory where checkpoints are saved\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "accuracy_list = []\n",
        "# Perform 1-subject leave-out cross-validation\n",
        "for idx in range(36):\n",
        "    # Load the checkpoint\n",
        "    checkpoint_path = os.path.join(save_dir, f\"subject_{idx}_model.pt\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    # Retrieve parameters\n",
        "    model_state_dict = checkpoint['model_state_dict']\n",
        "    lr = checkpoint['learning rate']\n",
        "    dr = checkpoint['dropout']\n",
        "    hidden_classes = checkpoint['hidden_classes']\n",
        "    num_heads = checkpoint['num_heads']\n",
        "    is_2nd_layer = checkpoint['is_model']\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}, num_heads: {num_heads}, is_2nd_layer: {is_2nd_layer}\")\n",
        "\n",
        "    model_GAT = GAT_variable_layer(node_features=240, hidden_channels=hidden_classes, num_heads=num_heads, num_classes=3, dropout_rate=dr, use_second_layer= is_2nd_layer).to(device)\n",
        "    # Load the model weights\n",
        "    model_GAT.load_state_dict(model_state_dict)\n",
        "    model_GAT.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Create DataLoader for the test dataset\n",
        "    test_dataset = [full_dataset[idx].to(device)]\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Testing loop\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            output_test = model_GAT(data, dropout_rate= dr, use_second_layer=is_2nd_layer)\n",
        "            predicted_test = output_test.argmax(dim=1)\n",
        "            target_test = data.y\n",
        "            print(\"Test Subject:\",idx,\"Predicted label:\", predicted_test, \"True label:\", target_test)\n",
        "            total_correct += (predicted_test == target_test).sum().item()\n",
        "            total_samples += data.y.size(0)\n",
        "            accuracy_list.append(accuracy_score(target_test.cpu().numpy(), predicted_test.cpu().numpy()))\n",
        "\n",
        "# Calculate and print test accuracy for the current subject\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(\"total correct :\", total_correct,\" total samples :\", total_samples)\n",
        "print(f\"Using Subject {idx} as test, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(\"Accuracy using inbuilt function:\", np.mean(accuracy_list))"
      ],
      "metadata": {
        "id": "Q-iR3kL4c6pO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpURwp0NCMH4"
      },
      "source": [
        "## Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training using Optuna for hyperparameter tuning on Audio data"
      ],
      "metadata": {
        "id": "LuA4ttPCcTm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial_dir = None\n",
        "best_trial_acc = float('-inf')\n",
        "task_name = 'Task5_A'\n",
        "def objective_ADAM(trial):\n",
        "    global best_trial_dir, best_trial_acc, task_name\n",
        "    # Hyperparams to tune\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2)\n",
        "    dr = trial.suggest_float(\"dropout\", 0.1, 0.8)\n",
        "    hidden_classes = trial.suggest_int('hidden_classes', 16, 180)\n",
        "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
        "    is_2nd_layer = trial.suggest_int(\"is_2nd_layer\", 0,1)\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}, num_heads: {num_heads}, is_2nd_layer: {is_2nd_layer}\")\n",
        "    # print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}, num_heads: {num_heads}\")\n",
        "    save_dir=f'/kaggle/working/GAT_checkpoints_{task_name}/'\n",
        "    trial_subdir = os.path.join(save_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(trial_subdir, exist_ok=True)\n",
        "    plot_dir=f'/kaggle/working/GAT_plots_{task_name}/'\n",
        "    plot_subdir = os.path.join(plot_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(plot_subdir, exist_ok=True)\n",
        "    # test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads, trial_subdir)\n",
        "    test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads,is_2nd_layer, trial_subdir, plot_subdir, task_name)\n",
        "    if test_acc > best_trial_acc:\n",
        "      best_trial_acc = test_acc\n",
        "      remove_trial_subdir= os.path.join(save_dir,f\"trial_{best_trial_dir}\")\n",
        "      if os.path.exists(remove_trial_subdir):\n",
        "          shutil.rmtree(remove_trial_subdir)\n",
        "      best_trial_dir= trial.number\n",
        "    else:\n",
        "        if os.path.exists(trial_subdir):\n",
        "            shutil.rmtree(trial_subdir)\n",
        "    return test_acc\n",
        "\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed= seed_value))\n",
        "study.optimize(objective_ADAM, n_trials= 40 )\n",
        "\n",
        "print(study.best_trial)"
      ],
      "metadata": {
        "id": "eTKqCI5PNVfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load model from checkpoints and then directly test it for audio data"
      ],
      "metadata": {
        "id": "oZTjSB_0cvFy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx2VM5Hw65c7"
      },
      "outputs": [],
      "source": [
        "batch_size= 1\n",
        "task_name = 'Task4_P3_A' # Give the name of task for which you are testing results using checkpoints\n",
        "save_dir=\"/kaggle/working/GAT_checkpoints_Task5_A/trial_8/epoch_1\" # path to directory where checkpoints are saved\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "accuracy_list = []\n",
        "# Perform 1-subject leave-out cross-validation\n",
        "for idx in range(36):\n",
        "    # Load the checkpoint\n",
        "    checkpoint_path = os.path.join(save_dir, f\"subject_{idx}_model.pt\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    # Retrieve parameters\n",
        "    model_state_dict = checkpoint['model_state_dict']\n",
        "    lr = checkpoint['learning rate']\n",
        "    dr = checkpoint['dropout']\n",
        "    hidden_classes = checkpoint['hidden_classes']\n",
        "    num_heads = checkpoint['num_heads']\n",
        "    is_2nd_layer = checkpoint['is_model']\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}, num_heads: {num_heads}, is_2nd_layer: {is_2nd_layer}\")\n",
        "\n",
        "    model_GAT = GAT_variable_layer(node_features=240, hidden_channels=hidden_classes, num_heads=num_heads, num_classes=3, dropout_rate=dr, use_second_layer= is_2nd_layer).to(device)\n",
        "    # Load the model weights\n",
        "    model_GAT.load_state_dict(model_state_dict)\n",
        "    model_GAT.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Create DataLoader for the test dataset\n",
        "    test_dataset = [full_dataset[idx].to(device)]\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Testing loop\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            output_test = model_GAT(data, dropout_rate= dr, use_second_layer=is_2nd_layer)\n",
        "            predicted_test = output_test.argmax(dim=1)\n",
        "            target_test = data.y\n",
        "            print(\"Test Subject:\",idx,\"Predicted label:\", predicted_test, \"True label:\", target_test)\n",
        "            total_correct += (predicted_test == target_test).sum().item()\n",
        "            total_samples += data.y.size(0)\n",
        "            accuracy_list.append(accuracy_score(target_test.cpu().numpy(), predicted_test.cpu().numpy()))\n",
        "\n",
        "# Calculate and print test accuracy for the current subject\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(\"total correct :\", total_correct,\" total samples :\", total_samples)\n",
        "print(f\"Using Subject {idx} as test, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(\"Accuracy using inbuilt function:\", np.mean(accuracy_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0liL-TTC8KP"
      },
      "source": [
        "## Early Fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training using Optuna for hyperparameter using on Early fusion of Video and Audio data"
      ],
      "metadata": {
        "id": "dBLh_a1adPiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial_dir = None\n",
        "best_trial_acc = float('-inf')\n",
        "task_name = 'Task5_E'\n",
        "def objective_ADAM(trial):\n",
        "    global best_trial_dir, best_trial_acc, task_name\n",
        "    # Hyperparams to tune\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2)\n",
        "    dr = trial.suggest_float(\"dropout\", 0.1, 0.8)\n",
        "    hidden_classes = trial.suggest_int('hidden_classes', 16, 180)\n",
        "    num_heads = trial.suggest_int('num_heads', 1, 8)\n",
        "    is_2nd_layer = trial.suggest_int(\"is_2nd_layer\", 0,1)\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}, num_heads: {num_heads}, is_2nd_layer: {is_2nd_layer}\")\n",
        "    # print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}, num_heads: {num_heads}\")\n",
        "    save_dir=f'/kaggle/working/GAT_checkpoints_{task_name}/'\n",
        "    trial_subdir = os.path.join(save_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(trial_subdir, exist_ok=True)\n",
        "    plot_dir=f'/kaggle/working/GAT_plots_{task_name}/'\n",
        "    plot_subdir = os.path.join(plot_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(plot_subdir, exist_ok=True)\n",
        "    # test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads, trial_subdir)\n",
        "    test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads,is_2nd_layer, trial_subdir, plot_subdir, task_name)\n",
        "    if test_acc > best_trial_acc:\n",
        "      best_trial_acc = test_acc\n",
        "      remove_trial_subdir= os.path.join(save_dir,f\"trial_{best_trial_dir}\")\n",
        "      if os.path.exists(remove_trial_subdir):\n",
        "          shutil.rmtree(remove_trial_subdir)\n",
        "      best_trial_dir= trial.number\n",
        "    else:\n",
        "        if os.path.exists(trial_subdir):\n",
        "            shutil.rmtree(trial_subdir)\n",
        "    return test_acc\n",
        "\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed= seed_value))\n",
        "study.optimize(objective_ADAM, n_trials= 40 )\n",
        "\n",
        "print(study.best_trial)"
      ],
      "metadata": {
        "id": "-gHd4tRAdV0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load model from checkpoints and then directly test it for Early fusion of Video and Audio data"
      ],
      "metadata": {
        "id": "65tz8HrPdW3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size= 1\n",
        "task_name = 'Task4_P3_E' # Give the name of task for which you are testing results using checkpoints\n",
        "save_dir=\"/kaggle/working/GAT_checkpoints_Task5_A/trial_8/epoch_1\" # path to directory where checkpoints are saved\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "accuracy_list = []\n",
        "# Perform 1-subject leave-out cross-validation\n",
        "for idx in range(36):\n",
        "    # Load the checkpoint\n",
        "    checkpoint_path = os.path.join(save_dir, f\"subject_{idx}_model.pt\")\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "    # Retrieve parameters\n",
        "    model_state_dict = checkpoint['model_state_dict']\n",
        "    lr = checkpoint['learning rate']\n",
        "    dr = checkpoint['dropout']\n",
        "    hidden_classes = checkpoint['hidden_classes']\n",
        "    num_heads = checkpoint['num_heads']\n",
        "    is_2nd_layer = checkpoint['is_model']\n",
        "    print(f\"learning rate:{lr},dropout: {dr}, hidden_classes: {hidden_classes}, num_heads: {num_heads}, is_2nd_layer: {is_2nd_layer}\")\n",
        "\n",
        "    model_GAT = GAT_variable_layer(node_features=240, hidden_channels=hidden_classes, num_heads=num_heads, num_classes=3, dropout_rate=dr, use_second_layer= is_2nd_layer).to(device)\n",
        "    # Load the model weights\n",
        "    model_GAT.load_state_dict(model_state_dict)\n",
        "    model_GAT.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Create DataLoader for the test dataset\n",
        "    test_dataset = [full_dataset[idx].to(device)]\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Testing loop\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            output_test = model_GAT(data, dropout_rate= dr, use_second_layer=is_2nd_layer)\n",
        "            predicted_test = output_test.argmax(dim=1)\n",
        "            target_test = data.y\n",
        "            print(\"Test Subject:\",idx,\"Predicted label:\", predicted_test, \"True label:\", target_test)\n",
        "            total_correct += (predicted_test == target_test).sum().item()\n",
        "            total_samples += data.y.size(0)\n",
        "            accuracy_list.append(accuracy_score(target_test.cpu().numpy(), predicted_test.cpu().numpy()))\n",
        "\n",
        "# Calculate and print test accuracy for the current subject\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(\"total correct :\", total_correct,\" total samples :\", total_samples)\n",
        "print(f\"Using Subject {idx} as test, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(\"Accuracy using inbuilt function:\", np.mean(accuracy_list))"
      ],
      "metadata": {
        "id": "ZgVwrg2ydq-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmL2cHDRB5CD"
      },
      "source": [
        "## Late Fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For late fusion the training and validation loop are different from previous training and validation loops."
      ],
      "metadata": {
        "id": "6mIHVkredw3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_late(train_loader1, dr1,is_2nd_layer1, optimizer1, model_GAT1, scheduler1, train_loader2, dr2,is_2nd_layer2, optimizer2, model_GAT2, scheduler2,criterion):\n",
        "    loss_train_per = []\n",
        "    accuracy_train = []\n",
        "    model_GAT1.train()\n",
        "    model_GAT2.train()\n",
        "    for data1, data2 in zip(train_loader1, train_loader2):\n",
        "        optimizer1.zero_grad()\n",
        "        output_train1 = model_GAT1(data1, dropout_rate= dr1, use_second_layer=is_2nd_layer1)\n",
        "        target_train1 = data1.y\n",
        "        loss1 = criterion(output_train1, target_train1)\n",
        "        loss1.backward(retain_graph= True)\n",
        "        optimizer1.step()\n",
        "\n",
        "        optimizer2.zero_grad()\n",
        "        output_train2 = model_GAT2(data2, dropout_rate= dr2, use_second_layer=is_2nd_layer2)\n",
        "        target_train2 = data2.y\n",
        "        loss2 = criterion(output_train2, target_train2)\n",
        "        loss2.backward(retain_graph= True)\n",
        "        optimizer2.step()\n",
        "        final_output = (output_train1+output_train2)/2.0\n",
        "        predicted_labels = torch.argmax(final_output, dim=1)\n",
        "        target_fused = torch.max(target_train1, target_train2)\n",
        "        loss = (loss1.item()+ loss2.item())/2.0\n",
        "        loss_train_per.append(loss)\n",
        "        accuracy_train.append(accuracy_score(target_fused.cpu().numpy(), predicted_labels.cpu().numpy()))\n",
        "    loss_train_total= np.mean(loss_train_per)\n",
        "    train_accuracy= np.mean(accuracy_train)\n",
        "    if scheduler1 is not None:\n",
        "        scheduler1.step()  # Update learning rate\n",
        "    if scheduler2 is not None:\n",
        "        scheduler2.step()  # Update learning rate\n",
        "\n",
        "    return train_accuracy, loss_train_total,model_GAT1, optimizer1, model_GAT2, optimizer2"
      ],
      "metadata": {
        "id": "FvSHnPBeNDGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valid_epoch_late(test_loader1, dr1, is_2nd_layer1, model_GAT1, test_loader2, dr2, is_2nd_layer2, model_GAT2, criterion):\n",
        "  val_loss_per=[]\n",
        "  accuracy_test=[]\n",
        "  model_GAT1.eval()\n",
        "  model_GAT2.eval()\n",
        "  with torch.no_grad():\n",
        "      for data1, data2 in zip(test_loader1, test_loader2):\n",
        "        output_test1 = model_GAT1(data1, dropout_rate= dr2, use_second_layer=is_2nd_layer2)\n",
        "        target_test1 = data1.y\n",
        "        output_test2 = model_GAT2(data2, dropout_rate= dr2, use_second_layer=is_2nd_layer2)\n",
        "        target_test2 = data2.y\n",
        "        output_test_fused = (output_test1+output_test2)/2.0\n",
        "        predicted_labels = torch.argmax(output_test_fused, dim=1)\n",
        "        target_fused = torch.max(target_test1, target_test2)\n",
        "        accuracy_test.append(accuracy_score(target_fused.cpu().numpy(), predicted_labels.cpu().numpy()))\n",
        "        loss = criterion(output_test_fused, target_fused)\n",
        "        val_loss_per.append(loss.item())\n",
        "  val_loss_total= np.mean(val_loss_per)\n",
        "  val_accuracy= np.mean(accuracy_test)\n",
        "  return val_accuracy, val_loss_total"
      ],
      "metadata": {
        "id": "N1z2mK_ENDBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_valid_loop_ADAM_late(lr1, dr1, hidden_classes1,num_heads1,is_2nd_layer1, lr2, dr2, hidden_classes2,num_heads2,is_2nd_layer2, trial_subdir, plot_subdir, task_name):\n",
        "    num_epochs = 20\n",
        "    best_epoch = -1\n",
        "    best_stop_epoch=-1\n",
        "    early_stop_count = 0\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc = 0\n",
        "    patience = 3\n",
        "    batch_size = 1\n",
        "    all_train_losses = [[] for _ in range(36)]\n",
        "    all_train_accuracies = [[] for _ in range(36)]\n",
        "    avg_train_losses=[] # Average of all training losses across all subjects per epoch\n",
        "    avg_train_accuracies=[] # Average of all training accuracies across all subjects per epoch\n",
        "    all_val_losses = []\n",
        "    all_val_accuracies = []\n",
        "    '''GAT MODEL with 2 layers in architecture\n",
        "    model_GAT = [GAT(node_features=240, hidden_channels= hidden_classes,num_heads=num_heads, num_classes=3, dropout_rate= dr).to(device) for _ in range(36)]\n",
        "    '''\n",
        "    # GAT model with parameters to choose no. of layers in architecture\n",
        "    model_GAT1 = [GAT_variable_layer(node_features=240, hidden_channels= hidden_classes1,num_heads=num_heads1, num_classes=3, dropout_rate= dr1, use_second_layer= is_2nd_layer1).to(device) for _ in range(36)]\n",
        "    optimizer1 = [torch.optim.Adam(model_GAT1[_].parameters(), lr= lr1) for _ in range(36)]\n",
        "    scheduler1 = [StepLR(optimizer1[_], step_size=1, gamma=0.9) for _ in range(36)] # decrease LR by a factor of 0.9 every epoch\n",
        "\n",
        "    model_GAT2 = [GAT_variable_layer(node_features=240, hidden_channels= hidden_classes2,num_heads=num_heads2, num_classes=3, dropout_rate= dr2, use_second_layer= is_2nd_layer2).to(device) for _ in range(36)]\n",
        "    optimizer2 = [torch.optim.Adam(model_GAT2[_].parameters(), lr= lr2) for _ in range(36)]\n",
        "    scheduler2 = [StepLR(optimizer2[_], step_size=1, gamma=0.9) for _ in range(36)] # decrease LR by a factor of 0.9 every epoch\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_subdir = os.path.join(trial_subdir, f\"epoch_{epoch}\")\n",
        "        os.makedirs(epoch_subdir, exist_ok=True)\n",
        "        # print(f\"## epoch {epoch}  ##\")\n",
        "        val_accuracies =[]\n",
        "        val_losses=[]\n",
        "        for idx in range(36):\n",
        "            # DataLoader setup\n",
        "            # print(f\"## Subject {idx} is test subject  ##\")\n",
        "            train_indices = list(range(36))\n",
        "            del train_indices[idx]\n",
        "            train_dataset1 = [full_dataset_V[i].to(device) for i in train_indices]\n",
        "            test_dataset1 = [full_dataset_V[idx].to(device)]\n",
        "            train_dataset2 = [full_dataset_A[i].to(device) for i in train_indices]\n",
        "            test_dataset2 = [full_dataset_A[idx].to(device)]\n",
        "\n",
        "            train_loader1 = DataLoader(train_dataset1, batch_size=batch_size, shuffle=True)\n",
        "            test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False)\n",
        "            train_loader2 = DataLoader(train_dataset2, batch_size=batch_size, shuffle=True)\n",
        "            test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            # Training loop\n",
        "            train_accuracy, train_loss, model_GAT1[idx], optimizer1[idx], model_GAT2[idx], optimizer2[idx] = train_epoch_late(train_loader1, dr1,is_2nd_layer1, optimizer1[idx], model_GAT1[idx], scheduler1[idx], train_loader2, dr2,is_2nd_layer2, optimizer2[idx], model_GAT2[idx], scheduler2[idx],criterion)\n",
        "            all_train_losses[idx].append(train_loss)\n",
        "            all_train_accuracies[idx].append(train_accuracy)\n",
        "\n",
        "            # Validation loop\n",
        "            val_accuracy,val_loss = valid_epoch_late(test_loader1, dr1, is_2nd_layer1, model_GAT1[idx], test_loader2, dr2, is_2nd_layer2, model_GAT2[idx], criterion)\n",
        "            val_losses.append(val_loss)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "            # Saving model checkpoints\n",
        "            model_filename1 = os.path.join(epoch_subdir, f\"Model1_subject_{idx}_model.pt\")\n",
        "            torch.save({\n",
        "                        'model_state_dict1': model_GAT1[idx].state_dict(),\n",
        "                        'learning rate1': lr1,\n",
        "                        'dropout1':dr1,\n",
        "                        'hidden_classes1':hidden_classes1,\n",
        "                        'num_heads1':num_heads1,\n",
        "                        'is_model1': is_2nd_layer1,\n",
        "                        }, model_filename1)\n",
        "            model_filename2 = os.path.join(epoch_subdir, f\"Model2_subject_{idx}_model.pt\")\n",
        "            torch.save({\n",
        "                        'model_state_dict2': model_GAT2[idx].state_dict(),\n",
        "                        'learning rate2': lr2,\n",
        "                        'dropout2':dr2,\n",
        "                        'hidden_classes2':hidden_classes2,\n",
        "                        'num_heads2':num_heads2,\n",
        "                        'is_model2': is_2nd_layer2,\n",
        "                        }, model_filename2)\n",
        "\n",
        "        all_val_losses.append(np.mean(val_losses))\n",
        "        all_val_accuracies.append(np.mean(val_accuracies))\n",
        "        print(f\" For epoch {epoch}, Validation Loss: {np.mean(val_losses):.4f}, Validation Accuracy: {np.mean(val_accuracies):.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if np.mean(val_losses) < best_val_loss:\n",
        "            best_val_loss = np.mean(val_losses)\n",
        "            early_stop_count = 0\n",
        "        else:\n",
        "            early_stop_count += 1\n",
        "\n",
        "        if np.mean(val_accuracies) > best_val_acc:\n",
        "            best_val_acc= np.mean(val_accuracies)\n",
        "            delete_dir_path= os.path.join(trial_subdir, f\"epoch_{best_epoch}\")\n",
        "            if os.path.exists(delete_dir_path):\n",
        "                  shutil.rmtree(delete_dir_path)\n",
        "            best_epoch=epoch\n",
        "        else:\n",
        "            delete_dir_path= os.path.join(trial_subdir, f\"epoch_{epoch}\")\n",
        "            if os.path.exists(delete_dir_path):\n",
        "                  shutil.rmtree(delete_dir_path)\n",
        "\n",
        "        if early_stop_count >= patience:\n",
        "            print(f'Early stopped at epoch: {epoch}')\n",
        "            best_stop_epoch= epoch\n",
        "            print(f\"Final Validation Accurcay: {best_val_acc}\")\n",
        "            break\n",
        "\n",
        "    # Calculating Average of all training losses across all subjects per epoch\n",
        "    for i in range(best_stop_epoch+1):\n",
        "      avg_train_losses.append(np.average([x[i] for x in all_train_losses]))\n",
        "      avg_train_accuracies.append(np.average([x[i] for x in all_train_accuracies]))\n",
        "\n",
        "#     print(f'length of average train losses, {len(avg_train_losses[0])}')\n",
        "        # Plotting code for all subjects\n",
        "    fig_loss, axs_loss = plt.subplots(6, 6, figsize=(18, 18))\n",
        "    fig_acc, axs_acc = plt.subplots(6, 6, figsize=(18, 18))\n",
        "\n",
        "    for idx in range(36):\n",
        "        # Plotting Loss\n",
        "        # epochs = range(len(all_train_losses[idx]))\n",
        "        ax_loss = axs_loss[idx // 6, idx % 6]\n",
        "        ax_loss.plot(range(len(all_train_losses[idx])), all_train_losses[idx], label='Train', color='red')\n",
        "        ax_loss.set_title(f\"Loss - Subject {idx}-{task_name}\")\n",
        "        ax_loss.legend()\n",
        "\n",
        "        # Plotting Accuracy\n",
        "        # epochs = range(len(all_train_accuracies[idx]))\n",
        "        ax_acc = axs_acc[idx // 6, idx % 6]\n",
        "        ax_acc.plot(range(len(all_train_accuracies[idx])), all_train_accuracies[idx], label='Train', color='green')\n",
        "        ax_acc.set_title(f\"Accuracy - Subject {idx}-{task_name}\")\n",
        "        ax_acc.legend()\n",
        "        # Set integer labels on x-axis\n",
        "        ax_loss.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "        ax_acc.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "\n",
        "    # Adjust layout\n",
        "    fig_loss.tight_layout()\n",
        "    fig_acc.tight_layout()\n",
        "\n",
        "    # Validation loss and Validation accuracy curves vs. epoch\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))\n",
        "    ax1.plot(range(len(all_val_losses)), all_val_losses, label='Validation Loss', color='orange')\n",
        "    ax1.plot(range(len(avg_train_losses)), avg_train_losses, label='Training Loss', color='blue')\n",
        "    ax1.set_title('Train + Val Loss vs. Epochs')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax2.plot(range(len(all_val_accuracies)), all_val_accuracies, label='Validation Accuracy', color='orange')\n",
        "    ax2.plot(range(len(avg_train_accuracies)), avg_train_accuracies, label='Training Accuracy', color='blue')\n",
        "    ax2.set_title('Train + Val Accuracy vs. Epochs')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "    ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: int(x)))\n",
        "    plt.tight_layout()\n",
        "\n",
        "    #  Saving the plots\n",
        "    fig_loss.savefig(os.path.join(plot_subdir+\"/\"+f\"training_loss.png\"))\n",
        "    fig_acc.savefig(os.path.join(plot_subdir+ \"/\"+ f'training_accuracy.png'))\n",
        "    fig.savefig(os.path.join(plot_subdir+ \"/\"+ f'validation_plots.png'))\n",
        "\n",
        "    # Show plots\n",
        "    plt.show()\n",
        "    print(\"Final validation accuracy :\", best_val_acc)\n",
        "    print(\"Epoch at which Best model with highest validation accuracy is obtained:\", best_epoch)\n",
        "    print(\"Epochs at which Early stopping is done:\", best_stop_epoch)\n",
        "    return best_val_acc"
      ],
      "metadata": {
        "id": "gZivnLOzNC68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training using Optuna for hyperparameter tuning on Late fusion of video and Audio data"
      ],
      "metadata": {
        "id": "0whm7Q5BeGFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial_dir = None\n",
        "best_trial_acc = float('-inf')\n",
        "task_name = 'Task5_L'\n",
        "def objective_ADAM_late(trial):\n",
        "    global best_trial_dir, best_trial_acc, task_name\n",
        "    # Hyperparams to tune\n",
        "    lr1 = trial.suggest_float(\"lr1\", 1e-5, 1e-2)\n",
        "    lr2 = trial.suggest_float(\"lr2\", 1e-5, 1e-2)\n",
        "    dr1 = trial.suggest_float(\"dropout1\", 0.1, 0.8)\n",
        "    dr2 = trial.suggest_float(\"dropout2\", 0.1, 0.8)\n",
        "    hidden_classes1 = trial.suggest_int(\"hidden_classes1\", 16, 180)\n",
        "    hidden_classes2 = trial.suggest_int(\"hidden_classes2\", 16, 180)\n",
        "    num_heads1= trial.suggest_int(\"num_heads1\", 1,8)\n",
        "    num_heads2= trial.suggest_int(\"num_heads2\", 1,8)\n",
        "    is_2nd_layer1 = trial.suggest_int(\"is_2nd_layer1\", 0,1)\n",
        "    is_2nd_layer2 = trial.suggest_int(\"is_2nd_layer2\", 0,1)\n",
        "    print(\"learning rate 1:\", lr1,\"learning rate 2:\", lr2,\"dropout rate1:\", dr1, \"dropout rate2:\", dr2,\"hidden_classes1 :\", hidden_classes1, \"hidden_classes2:\", hidden_classes2, \"num_heads1:\",num_heads1, \"num_heads2:\",num_heads2, \"is_2nd_layer1: \",is_2nd_layer1, \"is_2nd_layer2: \",is_2nd_layer2)\n",
        "    save_dir=f'/kaggle/working/GAT_checkpoints_{task_name}/'\n",
        "    trial_subdir = os.path.join(save_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(trial_subdir, exist_ok=True)\n",
        "    plot_dir=f'/kaggle/working/GAT_plots_{task_name}/'\n",
        "    plot_subdir = os.path.join(plot_dir, f\"trial_{trial.number}\")\n",
        "    os.makedirs(plot_subdir, exist_ok=True)\n",
        "    # test_acc = train_valid_loop_ADAM(lr, dr, hidden_classes,num_heads, trial_subdir)\n",
        "    test_acc = train_valid_loop_ADAM_late(lr1, dr1, hidden_classes1,num_heads1,is_2nd_layer1, lr2, dr2, hidden_classes2,num_heads2,is_2nd_layer2, trial_subdir, plot_subdir, task_name)\n",
        "    if test_acc > best_trial_acc:\n",
        "      best_trial_acc = test_acc\n",
        "      remove_trial_subdir= os.path.join(save_dir,f\"trial_{best_trial_dir}\")\n",
        "      if os.path.exists(remove_trial_subdir):\n",
        "          shutil.rmtree(remove_trial_subdir)\n",
        "      best_trial_dir= trial.number\n",
        "    else:\n",
        "        if os.path.exists(trial_subdir):\n",
        "            shutil.rmtree(trial_subdir)\n",
        "    return test_acc\n",
        "\n",
        "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed= seed_value))\n",
        "study.optimize(objective_ADAM_late, n_trials= 40 )\n",
        "\n",
        "print(study.best_trial)"
      ],
      "metadata": {
        "id": "2WLWQHuwNMCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load model from checkpoints and then directly test it for late fusion of video and Audio data"
      ],
      "metadata": {
        "id": "R-gk6qLklq3E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XINW3VecUY7y"
      },
      "outputs": [],
      "source": [
        "batch_size=1\n",
        "task_name = 'Task5_L' # name of task for which you are testing\n",
        "save_dir=\"/content/drive/MyDrive/GAT_checkpoints/Late Fusion/trial_23\" # path of directory where checkpoints are saved\n",
        "print(f\" dropout1: {dr1}, dropout2: {dr2}, hidden_classes1: {hidden_classes1}, hidden_classes2: {hidden_classes2}, num_heads1: {num_heads1}, num_heads2: {num_heads2}\")\n",
        "model_GAT1 = GAT(node_features=240, hidden_channels= hidden_classes1, num_heads=num_heads1, num_classes=3, dropout_rate=dr1).to(device)\n",
        "model_GAT2 = GAT(node_features=240, hidden_channels= hidden_classes2, num_heads=num_heads2, num_classes=3, dropout_rate=dr2).to(device)\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "test_accuracy = []\n",
        "# Perform 1-subject leave-out cross-validation\n",
        "for idx in range(36):\n",
        "    # Load the checkpoint\n",
        "    checkpoint_path1 = os.path.join(save_dir, f\"Model1_subject_{idx}_model.pt\")\n",
        "    checkpoint1 = torch.load(checkpoint_path1)\n",
        "    # Retrieve parameters\n",
        "    model_state_dict1 = checkpoint1['model_state_dict']\n",
        "    lr1 = checkpoint1['learning rate']\n",
        "    dr1 = checkpoint1['dropout']\n",
        "    hidden_classes1 = checkpoint1['hidden_classes']\n",
        "    num_heads1 = checkpoint1['num_heads']\n",
        "    is_2nd_layer1 = checkpoint1['is_model']\n",
        "    print(f\"learning rate:{lr1},dropout: {dr1}, hidden_classes: {hidden_classes1}, num_heads: {num_heads1}, is_2nd_layer: {is_2nd_layer1}\")\n",
        "\n",
        "    model_GAT1 = GAT_variable_layer(node_features=240, hidden_channels=hidden_classes1, num_heads=num_heads1, num_classes=3, dropout_rate=dr1, use_second_layer= is_2nd_layer1).to(device)\n",
        "    # Load the model weights\n",
        "    model_GAT1.load_state_dict(model_state_dict1)\n",
        "    model_GAT1.eval()  # Set the model to evaluation mode\n",
        "\n",
        "\n",
        "    checkpoint_path2 = os.path.join(save_dir, f\"Model2_subject_{idx}_model.pt\")\n",
        "    checkpoint2 = torch.load(checkpoint_path2)\n",
        "    # Retrieve parameters\n",
        "    model_state_dict2 = checkpoint2['model_state_dict']\n",
        "    lr2 = checkpoint2['learning rate']\n",
        "    dr2 = checkpoint2['dropout']\n",
        "    hidden_classes2 = checkpoint2['hidden_classes']\n",
        "    num_heads2 = checkpoint2['num_heads']\n",
        "    is_2nd_layer2 = checkpoint2['is_model']\n",
        "    print(f\"learning rate:{lr2},dropout: {dr2}, hidden_classes: {hidden_classes2}, num_heads: {num_heads2}, is_2nd_layer: {is_2nd_layer2}\")\n",
        "\n",
        "    model_GAT2 = GAT_variable_layer(node_features=240, hidden_channels=hidden_classes2, num_heads=num_heads2, num_classes=3, dropout_rate=dr2, use_second_layer= is_2nd_layer2).to(device)\n",
        "    # Load the model weights\n",
        "    model_GAT2.load_state_dict(model_state_dict2)\n",
        "    model_GAT2.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Create DataLoader for the test dataset\n",
        "    test_dataset1 = [full_dataset_V[idx].to(device)]\n",
        "    test_loader1 = DataLoader(test_dataset1, batch_size=batch_size, shuffle=False)\n",
        "    test_dataset2 = [full_dataset_A[idx].to(device)]\n",
        "    test_loader2 = DataLoader(test_dataset2, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Testing loop\n",
        "    model_GAT1.eval()\n",
        "    model_GAT2.eval()\n",
        "    with torch.no_grad():\n",
        "        for data1, data2 in zip(test_loader1, test_loader2):\n",
        "            output_test1 = model_GAT1(data1, dropout_rate= dr1)\n",
        "            target_test1 = data1.y\n",
        "            output_test2 = model_GAT2(data2, dropout_rate= dr2)\n",
        "            target_test2 = data2.y\n",
        "            final_output = (output_test1+output_test2)/2.0\n",
        "            predicted_labels = torch.argmax(final_output, dim=1)\n",
        "            target_fused = torch.max(target_test1, target_test2)\n",
        "            total_correct += (predicted_labels == target_fused).sum().item()\n",
        "            total_samples += data1.y.size(0)\n",
        "            print(\"Test Subject:\",idx,\" predicted label:\", predicted_labels ,\" True label : \", target_fused)\n",
        "            test_accuracy.append(accuracy_score(target_fused.cpu().numpy(), predicted_labels.cpu().numpy()))\n",
        "\n",
        "# Calculate and print test accuracy for the current subject\n",
        "test_accuracy = total_correct / total_samples\n",
        "print(\"total correct :\", total_correct,\" total samples :\", total_samples)\n",
        "print(f\"Using Subject {idx} as test, Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(\"Accuracy using inbuilt function:\", np.mean(test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66skrjP0-uQl"
      },
      "source": [
        "# ML Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rcVBgjN-w2B"
      },
      "outputs": [],
      "source": [
        "# all imports go here\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "!pip install boruta\n",
        "from boruta import BorutaPy\n",
        "from xgboost import plot_importance\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import tree\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.signal import savgol_filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9EA4mWc-9_C"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "import optuna\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, cross_val_predict, train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.model_selection import LeaveOneOut, LeaveOneGroupOut\n",
        "from sklearn.impute import SimpleImputer\n",
        "# prompt: ignore warnings\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(42)\n",
        "RANDOM_STATE = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG0AMnhn_ADs"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Defining Labels from values as per distribution\n",
        "values   labels\n",
        "0-2         0\n",
        "3-4         1\n",
        "5-19        2\n",
        "'''\n",
        "labels=[]\n",
        "values=[13,4,19,14,3,3,2,4,7,1,5,4,0,0,3,4,2,13,8,0,2,2,1,6,3,2,3,4,2,4,2,0,7,1,13,2]\n",
        "for i in values:\n",
        "  if (i<=2):\n",
        "        labels.append(0)\n",
        "  elif (i>=5):\n",
        "        labels.append(2)\n",
        "  else :\n",
        "        labels.append(1)\n",
        "print(labels)\n",
        "print(len(set(labels)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2uhRN0K_CZJ"
      },
      "outputs": [],
      "source": [
        "# Specify the directory containing CSV files\n",
        "directory_path = '/content/drive/MyDrive/Cambridge_Data/Modify_Audio_Data/Task5p_A'\n",
        "files=os.listdir(directory_path)\n",
        "files.sort()\n",
        "media = 'audio'\n",
        "# Define a function to calculate auto-correlation\n",
        "def auto_correlation(series, lag=1):\n",
        "    return np.corrcoef(series[:-lag], series[lag:])[0, 1]\n",
        "\n",
        "# Initialize an empty DataFrame to store results for each subject\n",
        "subject_results = pd.DataFrame()\n",
        "\n",
        "# Loop through each CSV file in the directory\n",
        "for file_name in files:\n",
        "      file_path = os.path.join(directory_path, file_name)\n",
        "\n",
        "      # Load CSV data into a pandas DataFrame\n",
        "      df = pd.read_csv(file_path)\n",
        "      if media=='audio':\n",
        "          df.columns=['Unnamed: 0','MFCC 1', 'MFCC 2', 'MFCC 3', 'MFCC 4', 'MFCC 5', 'MFCC 6', 'MFCC 7', 'MFCC 8', 'MFCC 9', 'MFCC 10', 'MFCC 11', 'MFCC 12', 'MFCC 13', 'MFCC Delta 1', 'MFCC Delta 2', 'MFCC Delta 3', 'MFCC Delta 4', 'MFCC Delta 5', 'MFCC Delta 6', 'MFCC Delta 7', 'MFCC Delta 8', 'MFCC Delta 9', 'MFCC Delta 10', 'MFCC Delta 11', 'MFCC Delta 12', 'MFCC Delta 13', 'MFCC Delta Delta 1', 'MFCC Delta Delta 2', 'MFCC Delta Delta 3', 'MFCC Delta Delta 4', 'MFCC Delta Delta 5', 'MFCC Delta Delta 6', 'MFCC Delta Delta 7', 'MFCC Delta Delta 8', 'MFCC Delta Delta 9', 'MFCC Delta Delta 10', 'MFCC Delta Delta 11', 'MFCC Delta Delta 12', 'MFCC Delta Delta 13', 'GTCC 1', 'GTCC 2', 'GTCC 3', 'GTCC 4', 'GTCC 5', 'GTCC 6', 'GTCC 7', 'GTCC 8', 'GTCC 9', 'GTCC 10', 'GTCC 11', 'GTCC 12', 'GTCC 13', 'GTCC Delta 1', 'GTCC Delta 2', 'GTCC Delta 3', 'GTCC Delta 4', 'GTCC Delta 5', 'GTCC Delta 6', 'GTCC Delta 7', 'GTCC Delta 8', 'GTCC Delta 9', 'GTCC Delta 10', 'GTCC Delta 11', 'GTCC Delta 12', 'GTCC Delta 13', 'GTCC Delta Delta 1','GTCC Delta Delta 2','GTCC Delta Delta 3', 'GTCC Delta Delta 4', 'GTCC Delta Delta 5', 'GTCC Delta Delta 6', 'GTCC Delta Delta 7', 'GTCC Delta Delta 8', 'GTCC Delta Delta 9', 'GTCC Delta Delta 10', 'GTCC Delta Delta 11', 'GTCC Delta Delta 12', 'GTCC Delta Delta 13', 'Spectral Centroid', 'Spectral Crest', 'Spectral Decrease', 'Spectral Entropy', 'Spectral Flatness', 'Spectral Flux', 'Spectral Kurtosis', 'Spectral Rolloff', 'Spectral Skewness', 'Spectral Slope', 'Spectral Spread', 'Pitch', 'Harmonic Ratio']\n",
        "      df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "      # # Create a dictionary to store statistical features for the current subject\n",
        "      # subject_features = {'Subject': file_name.split('.')[0]}\n",
        "      subject_features = pd.DataFrame(columns=['Subject'])\n",
        "      subject_features['Subject'] = [file_name.split('.')[0]]\n",
        "\n",
        "      # Calculate statistics for each feature and its derivatives\n",
        "      for column in df.columns:\n",
        "          feature_data = df[column]\n",
        "\n",
        "          # Original Feature\n",
        "          subject_features[f'Avg_{column}'] = np.mean(feature_data)\n",
        "          subject_features[f'Max_{column}'] = np.max(feature_data)\n",
        "          subject_features[f'Min_{column}'] = np.min(feature_data)\n",
        "          subject_features[f'Auto-Correlation_{column}'] = auto_correlation(feature_data)\n",
        "          subject_features[f'Std_Dev_{column}'] = np.std(feature_data)\n",
        "          subject_features[f'Median_{column}'] = np.median(feature_data)\n",
        "\n",
        "          # First Derivative\n",
        "          first_derivative = np.gradient(feature_data)\n",
        "          subject_features[f'Avg_Derivative1_{column}'] = np.mean(first_derivative)\n",
        "          subject_features[f'Max_Derivative1_{column}'] = np.max(first_derivative)\n",
        "          subject_features[f'Min_Derivative1_{column}'] = np.min(first_derivative)\n",
        "          subject_features[f'Auto-Correlation_Derivative1_{column}'] = auto_correlation(first_derivative)\n",
        "          subject_features[f'Std_Dev_Derivative1_{column}'] = np.std(first_derivative)\n",
        "          subject_features[f'Median_Derivative1_{column}'] = np.median(first_derivative)\n",
        "\n",
        "          # Second Derivative\n",
        "          second_derivative = np.gradient(first_derivative)\n",
        "          subject_features[f'Avg_Derivative2_{column}'] = np.mean(second_derivative)\n",
        "          subject_features[f'Max_Derivative2_{column}'] = np.max(second_derivative)\n",
        "          subject_features[f'Min_Derivative2_{column}'] = np.min(second_derivative)\n",
        "          subject_features[f'Auto-Correlation_Derivative2_{column}'] = auto_correlation(second_derivative)\n",
        "          subject_features[f'Std_Dev_Derivative2_{column}'] = np.std(second_derivative)\n",
        "          subject_features[f'Median_Derivative2_{column}'] = np.median(second_derivative)\n",
        "\n",
        "      # Append the features for the current subject to the DataFrame\n",
        "      subject_results = subject_results._append(subject_features, ignore_index=True)\n",
        "\n",
        "# Display or further process subject_results DataFrame as needed\n",
        "print(subject_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJuXzD4h_GwJ"
      },
      "outputs": [],
      "source": [
        "# Drop subject column from a dataframe as it is not a feature\n",
        "subject_results.drop('Subject', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CXDKzBM_Id2"
      },
      "outputs": [],
      "source": [
        "#  Replace Nan values with mean values of a column\n",
        "# Create an imputer transformer\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Fit the imputer to the data\n",
        "imputer.fit(subject_results)\n",
        "\n",
        "# Transform the data\n",
        "# subject_results = imputer.transform(subject_results)\n",
        "subject_results = pd.DataFrame(imputer.transform(subject_results), columns=subject_results.columns, index=subject_results.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYTNBUrikCup"
      },
      "outputs": [],
      "source": [
        "for train_index, test_index in LeaveOneOut().split(subject_results, labels):\n",
        "#       X_train_features = subject_results.iloc[train_index].columns\n",
        "#       X_test_features = subject_results.iloc[test_index].columns\n",
        "      X_train, X_test = subject_results.iloc[train_index], subject_results.iloc[test_index]\n",
        "      y_train, y_test = np.array(labels)[train_index], np.array(labels)[test_index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCh3SRnN_IaI"
      },
      "outputs": [],
      "source": [
        "# Function for testing of ML models\n",
        "def test_results(clf_model, best_params,subject_results, labels):\n",
        "      if clf_model == 'logistic_regression':\n",
        "        optimal_clf = LogisticRegression(**best_params)\n",
        "\n",
        "      elif clf_model == 'linear_svm':\n",
        "        optimal_clf = SVC(kernel='linear', probability=True, **best_params)\n",
        "\n",
        "      elif clf_model == 'rbf_svm':\n",
        "        optimal_clf = SVC(kernel='rbf', probability=True, **best_params)\n",
        "\n",
        "      elif clf_model == 'decision_tree':\n",
        "        optimal_clf = DecisionTreeClassifier(**best_params)\n",
        "\n",
        "      elif clf_model == 'rforest':\n",
        "        optimal_clf = RandomForestClassifier(**best_params)\n",
        "\n",
        "      elif clf_model == 'adaboost':\n",
        "        optimal_clf = AdaBoostClassifier(**best_params)\n",
        "\n",
        "      elif clf_model == 'xgboost':\n",
        "        optimal_clf = xgb.XGBClassifier(**best_params)\n",
        "\n",
        "      elif clf_model == 'bagging':\n",
        "        optimal_clf = BaggingClassifier(**best_params)\n",
        "\n",
        "      optimal_clf.fit(subject_results, labels)\n",
        "      y_pred = optimal_clf.predict(subject_results)\n",
        "      print(\"Model:\",clf_model, 'Test Accuracy:', accuracy_score(labels, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Aj1AcNo_IQh"
      },
      "outputs": [],
      "source": [
        "#  Optuna for hyperparameter tuning of ML models\n",
        "def objective(trial, clf_name, subject_results, labels):\n",
        "    if clf_name == 'logistic_regression':\n",
        "        C = trial.suggest_loguniform('C', 1e-2, 1e+2)\n",
        "        solver= trial.suggest_categorical(\"solver\",['liblinear','newton-cg','lbfgs','sag','saga'])\n",
        "        clf_model = LogisticRegression(C=C, max_iter=10000, solver=solver , random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'linear_svm':\n",
        "        C = trial.suggest_loguniform('C', 1e-2, 1e+2)\n",
        "        degree = trial.suggest_int('degree',1, 50)\n",
        "        gamma = trial.suggest_loguniform('gamma',0.001,10000)\n",
        "        clf_model = SVC(C=C, kernel='linear', degree=degree,gamma=gamma, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'rbf_svm':\n",
        "        C = trial.suggest_loguniform('C', 1e-2, 1e+2)\n",
        "        degree = trial.suggest_int('degree',1, 50)\n",
        "        gamma = trial.suggest_loguniform('gamma',1e-2,1e+2)\n",
        "        clf_model = SVC(C=C, kernel='rbf', degree=degree,gamma=gamma, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'decision_tree':\n",
        "        max_depth = trial.suggest_int('max_depth', 2, 20)\n",
        "        clf_model = DecisionTreeClassifier(max_depth=max_depth, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'rforest':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 2, 20)\n",
        "        max_depth = int(trial.suggest_int('max_depth', 1, 32, log=True))\n",
        "        clf_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'adaboost':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 20, 100)\n",
        "        clf_model = AdaBoostClassifier(n_estimators=n_estimators, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'xgboost':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 20, 100)\n",
        "        clf_model = xgb.XGBClassifier(n_estimators=n_estimators, random_state=RANDOM_STATE)\n",
        "\n",
        "    elif clf_name == 'bagging':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 20, 100)\n",
        "        clf_model = BaggingClassifier(n_estimators=n_estimators, random_state=RANDOM_STATE)\n",
        "\n",
        "    scores = []\n",
        "    for train_index, test_index in LeaveOneOut().split(subject_results, labels):\n",
        "#       X_train_features = subject_results.iloc[train_index].columns\n",
        "#       X_test_features = subject_results.iloc[test_index].columns\n",
        "      X_train, X_test = subject_results.iloc[train_index], subject_results.iloc[test_index]\n",
        "      y_train, y_test = np.array(labels)[train_index], np.array(labels)[test_index]\n",
        "      # get the scaled version of the train and test set\n",
        "#       mm = MinMaxScaler()\n",
        "#       X_train_scaled = pd.DataFrame(mm.fit_transform(X_train), columns= X_train_features)\n",
        "#       X_test_scaled = pd.DataFrame(mm.transform(X_test), columns=X_test_features)\n",
        "      clf_model.fit(X_train, y_train)\n",
        "      score = accuracy_score(y_test, clf_model.predict(X_test))\n",
        "      scores.append(score)\n",
        "    return np.mean(scores)\n",
        "\n",
        "\n",
        "# define models\n",
        "MODELS = ['logistic_regression',\n",
        "         'linear_svm',\n",
        "         'rbf_svm',\n",
        "         'decision_tree',\n",
        "          'rforest',\n",
        "         'adaboost',\n",
        "         'xgboost',\n",
        "         'bagging']\n",
        "\n",
        "for clf_model in MODELS:\n",
        "    # print clf_model\n",
        "    print(clf_model)\n",
        "    sampler = optuna.samplers.TPESampler(multivariate=True)\n",
        "    # default is Tree-structured Parzen Estimator (TPE) optimization algorithm\n",
        "    study = optuna.create_study(direction='maximize',\n",
        "                              sampler=sampler,\n",
        "                              study_name=clf_model,\n",
        "                              pruner=optuna.pruners.HyperbandPruner(min_resource=1, reduction_factor=3))\n",
        "    ## we use X_train in the following, not X_train_scaled, because optuna pipeline in objective function has MinMaxScaler() already\n",
        "    study.optimize(lambda trial:objective(trial, clf_model,subject_results, labels), n_trials= 35)\n",
        "    trial = study.best_trial\n",
        "    best_params = trial.params\n",
        "    print(\"Model:\",clf_model,\"Best accuracy:\", study.best_value,\"Best parameters:\", study.best_params)\n",
        "    test_results(clf_model, best_params, subject_results, labels)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "U0Ykh8yFfS8d",
        "MteEHxhY2pCa",
        "wBeu9fsIyxFN",
        "A9CNTDnzzTNn",
        "IhgxzHSB2W7O",
        "sa4hFYjy2uk5",
        "OM2edhxQzdBk",
        "1qRwVLgs5sGj",
        "JMC45fJe16T7",
        "lkkVOylG3zCa",
        "OkpxQnGYkQW4",
        "oqzosWHO8DPW",
        "BtDHjlTo0m7c",
        "jKW8QJ4B1RNT",
        "yT8ZtneK12q8",
        "avPI6_f5iLrP",
        "nFjUUjb84RoL",
        "TbpA29-f4Yd1",
        "ojCGxRST3b0h",
        "B2ftJ50oe2EI",
        "SxRbw3cKb3og",
        "gpURwp0NCMH4",
        "MmL2cHDRB5CD",
        "66skrjP0-uQl"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}